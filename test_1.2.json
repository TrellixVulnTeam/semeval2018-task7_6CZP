{"data": [{"qas_id": "L08-1450.12_L08-1450.13", "question_text": "representations [BREAK] representation", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "USAGE"}, {"qas_id": "L08-1450.17_L08-1450.18", "question_text": "documentation [BREAK] linguistic analyses", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "TOPIC"}, {"qas_id": "L08-1450.28_L08-1450.29", "question_text": "framework [BREAK] representations", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "USAGE"}, {"qas_id": "L08-1450.30_L08-1450.32", "question_text": "dependency structures [BREAK] sentence", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1450.34_L08-1450.35", "question_text": "issues [BREAK] querying", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1450.36_L08-1450.37", "question_text": "framework [BREAK] corpora", "context": "A LAF/GrAF based Encoding Scheme for underspecified Representations of syntactic Annotations. . Data models and encoding formats for syntactically annotated text corpora need to deal with syntactic ambiguity ; underspecified representations are particularly well suited for the representation of ambiguous data because they allow for high informational efficiency . We discuss the issue of being informationally efficient, and the trade-off between efficient encoding of linguistic annotations and complete documentation of linguistic analyses . The main topic of this article is a data model and an encoding scheme based on LAF/GrAF ( Ide and Romary, 2006 ; Ide and Suderman, 2007 ) which provides a flexible framework for encoding underspecified representations . We show how a set of dependency structures and a set of TiGer graphs ( Brants et al., 2002 ) representing the readings of an ambiguous sentence can be encoded, and we discuss basic issues in querying corpora which are encoded using the framework presented here.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1459.5_L08-1459.7", "question_text": "paper [BREAK] study", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "TOPIC"}, {"qas_id": "L08-1459.8_L08-1459.9", "question_text": "constructions [BREAK] corpora", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1459.15_L08-1459.16", "question_text": "understanding [BREAK] properties", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "TOPIC"}, {"qas_id": "L08-1459.18_L08-1459.20", "question_text": "natural language generation system [BREAK] output", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "RESULT"}, {"qas_id": "L08-1459.21_L08-1459.23", "question_text": "correlation [BREAK] types", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1459.30_L08-1459.31", "question_text": "strategies [BREAK] extracting", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "USAGE"}, {"qas_id": "L08-1459.33_L08-1459.35", "question_text": "properties [BREAK] results", "context": "A Study of Parentheticals in Discourse Corpora - Implications for NLG Systems . This paper presents a corpus study of parenthetical constructions in two different corpora : the Penn Discourse Treebank (PDTB, (PDTB- Group, 2008 )) and the RST Discourse Treebank ( Carlson et al., 2001 ). The motivation for the study is to gain a better understanding of the rhetorical properties of parentheticals in order to enable a natural language generation system to produce parentheticals as part of a rhetorically well-formed output . We argue that there is a correlation between syntactic and rhetorical types of parentheticals and establish two main categories : elaboration/ expansion-type NP-modifier parentheticals and non-elaboration/ expansion-type VP- or S- modifier parentheticals. We show several strategies for extracting these from the two corpora and discuss how the seemingly contradictory results obtained can be reconciled in light of the rhetorical and syntactic properties of parentheticals as well as the decisions taken in the annotation guidelines .", "tag": "RESULT"}, {"qas_id": "I05-2027.1_I05-2027.4", "question_text": "Machine Learning Approach [BREAK] Generation", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "USAGE"}, {"qas_id": "I05-2027.5_I05-2027.6", "question_text": "paper [BREAK] system", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "TOPIC"}, {"qas_id": "I05-2027.8_I05-2027.10", "question_text": "information [BREAK] technique", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "USAGE"}, {"qas_id": "I05-2027.12_I05-2027.13", "question_text": "headlines [BREAK] text", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "PART_WHOLE"}, {"qas_id": "I05-2027.14_I05-2027.15", "question_text": "system [BREAK] system", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "COMPARE"}, {"qas_id": "I05-2027.19_I05-2027.20", "question_text": "topic [BREAK] headlines", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-2027.32_I05-2027.34", "question_text": "learning approach [BREAK] system", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "USAGE"}, {"qas_id": "I05-2027.36_I05-2027.37", "question_text": "systems [BREAK] performance", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "RESULT"}, {"qas_id": "I05-2027.39_I05-2027.43", "question_text": "collection [BREAK] ROUGE", "context": "Machine Learning Approach to Augmenting News Headline Generation .In this paper , we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text . We compare our system with the Topiary system which, in contrast , uses a statistical learning approach to finding topic descriptors for headlines . The Topiary system , developed at the University of Maryland with BBN, was the top performing headline generation system at DUC 2004. Topiary-style headlines consist of a number of general topic labels followed by a compressed version of the lead sentence of a news story. The Topiary system uses a statistical learning approach to finding topic labels. The performance of these systems is evaluated using the ROUGE evaluation suite on the DUC 2004 news stories collection .", "tag": "USAGE"}, {"qas_id": "N03-1014.5_N03-1014.6", "question_text": "method [BREAK] representations", "context": "Inducing History Representations For Broad Coverage Statistical Parsing . We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions .", "tag": "USAGE"}, {"qas_id": "N03-1014.8_N03-1014.9", "question_text": "representations [BREAK] probabilities", "context": "Inducing History Representations For Broad Coverage Statistical Parsing . We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions .", "tag": "USAGE"}, {"qas_id": "N03-1014.15_N03-1014.18", "question_text": "performance [BREAK] parser", "context": "Inducing History Representations For Broad Coverage Statistical Parsing . We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions .", "tag": "COMPARE"}, {"qas_id": "N03-1014.23_N03-1014.24", "question_text": "biases [BREAK] success", "context": "Inducing History Representations For Broad Coverage Statistical Parsing . We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser . The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task , despite using a smaller vocabulary size and less prior linguistic knowledge . Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions .", "tag": "RESULT"}, {"qas_id": "N03-2021.2_N03-2021.3", "question_text": "Machine Translation [BREAK] Recall", "context": "Precision And Recall Of Machine Translation . Machine translation can be evaluated using precision , recall , and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives . More importantly, the standard measures have an intuitive interpretation , which can facilitate insights into how MT systems might be improved . The relevant software is publicly available.", "tag": "RESULT"}, {"qas_id": "N03-2021.10_N03-2021.12", "question_text": "judgments [BREAK] alternatives", "context": "Precision And Recall Of Machine Translation . Machine translation can be evaluated using precision , recall , and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives . More importantly, the standard measures have an intuitive interpretation , which can facilitate insights into how MT systems might be improved . The relevant software is publicly available.", "tag": "COMPARE"}, {"qas_id": "N03-2021.15_N03-2021.16", "question_text": "insights [BREAK] MT systems", "context": "Precision And Recall Of Machine Translation . Machine translation can be evaluated using precision , recall , and the F-measure. These standard measures have significantly higher correlation with human judgments than recently proposed alternatives . More importantly, the standard measures have an intuitive interpretation , which can facilitate insights into how MT systems might be improved . The relevant software is publicly available.", "tag": "TOPIC"}, {"qas_id": "N06-1042.6_N06-1042.7", "question_text": "model [BREAK] disambiguation", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "USAGE"}, {"qas_id": "N06-1042.12_N06-1042.13", "question_text": "algorithm [BREAK] training", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "USAGE"}, {"qas_id": "N06-1042.14_N06-1042.17", "question_text": "ambiguity [BREAK] languages", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.18_N06-1042.19", "question_text": "words [BREAK] text", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "PART_WHOLE"}, {"qas_id": "N06-1042.20_N06-1042.22", "question_text": "suffixes [BREAK] word", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.23_N06-1042.24", "question_text": "number [BREAK] tags", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.27_N06-1042.28", "question_text": "model [BREAK] morphological features", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.33_N06-1042.34", "question_text": "parses [BREAK] word", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.35_N06-1042.37", "question_text": "confidence [BREAK] parse", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "N06-1042.38_N06-1042.41", "question_text": "accuracy [BREAK] results", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "COMPARE"}, {"qas_id": "N06-1042.46_N06-1042.47", "question_text": "tags [BREAK] list", "context": "Learning Morphological Disambiguation Rules For Turkish . In this paper , we present a rule based model for morphological disambiguation of Turkish. The rules are generated by a novel decision list learning algorithm using supervised training . Morphological ambiguity (e.g. lives = live+s or life+s) is a challenging problem for agglutinative languages like Turkish where close to half of the words in running text are morphologically ambiguous. Furthermore, it is possible for a word to take an unlimited number of suffixes , therefore the number of possible morphological tags is unlimited. We attempted to cope with these problems by training a separate model for each of the 126 morphological features recognized by the morphological analyzer . The resulting decision lists independently vote on each of the potential parses of a word and the final parse is selected based on our confidence on these votes. The accuracy of our model (96%) is slightly above the best previously reported results which use statistical models . For comparison , when we train a single decision list on full tags instead of using separate models on each feature we get 91% accuracy .", "tag": "PART_WHOLE"}, {"qas_id": "M92-1018.4_M92-1018.6", "question_text": "paper [BREAK] results", "context": "SRA SOLOMON: MUC-4 Test Results And Analysis . In this paper , we report SRA's results on the MUC-4 task and describe how we trained our natural language processing system for MUC-4. We also report on what worked, what didn't work, and lessons learned. Our MUC-4 system embeds the SOLOMON knowledge-based NLP shell which is designed for both and We are currently using SOLOMON for a Spanish and Japanese text understanding project in a different domain . Although this was our first year participating in MUC, we have built and are currently building other data extraction systems .", "tag": "TOPIC"}, {"qas_id": "E95-1014.3_E95-1014.5", "question_text": "Method [BREAK] Identification", "context": "Corpus- Based Method For Automatic Identification Of Support Verbs For Nominalizations . Nominalization is a highly productive phenomena in most languages . The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal ).", "tag": "USAGE"}, {"qas_id": "E95-1014.7_E95-1014.8", "question_text": "phenomena [BREAK] languages", "context": "Corpus- Based Method For Automatic Identification Of Support Verbs For Nominalizations . Nominalization is a highly productive phenomena in most languages . The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal ).", "tag": "PART_WHOLE"}, {"qas_id": "E95-1014.10_E95-1014.12", "question_text": "role [BREAK] verb", "context": "Corpus- Based Method For Automatic Identification Of Support Verbs For Nominalizations . Nominalization is a highly productive phenomena in most languages . The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal ).", "tag": "MODEL-FEATURE"}, {"qas_id": "E95-1014.13_E95-1014.15", "question_text": "verb [BREAK] verb", "context": "Corpus- Based Method For Automatic Identification Of Support Verbs For Nominalizations . Nominalization is a highly productive phenomena in most languages . The process of nominalization ejects a verb from its syntactic role into a nominal position. The original verb is often replaced by a semantically emptied support verb (e.g., make a proposal ).", "tag": "COMPARE"}, {"qas_id": "A00-1030.1_A00-1030.4", "question_text": "Morphology [BREAK] Coverage", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "USAGE"}, {"qas_id": "A00-1030.5_A00-1030.6", "question_text": "paper [BREAK] approach", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "TOPIC"}, {"qas_id": "A00-1030.8_A00-1030.9", "question_text": "lexical information [BREAK] natural language processing", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "USAGE"}, {"qas_id": "A00-1030.11_A00-1030.12", "question_text": "rules [BREAK] system", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "USAGE"}, {"qas_id": "A00-1030.14_A00-1030.15", "question_text": "words [BREAK] lexicon", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "PART_WHOLE"}, {"qas_id": "A00-1030.27_A00-1030.28", "question_text": "words [BREAK] corpus", "context": "Aggressive Morphology For Robust Lexical Coverage . This paper describes an approach to providing lexical information for natural language processing in unrestricted domains . A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms . The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus .", "tag": "PART_WHOLE"}, {"qas_id": "A00-2029.3_A00-2029.6", "question_text": "hypothesis [BREAK] dialogue systems", "context": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues . In spoken dialogue systems , it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input , or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition . We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.", "tag": "USAGE"}, {"qas_id": "A00-2029.9_A00-2029.11", "question_text": "errors [BREAK] strategy", "context": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues . In spoken dialogue systems , it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input , or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition . We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.", "tag": "RESULT"}, {"qas_id": "A00-2029.14_A00-2029.16", "question_text": "error [BREAK] hypothesis", "context": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues . In spoken dialogue systems , it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input , or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition . We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.", "tag": "PART_WHOLE"}, {"qas_id": "A00-2029.18_A00-2029.19", "question_text": "thresholds [BREAK] automatic speech recognition", "context": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues . In spoken dialogue systems , it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input , or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition . We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.", "tag": "USAGE"}, {"qas_id": "A00-2029.20_A00-2029.21", "question_text": "differences [BREAK] results", "context": "Predicting Automatic Speech Recognition Performance Using Prosodic Cues . In spoken dialogue systems , it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input , or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodie features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition . We present analytic results indicating that there are significant prosodie differences between correctly and incorrectly recognized turns.", "tag": "RESULT"}, {"qas_id": "H92-1099.2_H92-1099.3", "question_text": "Information [BREAK] Speech Recognition", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "USAGE"}, {"qas_id": "H92-1099.6_H92-1099.8", "question_text": "project [BREAK] information", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "TOPIC"}, {"qas_id": "H92-1099.15_H92-1099.16", "question_text": "information [BREAK] parsing", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "USAGE"}, {"qas_id": "H92-1099.17_H92-1099.18", "question_text": "research [BREAK] representation", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "TOPIC"}, {"qas_id": "H92-1099.19_H92-1099.20", "question_text": "information [BREAK] speech understanding system", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "USAGE"}, {"qas_id": "H92-1099.22_H92-1099.23", "question_text": "algorithms [BREAK] detection", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "USAGE"}, {"qas_id": "H92-1099.24_H92-1099.25", "question_text": "cues [BREAK] speech", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "PART_WHOLE"}, {"qas_id": "H92-1099.27_H92-1099.28", "question_text": "cues [BREAK] parser", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "USAGE"}, {"qas_id": "H92-1099.30_H92-1099.33", "question_text": "Spoken Language System [BREAK] improvements", "context": "Evaluating The Use Of Prosodic Information In Speech Recognition And Understanding . The goal of this project is to investigate the use of different levels of prosodie information in speech recognition and understanding . In particular, the current focus of the work is the use of prosodie phrase boundary information in parsing . The research involves determining a representation of prosodie information suitable for use in a speech understanding system , developing reliable algorithms for detection of the prosodie cues in speech , investigating architectures for integrating prosodie cues in a parser , and evaluating the potential improvements of prosody in the context of the SRI Spoken Language System . This research is sponsored jointly by DARPA and NSF.", "tag": "RESULT"}, {"qas_id": "H93-1048.2_H93-1048.3", "question_text": "Tree [BREAK] Text", "context": "Prediction Of Lexicalized Tree Fragments In Text . There is a mismatch between the distribution of information in text , and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars . Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees . These can be used to enhance an ngram model , and in analogical parsing .", "tag": "PART_WHOLE"}, {"qas_id": "H93-1048.6_H93-1048.7", "question_text": "information [BREAK] text", "context": "Prediction Of Lexicalized Tree Fragments In Text . There is a mismatch between the distribution of information in text , and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars . Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees . These can be used to enhance an ngram model , and in analogical parsing .", "tag": "PART_WHOLE"}, {"qas_id": "H93-1048.8_H93-1048.9", "question_text": "variety [BREAK] formalisms", "context": "Prediction Of Lexicalized Tree Fragments In Text . There is a mismatch between the distribution of information in text , and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars . Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees . These can be used to enhance an ngram model , and in analogical parsing .", "tag": "MODEL-FEATURE"}, {"qas_id": "H93-1048.15_H93-1048.18", "question_text": "distributions [BREAK] trees", "context": "Prediction Of Lexicalized Tree Fragments In Text . There is a mismatch between the distribution of information in text , and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars . Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees . These can be used to enhance an ngram model , and in analogical parsing .", "tag": "MODEL-FEATURE"}, {"qas_id": "H93-1060.4_H93-1060.7", "question_text": "resources [BREAK] analysis", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "USAGE"}, {"qas_id": "H93-1060.11_H93-1060.12", "question_text": "research [BREAK] computational linguistics", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "TOPIC"}, {"qas_id": "H93-1060.21_H93-1060.23", "question_text": "information [BREAK] word", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "H93-1060.34_H93-1060.36", "question_text": "Project [BREAK] dictionary", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "TOPIC"}, {"qas_id": "H93-1060.37_H93-1060.39", "question_text": "syntactic features [BREAK] words", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "H93-1060.42_H93-1060.44", "question_text": "forms [BREAK] dictionary", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "PART_WHOLE"}, {"qas_id": "H93-1060.46_H93-1060.49", "question_text": "specifications [BREAK] dictionary", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "PART_WHOLE"}, {"qas_id": "H93-1060.52_H93-1060.56", "question_text": "information [BREAK] analyzers", "context": "The COMLEX Syntax Project . \" Developing more shareable resources to support natural language analysis will make it easier and cheaper to create new language processing applications and to support research in computational linguistics . One natural candidate for such a resource is a broad-coverage dictionary , since the work required to create such a dictionary is large but there is general agreement on at least some of the information to be recorded for each word . The Linguistic Data Consortium has begun an effort to create several such lexical resources , under the rubric \"\"COMLEX\"\" ( COMmon LEXicon ); one of these projects is the COMLEX Syntax Project . The goal of the COMLEX Syntax Project is to create a moderately-broad-coverage shareable dictionary containing the syntactic features of English words , intended for automatic language analysis . We are initially aiming for a dictionary of 35,000 to 40,000 base forms , although this of course may be enlarged if the initial effort is positively received. The dictionary should include detailed syntactic specifications , particularly for subcategorization; our intent is to provide sufficient detail so that the information required by a number of major English analyzers can be automatically derived from the information we provide . As with other Linguistic Data Consortium resources , our intent is to provide a lexicon available without license constraint to all Consortium members. Finally, our goal is to provide an initial lexicon relatively quickly within about a year, funding permitting. This implies a certain flexibility , where some of the features will probably be changed and refined as the coding is taking place. \"", "tag": "USAGE"}, {"qas_id": "A97-1008.2_A97-1008.4", "question_text": "Strategies [BREAK] Verification", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "USAGE"}, {"qas_id": "A97-1008.9_A97-1008.10", "question_text": "situations [BREAK] dialog", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "PART_WHOLE"}, {"qas_id": "A97-1008.18_A97-1008.20", "question_text": "dialog system [BREAK] utterances", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "USAGE"}, {"qas_id": "A97-1008.29_A97-1008.31", "question_text": "verification [BREAK] utterances", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "USAGE"}, {"qas_id": "A97-1008.32_A97-1008.36", "question_text": "paper [BREAK] strategies", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "TOPIC"}, {"qas_id": "A97-1008.40_A97-1008.41", "question_text": "context [BREAK] selection", "context": "An Evaluation Of Strategies For Selective Utterance Verification For Spoken Natural Language Dialog . As with human-human interaction , spoken human-computer dialog will contain situations where there is miscommunication. In experimental trials consisting of eight different users , 141 problem-solving dialogs , and 2840 user utterances , the Circuit Fix-It Shop natural language dialog system misinterpreted 18.5% of user utterances . These miscommunications created various problems for the dialog interaction , ranging from repetitive dialog to experimenter intervention to occasional failure of the dialog . One natural strategy for reducing the impact of miscommunication is selective verification of the user 's utterances . This paper reports on both context-independent and context-dependent strategies for utterance verification that show that the use of dialog context is crucial for intelligent selection of which utterances to verify.", "tag": "USAGE"}, {"qas_id": "W97-1206.6_W97-1206.7", "question_text": "rules [BREAK] computation", "context": "Computing Prosodic Properties In A Data- To- Speech System . We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to- Speech system . The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system . In a Text-to- Speech system , this information would have to be obtained through text analysis , but in Data-to- Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodie realization , resulting in natural-sounding intonation .", "tag": "USAGE"}, {"qas_id": "W97-1206.13_W97-1206.17", "question_text": "syntactic information [BREAK] rules", "context": "Computing Prosodic Properties In A Data- To- Speech System . We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to- Speech system . The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system . In a Text-to- Speech system , this information would have to be obtained through text analysis , but in Data-to- Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodie realization , resulting in natural-sounding intonation .", "tag": "USAGE"}, {"qas_id": "W97-1206.23_W97-1206.24", "question_text": "text analysis [BREAK] information", "context": "Computing Prosodic Properties In A Data- To- Speech System . We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to- Speech system . The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system . In a Text-to- Speech system , this information would have to be obtained through text analysis , but in Data-to- Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodie realization , resulting in natural-sounding intonation .", "tag": "RESULT"}, {"qas_id": "W97-1206.28_W97-1206.30", "question_text": "properties [BREAK] sentences", "context": "Computing Prosodic Properties In A Data- To- Speech System . We propose a set of rules for the computation of prosody which are implemented in an existing generic Data-to- Speech system . The rules make crucial use of both sentence-internal and sentence-external semantic and syntactic information provided by the system . In a Text-to- Speech system , this information would have to be obtained through text analysis , but in Data-to- Speech it is readily available, and its reliable and detailed character makes it possible to compute the prosodie properties of generated sentences in a sophisticated way. This in turn allows for a close control of prosodie realization , resulting in natural-sounding intonation .", "tag": "MODEL-FEATURE"}, {"qas_id": "W99-0305.8_W99-0305.9", "question_text": "schemes [BREAK] dialogue acts", "context": "Standardisation Efforts On The Level Of Dialogue Act In The MATE Project . This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field . We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task , and language dependencies of schemes . We discuss solution strategies which have in mind the reusability of corpora . Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project . MATE aims to develop general methodological guidelines for the creation , annotation, retrieval and analysis of annotated corpora .", "tag": "USAGE"}, {"qas_id": "W99-0305.14_W99-0305.15", "question_text": "comparison [BREAK] schemes", "context": "Standardisation Efforts On The Level Of Dialogue Act In The MATE Project . This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field . We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task , and language dependencies of schemes . We discuss solution strategies which have in mind the reusability of corpora . Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project . MATE aims to develop general methodological guidelines for the creation , annotation, retrieval and analysis of annotated corpora .", "tag": "TOPIC"}, {"qas_id": "W99-0305.23_W99-0305.24", "question_text": "dependencies [BREAK] schemes", "context": "Standardisation Efforts On The Level Of Dialogue Act In The MATE Project . This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field . We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task , and language dependencies of schemes . We discuss solution strategies which have in mind the reusability of corpora . Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project . MATE aims to develop general methodological guidelines for the creation , annotation, retrieval and analysis of annotated corpora .", "tag": "MODEL-FEATURE"}, {"qas_id": "W99-0305.32_W99-0305.33", "question_text": "variety [BREAK] schemes", "context": "Standardisation Efforts On The Level Of Dialogue Act In The MATE Project . This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field . We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task , and language dependencies of schemes . We discuss solution strategies which have in mind the reusability of corpora . Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project . MATE aims to develop general methodological guidelines for the creation , annotation, retrieval and analysis of annotated corpora .", "tag": "MODEL-FEATURE"}, {"qas_id": "W99-0305.41_W99-0305.42", "question_text": "guidelines [BREAK] creation", "context": "Standardisation Efforts On The Level Of Dialogue Act In The MATE Project . This paper describes the state of the art of coding schemes for dialogue acts and the efforts to establish a standard in this field . We present a review and comparison of currently available schemes and outline the comparison problems we had due to domain , task , and language dependencies of schemes . We discuss solution strategies which have in mind the reusability of corpora . Reusability is a crucial point because production and annotation of corpora is very time and cost consuming but the current broad variety of schemes makes reusability of annotated corpora very hard. The work of this paper takes place in the framework of the European Union funded MATE project . MATE aims to develop general methodological guidelines for the creation , annotation, retrieval and analysis of annotated corpora .", "tag": "TOPIC"}, {"qas_id": "W00-1002.11_W00-1002.12", "question_text": "dialogues [BREAK] corpus", "context": "ADAM - An Architecture For XML-Based Dialogue Annotation On Multiple Levels . In this paper annotation modularity and use of annotation meta-schemes are identified as basic requirements for achieving actual corpora reusability. We discuss these concepts and the way they are implemented in the architectural framework of the ADAM corpus , which is a corpus of 450 Italian spontaneous dialogues . The design of ADAM architecture is compatible with as many practices of dialogue annotation as possible, as well as approaches to annotation at different levels .", "tag": "PART_WHOLE"}, {"qas_id": "W00-1213.2_W00-1213.4", "question_text": "Structures [BREAK] Texts", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "PART_WHOLE"}, {"qas_id": "W00-1213.8_W00-1213.9", "question_text": "information structures [BREAK] texts", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "MODEL-FEATURE"}, {"qas_id": "W00-1213.10_W00-1213.11", "question_text": "components [BREAK] information structure", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "PART_WHOLE"}, {"qas_id": "W00-1213.14_W00-1213.16", "question_text": "unit [BREAK] texts", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "MODEL-FEATURE"}, {"qas_id": "W00-1213.18_W00-1213.21", "question_text": "approach [BREAK] understanding", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "USAGE"}, {"qas_id": "W00-1213.22_W00-1213.24", "question_text": "paper [BREAK] overview", "context": "Annotating Information Structures In Chinese Texts Using HowNet . This paper reported our work on annotating Chinese texts with information structures derived from HowNet. An information structure consists of two components : HowNet definitions and dependency relations . It is the unit of representation of the meaning of texts . This work is part of a multi-sentential approach to Chinese text understanding . An overview of HowNet and information structure are described in this paper .", "tag": "TOPIC"}, {"qas_id": "W02-0221.4_W02-0221.6", "question_text": "dialogue acts [BREAK] schema", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "USAGE"}, {"qas_id": "W02-0221.8_W02-0221.10", "question_text": "behaviors [BREAK] dialogues", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "PART_WHOLE"}, {"qas_id": "W02-0221.11_W02-0221.13", "question_text": "effort [BREAK] utility", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "RESULT"}, {"qas_id": "W02-0221.15_W02-0221.17", "question_text": "scheme [BREAK] dialogues", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "MODEL-FEATURE"}, {"qas_id": "W02-0221.21_W02-0221.23", "question_text": "creation [BREAK] evaluating", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "USAGE"}, {"qas_id": "W02-0221.24_W02-0221.25", "question_text": "dialogue systems [BREAK] dialogues", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "COMPARE"}, {"qas_id": "W02-0221.27_W02-0221.31", "question_text": "classifier [BREAK] results", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "RESULT"}, {"qas_id": "W02-0221.33_W02-0221.35", "question_text": "utterances [BREAK] dialogues", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "PART_WHOLE"}, {"qas_id": "W02-0221.40_W02-0221.41", "question_text": "tags [BREAK] dialogue act", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "MODEL-FEATURE"}, {"qas_id": "W02-0221.44_W02-0221.47", "question_text": "DATE [BREAK] corpora", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "USAGE"}, {"qas_id": "W02-0221.53_W02-0221.55", "question_text": "data [BREAK] accuracy", "context": "Training A Dialogue Act Tagger For Human-Human And Human- Computer Travel Dialogues . While dialogue acts provide a useful schema for characterizing dialogue behaviors in human-computer and human-human dialogues , their utility is limited by the huge effort involved in hand-labelling dialogues with a dialogue act labelling scheme . In this work, we examine whether it is possible to fully automate the tagging task with the goal of enabling rapid creation of corpora for evaluating spoken dialogue systems and comparing them to human-human dialogues . We report results for training and testing an automatic classifier to label the information provider's utterances in spoken human-computer and human-human dialogues with DATE ( Dialogue Act Tagging for Evaluation ) dialogue act tags . We train and test the DATE tagger on various combinations of the DARPA Communicator June-2000 and October-2001 human-computer corpora , and the CMU human-human corpus in the travel planning domain . Our results show that we can achieve high accuracies on the humancomputer data, and surprisingly, that the human-computer data improves accuracy on the human-human data, when only small amounts of human-human training data are available.", "tag": "RESULT"}, {"qas_id": "W02-1504.1_W02-1504.2", "question_text": "Analysis [BREAK] Machine Translation", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "USAGE"}, {"qas_id": "W02-1504.4_W02-1504.8", "question_text": "components [BREAK] machine translation", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "USAGE"}, {"qas_id": "W02-1504.15_W02-1504.17", "question_text": "modules [BREAK] system", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "USAGE"}, {"qas_id": "W02-1504.18_W02-1504.19", "question_text": "analysis [BREAK] translation", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "USAGE"}, {"qas_id": "W02-1504.28_W02-1504.29", "question_text": "output [BREAK] analysis", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "COMPARE"}, {"qas_id": "W02-1504.30_W02-1504.32", "question_text": "correlation [BREAK] analysis", "context": "Machine Translation As A Testbed For Multilingual Analysis . We propose that machine translation (MT) is a useful application for evaluating and deriving the development of NL components , especially in a wide-coverage analysis system . Given the architecture of our MT system , which is a transfer system based on linguistic modules , correct analysis is expected to be a prerequisite for correct translation , suggesting a correlation between the two, given relatively mature transfer and generation components . We show through error analysis that there is indeed a strong correlation between the quality of the translated output and the subjectively determined goodness of the analysis . We use this correlation as a guide for development of a coordinated parallel analysis effort in 7 languages .", "tag": "USAGE"}, {"qas_id": "W03-0421.5_W03-0421.9", "question_text": "paper [BREAK] system", "context": "A Simple Named Entity Extractor Using AdaBoost . This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition . As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem , recognition (NER) and classification (NEC), sequentially and independently with separate modules . Both modules are machine learning based systems , which make use of binary and multiclass AdaBoost classifiers . Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme . This tagging process makes use of three binary classifiers trained to be experts", "tag": "TOPIC"}, {"qas_id": "W03-0421.13_W03-0421.16", "question_text": "recognition [BREAK] task", "context": "A Simple Named Entity Extractor Using AdaBoost . This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition . As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem , recognition (NER) and classification (NEC), sequentially and independently with separate modules . Both modules are machine learning based systems , which make use of binary and multiclass AdaBoost classifiers . Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme . This tagging process makes use of three binary classifiers trained to be experts", "tag": "PART_WHOLE"}, {"qas_id": "W03-0421.22_W03-0421.23", "question_text": "classifiers [BREAK] systems", "context": "A Simple Named Entity Extractor Using AdaBoost . This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition . As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem , recognition (NER) and classification (NEC), sequentially and independently with separate modules . Both modules are machine learning based systems , which make use of binary and multiclass AdaBoost classifiers . Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme . This tagging process makes use of three binary classifiers trained to be experts", "tag": "USAGE"}, {"qas_id": "W03-0421.26_W03-0421.30", "question_text": "procedure [BREAK] recognition", "context": "A Simple Named Entity Extractor Using AdaBoost . This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition . As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem , recognition (NER) and classification (NEC), sequentially and independently with separate modules . Both modules are machine learning based systems , which make use of binary and multiclass AdaBoost classifiers . Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme . This tagging process makes use of three binary classifiers trained to be experts", "tag": "USAGE"}, {"qas_id": "W03-0421.33_W03-0421.34", "question_text": "classifiers [BREAK] process", "context": "A Simple Named Entity Extractor Using AdaBoost . This paper presents a Named Entity Extraction (NEE) system for the CoNLL-2003 shared task competition . As in the past year edition ( Carreras et al., 2002a ), we have approached the task by treating the two main sub-tasks of the problem , recognition (NER) and classification (NEC), sequentially and independently with separate modules . Both modules are machine learning based systems , which make use of binary and multiclass AdaBoost classifiers . Named Entity recognition is performed as a greedy sequence tagging procedure under the well-known BIO labelling scheme . This tagging process makes use of three binary classifiers trained to be experts", "tag": "USAGE"}, {"qas_id": "W03-0802.1_W03-0802.2", "question_text": "Infrastructure [BREAK] Integration", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "USAGE"}, {"qas_id": "W03-0802.9_W03-0802.10", "question_text": "project [BREAK] system", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "RESULT"}, {"qas_id": "W03-0802.11_W03-0802.12", "question_text": "morphology [BREAK] parser", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "USAGE"}, {"qas_id": "W03-0802.22_W03-0802.24", "question_text": "integration [BREAK] robustness", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "RESULT"}, {"qas_id": "W03-0802.29_W03-0802.31", "question_text": "paper [BREAK] integration", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "TOPIC"}, {"qas_id": "W03-0802.33_W03-0802.35", "question_text": "integration [BREAK] benefits", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "RESULT"}, {"qas_id": "W03-0802.39_W03-0802.40", "question_text": "infrastructure [BREAK] development", "context": "WHAT: An XSLT-Based Infrastructure For The Integration Of Natural Language Processing Components . The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy. The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology , lexical , named entity , phrase chunk and (for German) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open, portable and well suited for, but not restricted to the development of hybrid NLP architectures as well as NLP applications .", "tag": "USAGE"}, {"qas_id": "W99-0907.4_W99-0907.5", "question_text": "paper [BREAK] task", "context": "Detecting Sub- Topic Correspondence Through Bipartite Term Clustering . This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments , enhancing common notions of text similarity . This task is addressed by coupling corresponding term subsets through bipartite clustering . The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method , providing illustrating results .", "tag": "TOPIC"}, {"qas_id": "W99-0907.7_W99-0907.10", "question_text": "correspondence [BREAK] fragments", "context": "Detecting Sub- Topic Correspondence Through Bipartite Term Clustering . This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments , enhancing common notions of text similarity . This task is addressed by coupling corresponding term subsets through bipartite clustering . The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method , providing illustrating results .", "tag": "PART_WHOLE"}, {"qas_id": "W99-0907.15_W99-0907.17", "question_text": "clustering [BREAK] task", "context": "Detecting Sub- Topic Correspondence Through Bipartite Term Clustering . This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments , enhancing common notions of text similarity . This task is addressed by coupling corresponding term subsets through bipartite clustering . The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method , providing illustrating results .", "tag": "USAGE"}, {"qas_id": "W99-0907.18_W99-0907.21", "question_text": "paper [BREAK] scheme", "context": "Detecting Sub- Topic Correspondence Through Bipartite Term Clustering . This paper addresses a novel task of detecting sub-topic correspondence in a pair of text fragments , enhancing common notions of text similarity . This task is addressed by coupling corresponding term subsets through bipartite clustering . The paper presents a cost-based clustering scheme and compares it with a bipartite version of the single-link method , providing illustrating results .", "tag": "TOPIC"}, {"qas_id": "W08-0327.2_W08-0327.3", "question_text": "paper [BREAK] submissions", "context": "Can we Relearn an RBMT System ? . This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French- English statistical model trained without the use of any human-translated parallel corpus . In substitution , we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus . The results are provided herein, along with a measure of error analysis .", "tag": "TOPIC"}, {"qas_id": "W08-0327.5_W08-0327.6", "question_text": "Workshop [BREAK] Statistical Machine Translation", "context": "Can we Relearn an RBMT System ? . This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French- English statistical model trained without the use of any human-translated parallel corpus . In substitution , we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus . The results are provided herein, along with a measure of error analysis .", "tag": "TOPIC"}, {"qas_id": "W08-0327.8_W08-0327.10", "question_text": "contribution [BREAK] statistical model", "context": "Can we Relearn an RBMT System ? . This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French- English statistical model trained without the use of any human-translated parallel corpus . In substitution , we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus . The results are provided herein, along with a measure of error analysis .", "tag": "TOPIC"}, {"qas_id": "W08-0327.18_W08-0327.19", "question_text": "engine [BREAK] parallel corpus", "context": "Can we Relearn an RBMT System ? . This paper describes SYSTRAN submissions for the shared task of the third Workshop on Statistical Machine Translation at ACL. Our main contribution consists in a French- English statistical model trained without the use of any human-translated parallel corpus . In substitution , we translated a monolingual corpus with SYSTRAN rule-based translation engine to produce the parallel corpus . The results are provided herein, along with a measure of error analysis .", "tag": "USAGE"}, {"qas_id": "J95-1004.2_J95-1004.5", "question_text": "Procedure [BREAK] Identification", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "USAGE"}, {"qas_id": "J95-1004.11_J95-1004.13", "question_text": "semantic interpretation [BREAK] sentence", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "MODEL-FEATURE"}, {"qas_id": "J95-1004.15_J95-1004.19", "question_text": "information [BREAK] identification", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "USAGE"}, {"qas_id": "J95-1004.24_J95-1004.25", "question_text": "order [BREAK] items", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "MODEL-FEATURE"}, {"qas_id": "J95-1004.29_J95-1004.30", "question_text": "semantic properties [BREAK] words", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "MODEL-FEATURE"}, {"qas_id": "J95-1004.31_J95-1004.32", "question_text": "algorithm [BREAK] analysis", "context": "An Automatic Procedure For Topic- Focus Identification . The dichotomy of topic and focus , based , in the Praguean Functional Generative Description , on the scale of communicative dynamism, is relevant not only for a possible placement of the sentence in a context , but also for its semantic interpretation . An automatic identification of topic and focus may use the input information on word order , on the systemic ordering of kinds of complementations (reflected by the underlying order of the items included in the focus ), on definiteness, and on lexical semantic properties of words . An algorithm for the analysis of English sentences has been implemented and is discussed and illustrated on several examples .", "tag": "USAGE"}, {"qas_id": "J96-3003.5_J96-3003.6", "question_text": "conversion [BREAK] languages", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "J96-3003.10_J96-3003.11", "question_text": "methods [BREAK] process", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "J96-3003.14_J96-3003.15", "question_text": "problems [BREAK] languages", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "MODEL-FEATURE"}, {"qas_id": "J96-3003.16_J96-3003.17", "question_text": "paper [BREAK] problem", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "TOPIC"}, {"qas_id": "J96-3003.20_J96-3003.22", "question_text": "rules [BREAK] statistics", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "COMPARE"}, {"qas_id": "J96-3003.29_J96-3003.31", "question_text": "Markov models [BREAK] features", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "MODEL-FEATURE"}, {"qas_id": "J96-3003.33_J96-3003.34", "question_text": "algorithm [BREAK] conversion", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "J96-3003.35_J96-3003.37", "question_text": "system [BREAK] corpora", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "J96-3003.40_J96-3003.41", "question_text": "algorithm [BREAK] transcription", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "J96-3003.48_J96-3003.50", "question_text": "transcriptions [BREAK] input", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "MODEL-FEATURE"}, {"qas_id": "J96-3003.51_J96-3003.52", "question_text": "system [BREAK] words", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "RESULT"}, {"qas_id": "J96-3003.59_J96-3003.61", "question_text": "system [BREAK] language", "context": "Efficient Multilingual Phoneme- To-Grapheme Conversion Based On HMM . Grapheme-to-phoneme conversion (GTPC) has been achieved in most European languages by dictionary look-up or using rules . The application of these methods , however, in the reverse process , (i.e., in phoneme-to-grapheme conversion [PTGC]) creates serious problems , especially in inflectionally rich languages . In this paper the PTGC problem is approached from a completely different point of view . Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited. The novelty lies in modeling the natural language intraword features using the theory of hidden Markov models (HMM) and performing the conversion using the Viterbi algorithm . The PTGC system has been established and tested on various multilingual corpora . Initially, the first-order HMM and the common Viterbi algorithm were used to obtain a single transcription for each word . Afterwards, the second-order HMM and the N-best algorithm adapted to PTGC were implemented to provide one or more transcriptions for each word input (homophones). This system gave an average score of more than 99% correctly transcribed words (overall success in the first four candidates ) for most of the seven languages it was tested on (Dutch, English , French, German, Greek , Italian, and Spanish). The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .", "tag": "USAGE"}, {"qas_id": "P98-2171.6_P98-2171.7", "question_text": "paper [BREAK] interface", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "TOPIC"}, {"qas_id": "P98-2171.8_P98-2171.11", "question_text": "generator [BREAK] system", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2171.12_P98-2171.13", "question_text": "phenomena [BREAK] intonation", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2171.14_P98-2171.15", "question_text": "interaction [BREAK] dependencies", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-2171.24_P98-2171.26", "question_text": "dimensions [BREAK] processing", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2171.30_P98-2171.31", "question_text": "dimensions [BREAK] phenomena", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-2171.35_P98-2171.36", "question_text": "representation [BREAK] data", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-2171.39_P98-2171.44", "question_text": "morphology [BREAK] data", "context": "From Information Structure to Intonation : A Phonological Interface for Concept-to- Speech . The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS. It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies ( projection of information structure into syntax ) and prosodie context ( performance-related modifications to intonation patterns ). Phonological processing in our system comprises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation . Phonological phenomena often touch upon more than one of these dimensions , so that mutual accessibility of the data structures on each dimension had to be ensured. We present a linear representation of the multidimensional phonological data based on a straightforward linearization convention , which suffices to bring this conceptually multilinear data set under the scope of the well-known processing techniques for two-level morphology .", "tag": "USAGE"}, {"qas_id": "P98-2175.4_P98-2175.5", "question_text": "paper [BREAK] system", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "TOPIC"}, {"qas_id": "P98-2175.9_P98-2175.10", "question_text": "entries [BREAK] dictionaries", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2175.12_P98-2175.13", "question_text": "examples [BREAK] parallel corpora", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2175.23_P98-2175.25", "question_text": "rules [BREAK] system", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "USAGE"}, {"qas_id": "P98-2175.28_P98-2175.29", "question_text": "lemmas [BREAK] words", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-2175.32_P98-2175.33", "question_text": "architecture [BREAK] dictionaries", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "USAGE"}, {"qas_id": "P98-2175.36_P98-2175.37", "question_text": "expressions [BREAK] corpora", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "PART_WHOLE"}, {"qas_id": "P98-2175.40_P98-2175.41", "question_text": "translations [BREAK] examples", "context": "An Intelligent Multi- Dictionary Environment . An open, extendible multi-dictionary system is introduced in the paper . It supports the translator in accessing adequate entries of various bi- and monolingual dictionaries and translation examples from parallel corpora . Simultaneously an unlimited number of dictionaries can be held open, thus by a single interrogation step , all the dictionaries ( translations , explanations , synonyms , etc.) can be surveyed . The implemented system ( called MoBiDic) knows morphological rules of the dictionaries ' languages . Thus, never the actual (inflected) words , but always their lemmas - that is, the right dictionary entries - are looked up. MoBiDic has an open, multimedial architecture , thus it is suitable for handling not only textual, but speaking or picture dictionaries , as well. The same system is also able to find words and expressions in corpora , dynamically providing the translators with examples from their earlier translations or other translators ' works. MoBiDic has been designed for translator workgroups, where the translators ' own glossaries (built also with the help of the system ) may also be disseminated among the members of the group, with different access rights, if needed. The system has a TCP/IP-based client-server implementation for various platforms and available with a gradually increasing number of dictionaries for numerous language pairs .", "tag": "PART_WHOLE"}, {"qas_id": "I08-1001.1_I08-1001.3", "question_text": "Method [BREAK] Information Retrieval", "context": "A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval . In Modern Mongolian, a content word can be inflected when concatenated with suffixes . Identifying the original forms of content words is crucial for natural language processing and information retrieval . We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval . We use technical abstracts to show the effectiveness of our method experimentally.", "tag": "USAGE"}, {"qas_id": "I08-1001.7_I08-1001.8", "question_text": "forms [BREAK] content words", "context": "A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval . In Modern Mongolian, a content word can be inflected when concatenated with suffixes . Identifying the original forms of content words is crucial for natural language processing and information retrieval . We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval . We use technical abstracts to show the effectiveness of our method experimentally.", "tag": "MODEL-FEATURE"}, {"qas_id": "I08-1001.14_I08-1001.15", "question_text": "method [BREAK] indexing", "context": "A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval . In Modern Mongolian, a content word can be inflected when concatenated with suffixes . Identifying the original forms of content words is crucial for natural language processing and information retrieval . We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval . We use technical abstracts to show the effectiveness of our method experimentally.", "tag": "USAGE"}, {"qas_id": "I08-1001.17_I08-1001.19", "question_text": "method [BREAK] abstracts", "context": "A Lemmatization Method for Modern Mongolian and its Application to Information Retrieval . In Modern Mongolian, a content word can be inflected when concatenated with suffixes . Identifying the original forms of content words is crucial for natural language processing and information retrieval . We propose a lemmatization method for Modern Mongolian and apply our method to indexing for information retrieval . We use technical abstracts to show the effectiveness of our method experimentally.", "tag": "USAGE"}, {"qas_id": "W04-2003.2_W04-2003.5", "question_text": "Linguistic Theory [BREAK] Parsing", "context": "A Robust And Hybrid Deep- Linguistic Theory Applied To Large- Scale Parsing . Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers . We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser . The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels , combining statistical and rule-based approaches , constituency and dependency grammar , shallow and deep processing , full and near-full parsing . With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.", "tag": "USAGE"}, {"qas_id": "W04-2003.7_W04-2003.10", "question_text": "parsers [BREAK] parsers", "context": "A Robust And Hybrid Deep- Linguistic Theory Applied To Large- Scale Parsing . Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers . We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser . The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels , combining statistical and rule-based approaches , constituency and dependency grammar , shallow and deep processing , full and near-full parsing . With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.", "tag": "COMPARE"}, {"qas_id": "W04-2003.16_W04-2003.18", "question_text": "complexity [BREAK] parser", "context": "A Robust And Hybrid Deep- Linguistic Theory Applied To Large- Scale Parsing . Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers . We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser . The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels , combining statistical and rule-based approaches , constituency and dependency grammar , shallow and deep processing , full and near-full parsing . With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.", "tag": "MODEL-FEATURE"}, {"qas_id": "W04-2003.35_W04-2003.38", "question_text": "parser [BREAK] applications", "context": "A Robust And Hybrid Deep- Linguistic Theory Applied To Large- Scale Parsing . Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers . We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser . The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels , combining statistical and rule-based approaches , constituency and dependency grammar , shallow and deep processing , full and near-full parsing . With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.", "tag": "USAGE"}, {"qas_id": "W04-2306.8_W04-2306.11", "question_text": "research project [BREAK] development", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "TOPIC"}, {"qas_id": "W04-2306.12_W04-2306.14", "question_text": "platform [BREAK] dialogue", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "USAGE"}, {"qas_id": "W04-2306.15_W04-2306.16", "question_text": "interfaces [BREAK] databases", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "PART_WHOLE"}, {"qas_id": "W04-2306.20_W04-2306.21", "question_text": "efficiency [BREAK] platform", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "MODEL-FEATURE"}, {"qas_id": "W04-2306.23_W04-2306.25", "question_text": "platform [BREAK] applications", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "USAGE"}, {"qas_id": "W04-2306.31_W04-2306.32", "question_text": "framework [BREAK] interaction", "context": "Semi- Automatic Generation Of Dialogue Applications In The GEMINI Project . GEMINI (Generic Environment for Multilingual Interactive Natural Interfaces) is an EC funded research project , which has two main objectives : First, the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and, second, the demonstration of the platform 's efficiency through the development of two different applications based on this platform : EG-Banking, a voice-portal for high-quality interactions for bank customers , and CitizenCare, an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .", "tag": "USAGE"}, {"qas_id": "W04-2501.1_W04-2501.2", "question_text": "Strategies [BREAK] Question Answering", "context": "Strategies For Advanced Question Answering . Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity ; (2) enhancing the precision of question interpretation and answer extraction ; and (3) question decomposition and answer fusion . In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.", "tag": "USAGE"}, {"qas_id": "W04-2501.4_W04-2501.5", "question_text": "strategies [BREAK] Question Answering", "context": "Strategies For Advanced Question Answering . Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity ; (2) enhancing the precision of question interpretation and answer extraction ; and (3) question decomposition and answer fusion . In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.", "tag": "USAGE"}, {"qas_id": "W04-2501.7_W04-2501.9", "question_text": "complexity [BREAK] classes", "context": "Strategies For Advanced Question Answering . Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity ; (2) enhancing the precision of question interpretation and answer extraction ; and (3) question decomposition and answer fusion . In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.", "tag": "MODEL-FEATURE"}, {"qas_id": "W04-2501.10_W04-2501.12", "question_text": "interpretation [BREAK] precision", "context": "Strategies For Advanced Question Answering . Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity ; (2) enhancing the precision of question interpretation and answer extraction ; and (3) question decomposition and answer fusion . In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.", "tag": "RESULT"}, {"qas_id": "W04-2501.17_W04-2501.18", "question_text": "paper [BREAK] impact", "context": "Strategies For Advanced Question Answering . Progress in Question Answering can be achieved by (1) combining multiple strategies that optimally resolve different question classes of various degrees of complexity ; (2) enhancing the precision of question interpretation and answer extraction ; and (3) question decomposition and answer fusion . In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.", "tag": "TOPIC"}, {"qas_id": "W05-0617.9_W05-0617.10", "question_text": "regularities [BREAK] approach", "context": "Morphology Induction From Term Clusters . We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . Like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data . This focuses the system on substrings having syntactic function , and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database .", "tag": "USAGE"}, {"qas_id": "W05-0617.14_W05-0617.15", "question_text": "rules [BREAK] correspondences", "context": "Morphology Induction From Term Clusters . We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . Like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data . This focuses the system on substrings having syntactic function , and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database .", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-0617.17_W05-0617.18", "question_text": "clusters [BREAK] data", "context": "Morphology Induction From Term Clusters . We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . Like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data . This focuses the system on substrings having syntactic function , and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database .", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-0617.26_W05-0617.28", "question_text": "system [BREAK] forms", "context": "Morphology Induction From Term Clusters . We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . Like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data . This focuses the system on substrings having syntactic function , and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database .", "tag": "USAGE"}, {"qas_id": "W05-0617.34_W05-0617.35", "question_text": "database [BREAK] approach", "context": "Morphology Induction From Term Clusters . We address the problem of learning a morphological automaton directly from a monolingual text corpus without recourse to additional resources . Like previous work in this area , our approach exploits orthographic regularities in a search for possible morphological segmentation points. Instead of affixes, however, we search for affix transformation rules that express correspondences between term clusters induced from the data . This focuses the system on substrings having syntactic function , and yields cluster-to-cluster transformation rules which enable the system to process unknown morphological forms of known words accurately. A stem-weighting algorithm based on Hubs and Authorities is used to clarify ambiguous segmentation points. We evaluate our approach using the CELEX database .", "tag": "USAGE"}, {"qas_id": "W05-0802.3_W05-0802.5", "question_text": "Models [BREAK] Text Categorization", "context": "Cross Language Text Categorization By Acquiring Multilingual Domain Models From Comparable Corpora . In a multilingual scenario , the classical monolingual text categorization problem can be reformulated as a cross language TC English Italian). English ), Italian).", "tag": "USAGE"}, {"qas_id": "W05-1514.8_W05-1514.13", "question_text": "method [BREAK] parsing", "context": "Chunk Parsing Revisited . Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking . Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/ sentence ). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .", "tag": "USAGE"}, {"qas_id": "W05-1514.15_W05-1514.17", "question_text": "classifiers [BREAK] recognition", "context": "Chunk Parsing Revisited . Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking . Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/ sentence ). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .", "tag": "USAGE"}, {"qas_id": "W05-1514.25_W05-1514.28", "question_text": "parser [BREAK] outputs", "context": "Chunk Parsing Revisited . Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking . Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/ sentence ). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .", "tag": "RESULT"}, {"qas_id": "W05-1514.32_W05-1514.33", "question_text": "method [BREAK] searching", "context": "Chunk Parsing Revisited . Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking . Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/ sentence ). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .", "tag": "USAGE"}, {"qas_id": "W05-1514.40_W05-1514.43", "question_text": "method [BREAK] accuracy", "context": "Chunk Parsing Revisited . Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding-window method and maximum entropy classifiers for phrase recognition in each level of chunking . Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/ sentence ). We also present a parsing method for searching the best parse by considering the probabilities output by the maximum entropy classifiers , and show that the search method can further improve the parsing accuracy .", "tag": "RESULT"}, {"qas_id": "W06-0205.2_W06-0205.4", "question_text": "Algorithm [BREAK] Knowledge Representation", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "USAGE"}, {"qas_id": "W06-0205.7_W06-0205.9", "question_text": "Lexical Chains [BREAK] documents", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-0205.14_W06-0205.16", "question_text": "algorithms [BREAK] English", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "USAGE"}, {"qas_id": "W06-0205.17_W06-0205.20", "question_text": "paper [BREAK] algorithm", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "TOPIC"}, {"qas_id": "W06-0205.22_W06-0205.23", "question_text": "Lexical Chains [BREAK] texts", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "PART_WHOLE"}, {"qas_id": "W06-0205.26_W06-0205.28", "question_text": "base [BREAK] texts", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "PART_WHOLE"}, {"qas_id": "W06-0205.31_W06-0205.33", "question_text": "methodology [BREAK] language", "context": "Automatic Knowledge Representation Using A Graph- Based Algorithm For Language- Independent Lexical Chaining . Lexical Chains are powerful representations of documents . In particular, they have successfully been used in the field of Automatic Text Summarization . However, until now, Lexical Chaining algorithms have only been proposed for English . In this paper , we propose a greedy Language- Independent algorithm that automatically extracts Lexical Chains from texts . For that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole- Based Overlapping Clustering Algorithm . As a consequence, our methodology can be applied to any language and proposes a solution to language-dependent Lexical Chainers.", "tag": "USAGE"}, {"qas_id": "W06-1002.6_W06-1002.7", "question_text": "research [BREAK] lexical resources", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "TOPIC"}, {"qas_id": "W06-1002.8_W06-1002.9", "question_text": "complexity [BREAK] Chinese", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-1002.25_W06-1002.26", "question_text": "lack [BREAK] Japanese", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-1002.27_W06-1002.28", "question_text": "paper [BREAK] issues", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "TOPIC"}, {"qas_id": "W06-1002.30_W06-1002.31", "question_text": "lexical resources [BREAK] NLP applications", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "USAGE"}, {"qas_id": "W06-1002.33_W06-1002.34", "question_text": "resources [BREAK] accuracy", "context": "The Role Of Lexical Resources In CJK Natural Language Processing . The role of lexical resources is often understated in NLP research . The complexity of Chinese , Japanese and Korean (CJK) poses special challenges to developers of NLP tools , especially in the area of word segmentation (WS), information retrieval (IR), named entity extraction (NER), and machine translation (MT). These difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography, especially in Japanese . This paper summarizes some of the major linguistic issues in the development NLP applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of NLP tools .", "tag": "RESULT"}, {"qas_id": "W06-1006.7_W06-1006.9", "question_text": "POS tagging [BREAK] extraction", "context": "Multilingual Collocation Extraction : Issues And Solutions . Although traditionally seen as a language-independent task , collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging , chunking or parsing ) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser . Finally, it presents a case-study on the performance of an association measure across a number of languages .", "tag": "USAGE"}, {"qas_id": "W06-1006.14_W06-1006.17", "question_text": "paper [BREAK] review", "context": "Multilingual Collocation Extraction : Issues And Solutions . Although traditionally seen as a language-independent task , collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging , chunking or parsing ) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser . Finally, it presents a case-study on the performance of an association measure across a number of languages .", "tag": "TOPIC"}, {"qas_id": "W06-1006.20_W06-1006.21", "question_text": "issues [BREAK] extraction", "context": "Multilingual Collocation Extraction : Issues And Solutions . Although traditionally seen as a language-independent task , collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging , chunking or parsing ) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser . Finally, it presents a case-study on the performance of an association measure across a number of languages .", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-1006.24_W06-1006.26", "question_text": "parser [BREAK] extraction system", "context": "Multilingual Collocation Extraction : Issues And Solutions . Although traditionally seen as a language-independent task , collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging , chunking or parsing ) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser . Finally, it presents a case-study on the performance of an association measure across a number of languages .", "tag": "USAGE"}, {"qas_id": "W06-1006.27_W06-1006.28", "question_text": "case-study [BREAK] performance", "context": "Multilingual Collocation Extraction : Issues And Solutions . Although traditionally seen as a language-independent task , collocation extraction relies nowadays more and more on the linguistic preprocessing of texts (e.g., lemmatization, POS tagging , chunking or parsing ) prior to the application of statistical measures. This paper provides a language-oriented review of the existing extraction work. It points out several language-specific issues related to extraction and proposes a strategy for coping with them. It then describes a hybrid extraction system based on a multilingual parser . Finally, it presents a case-study on the performance of an association measure across a number of languages .", "tag": "TOPIC"}, {"qas_id": "W06-1303.3_W06-1303.4", "question_text": "paper [BREAK] methods", "context": "Building Effective Question Answering Characters . In this paper , we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested , including text classification using support vector machines , language-model based retrieval , and cross-language information retrieval techniques , with the latter having the highest success rate . We also evaluated the effect of speech recognition errors on performance with users , finding that retrieval is robust until recognition reaches over 50% WER.", "tag": "TOPIC"}, {"qas_id": "W06-1303.13_W06-1303.14", "question_text": "support vector machines [BREAK] text classification", "context": "Building Effective Question Answering Characters . In this paper , we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested , including text classification using support vector machines , language-model based retrieval , and cross-language information retrieval techniques , with the latter having the highest success rate . We also evaluated the effect of speech recognition errors on performance with users , finding that retrieval is robust until recognition reaches over 50% WER.", "tag": "USAGE"}, {"qas_id": "W06-1303.20_W06-1303.22", "question_text": "techniques [BREAK] rate", "context": "Building Effective Question Answering Characters . In this paper , we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested , including text classification using support vector machines , language-model based retrieval , and cross-language information retrieval techniques , with the latter having the highest success rate . We also evaluated the effect of speech recognition errors on performance with users , finding that retrieval is robust until recognition reaches over 50% WER.", "tag": "RESULT"}, {"qas_id": "W06-1303.26_W06-1303.27", "question_text": "errors [BREAK] performance", "context": "Building Effective Question Answering Characters . In this paper , we describe methods for building and evaluation of limited domain question-answering characters. Several classification techniques are tested , including text classification using support vector machines , language-model based retrieval , and cross-language information retrieval techniques , with the latter having the highest success rate . We also evaluated the effect of speech recognition errors on performance with users , finding that retrieval is robust until recognition reaches over 50% WER.", "tag": "RESULT"}, {"qas_id": "I08-2123.2_I08-2123.5", "question_text": "Graph-based Approach [BREAK] Extraction", "context": "A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts . A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval , sentiment analysis and name disambiguation . We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts . Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name , its neighboring nodes in the graph are considered as candidates of its aliases . We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name . We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name . Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.", "tag": "USAGE"}, {"qas_id": "I08-2123.20_I08-2123.22", "question_text": "relations [BREAK] anchor texts", "context": "A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts . A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval , sentiment analysis and name disambiguation . We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts . Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name , its neighboring nodes in the graph are considered as candidates of its aliases . We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name . We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name . Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.", "tag": "PART_WHOLE"}, {"qas_id": "I08-2123.23_I08-2123.25", "question_text": "nodes [BREAK] Words", "context": "A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts . A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval , sentiment analysis and name disambiguation . We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts . Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name , its neighboring nodes in the graph are considered as candidates of its aliases . We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name . We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name . Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.", "tag": "MODEL-FEATURE"}, {"qas_id": "I08-2123.33_I08-2123.35", "question_text": "nodes [BREAK] aliases", "context": "A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts . A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval , sentiment analysis and name disambiguation . We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts . Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name , its neighboring nodes in the graph are considered as candidates of its aliases . We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name . We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name . Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.", "tag": "MODEL-FEATURE"}, {"qas_id": "I08-2123.42_I08-2123.45", "question_text": "support vector machines [BREAK] function", "context": "A Co-occurrence Graph-based Approach for Personal Name Alias Extraction from Anchor Texts . A person may have multiple name aliases on the Web. Identifying aliases of a name is important for various tasks such as information retrieval , sentiment analysis and name disambiguation . We introduce the notion of a word co-occurrence graph to represent the mutual relations between words that appear in anchor texts . Words in anchor texts are represented as nodes in the co-occurrence graph and an edge is formed between nodes which link to the same url. For a given personal name , its neighboring nodes in the graph are considered as candidates of its aliases . We formalize alias identification as a problem of ranking nodes in this graph with respect to a given name . We integrate various ranking scores through support vector machines to leverage a robust ranking function and use it to extract aliases for a given name . Experimental results on a dataset of Japanese celebrities show that the proposed method outperforms all baselines, displaying a MRR score of 0.562.", "tag": "USAGE"}, {"qas_id": "W06-2801.5_W06-2801.7", "question_text": "characteristics [BREAK] networks", "context": "Text Linkage In The Wiki Medium - A Comparative Study . We analyze four different types of document networks with respect to their small world characteristics . These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems . It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks .", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-2801.9_W06-2801.12", "question_text": "wiki-based systems [BREAK] networks", "context": "Text Linkage In The Wiki Medium - A Comparative Study . We analyze four different types of document networks with respect to their small world characteristics . These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems . It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks .", "tag": "COMPARE"}, {"qas_id": "W06-2801.13_W06-2801.15", "question_text": "study [BREAK] evidence", "context": "Text Linkage In The Wiki Medium - A Comparative Study . We analyze four different types of document networks with respect to their small world characteristics . These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems . It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks .", "tag": "TOPIC"}, {"qas_id": "W06-2801.22_W06-2801.24", "question_text": "linking [BREAK] networks", "context": "Text Linkage In The Wiki Medium - A Comparative Study . We analyze four different types of document networks with respect to their small world characteristics . These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyperlinks. The study provides evidence that a more appropriate network model is needed which better reflects the specifics of wiki systems . It puts emphasize on their topological differences as a result of wiki-related linking compared to other text-based networks .", "tag": "COMPARE"}, {"qas_id": "W06-2802.2_W06-2802.4", "question_text": "document [BREAK] challenges", "context": "Errors In Wikis . This discussion document concerns the challenges to assessments of reliability posed by wikis and the potential for language processing techniques for aiding readers to decide whether to trust particular text .", "tag": "TOPIC"}, {"qas_id": "W06-3115.1_W06-3115.3", "question_text": "System [BREAK] Shared Task", "context": "NTT System Description For The WMT2006 Shared Task . \"We present two translation systems experimented for the shared-task of \"\" Workshop on Statistical Machine Translation ,\"\" a phrase-based model and a hierarchical phrase-based model . The former uses a phrasal unit for translation , whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model . We also report a phrase / rule extraction technique differentiating tokenization of corpora . \"", "tag": "USAGE"}, {"qas_id": "W06-3115.4_W06-3115.6", "question_text": "translation systems [BREAK] shared-task", "context": "NTT System Description For The WMT2006 Shared Task . \"We present two translation systems experimented for the shared-task of \"\" Workshop on Statistical Machine Translation ,\"\" a phrase-based model and a hierarchical phrase-based model . The former uses a phrasal unit for translation , whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model . We also report a phrase / rule extraction technique differentiating tokenization of corpora . \"", "tag": "USAGE"}, {"qas_id": "W06-3115.11_W06-3115.12", "question_text": "unit [BREAK] translation", "context": "NTT System Description For The WMT2006 Shared Task . \"We present two translation systems experimented for the shared-task of \"\" Workshop on Statistical Machine Translation ,\"\" a phrase-based model and a hierarchical phrase-based model . The former uses a phrasal unit for translation , whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model . We also report a phrase / rule extraction technique differentiating tokenization of corpora . \"", "tag": "USAGE"}, {"qas_id": "W06-3115.15_W06-3115.17", "question_text": "phrase-based model [BREAK] phrase-based model", "context": "NTT System Description For The WMT2006 Shared Task . \"We present two translation systems experimented for the shared-task of \"\" Workshop on Statistical Machine Translation ,\"\" a phrase-based model and a hierarchical phrase-based model . The former uses a phrasal unit for translation , whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model . We also report a phrase / rule extraction technique differentiating tokenization of corpora . \"", "tag": "COMPARE"}, {"qas_id": "W06-3115.22_W06-3115.23", "question_text": "technique [BREAK] corpora", "context": "NTT System Description For The WMT2006 Shared Task . \"We present two translation systems experimented for the shared-task of \"\" Workshop on Statistical Machine Translation ,\"\" a phrase-based model and a hierarchical phrase-based model . The former uses a phrasal unit for translation , whereas the latter is conceptualized as a synchronous-CFG in which phrases are hierarchically combined using non-terminals. Experiments showed that the hierarchical phrase-based model performed very comparable to the phrase-based model . We also report a phrase / rule extraction technique differentiating tokenization of corpora . \"", "tag": "USAGE"}, {"qas_id": "W06-1710.3_W06-1710.4", "question_text": "paper [BREAK] approach", "context": "Web Corpus Mining By Instance Of Wikipedia . In this paper we present an approach to structure learning in the area of web documents . This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics . A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis .", "tag": "TOPIC"}, {"qas_id": "W06-1710.5_W06-1710.7", "question_text": "structure [BREAK] web documents", "context": "Web Corpus Mining By Instance Of Wikipedia . In this paper we present an approach to structure learning in the area of web documents . This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics . A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis .", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-1710.18_W06-1710.22", "question_text": "approaches [BREAK] information gain", "context": "Web Corpus Mining By Instance Of Wikipedia . In this paper we present an approach to structure learning in the area of web documents . This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics . A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis .", "tag": "RESULT"}, {"qas_id": "W06-1710.23_W06-1710.26", "question_text": "approaches [BREAK] analysis", "context": "Web Corpus Mining By Instance Of Wikipedia . In this paper we present an approach to structure learning in the area of web documents . This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics . A central outcome of the paper is that purely structure oriented approaches to web document classification provide an information gain which may be utilized in combined approaches of web content and structure analysis .", "tag": "USAGE"}, {"qas_id": "W06-1906.2_W06-1906.5", "question_text": "Classifier [BREAK] Classification", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "USAGE"}, {"qas_id": "W06-1906.7_W06-1906.10", "question_text": "Classification [BREAK] Systems", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "PART_WHOLE"}, {"qas_id": "W06-1906.13_W06-1906.15", "question_text": "machine learning [BREAK] Classifier", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "USAGE"}, {"qas_id": "W06-1906.20_W06-1906.22", "question_text": "system [BREAK] collections", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "USAGE"}, {"qas_id": "W06-1906.32_W06-1906.34", "question_text": "semantic features [BREAK] performance", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "RESULT"}, {"qas_id": "W06-1906.36_W06-1906.37", "question_text": "system [BREAK] classifications", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "RESULT"}, {"qas_id": "W06-1906.49_W06-1906.50", "question_text": "combination [BREAK] result", "context": "BRUJA: Question Classification For Spanish Using Machine Translation and An English Classifier . Question Classification is an important task in Question Answering Systems . This paper presents a Spanish Question Classifier based on machine learning , automatic online translators and different language features . Our system works with English collections and bilingual questions ( English /Spanish). We have tested two Spanish- English online translators to identify the lost of precision . We have made experiments using lexical , syntactic and semantic features to test which ones made a better performance . The obtained results show that our system makes good classifications , over a 80% in terms of accuracy using the original English questions and over a 65% using Spanish questions and machine translation systems . Our conclusion about the features is that a lexical , syntactic and semantic features combination obtains the best result .", "tag": "RESULT"}, {"qas_id": "I08-4033.4_I08-4033.9", "question_text": "method [BREAK] analyzer", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.11_I08-4033.12", "question_text": "rule-based method [BREAK] expressions", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.15_I08-4033.17", "question_text": "dictionary-based method [BREAK] words", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.22_I08-4033.23", "question_text": "approach [BREAK] results", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.28_I08-4033.30", "question_text": "tagging [BREAK] corpus", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.33_I08-4033.35", "question_text": "system [BREAK] performance", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "RESULT"}, {"qas_id": "I08-4033.36_I08-4033.38", "question_text": "word segmentation [BREAK] corpus", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.43_I08-4033.45", "question_text": "approach [BREAK] tagging", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "USAGE"}, {"qas_id": "I08-4033.46_I08-4033.48", "question_text": "approach [BREAK] post-evaluation results", "context": "Achilles: NiCT/ATR Chinese Morphological Analyzer for the Fourth Sighan Bakeoff . We created a new Chinese morphological analyzer , Achilles , by integrating rule-based , dictionary-based , and statistical machine learning method , conditional random fields (CRF). The rule-based method is used to recognize regular expressions : numbers , time and alphabets. The dictionary-based method is used to find in-vocabulary (IV) words while out-of-vocabulary (OOV) words are detected by the CRFs. At last, confidence measure based approach is used to weigh all the results and output the best ones. Achilles was used and evaluated in the bakeoff. We participated the closed tracks of word segmentation and part-of-speech tagging for all the provided corpus . In spite of an unexpected file encoding errors , the system exhibited a top level performance . A higher word segmentation accuracy for the corpus ckip and ncc were achieved. We are ranked at the fifth and eighth position out of all 19 and 26 submissions respectively for the two corpus . Achilles uses a feature combined approach for part-of-speech tagging . Our post-evaluation results prove the effectiveness of this approach for POS tagging .", "tag": "RESULT"}, {"qas_id": "P07-1078.1_P07-1078.2", "question_text": "Training [BREAK] Enhancement", "context": "Self- Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets . Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains . In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . In particular, we achieve 50% reduction in annotation cost for the in-domain case , yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case . This is the first time that self-training with small labeled datasets is applied successfully to these tasks . We were also able to formulate a characterization of when self-training is valuable.", "tag": "USAGE"}, {"qas_id": "P07-1078.7_P07-1078.10", "question_text": "data [BREAK] parsers", "context": "Self- Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets . Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains . In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . In particular, we achieve 50% reduction in annotation cost for the in-domain case , yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case . This is the first time that self-training with small labeled datasets is applied successfully to these tasks . We were also able to formulate a characterization of when self-training is valuable.", "tag": "USAGE"}, {"qas_id": "P07-1078.11_P07-1078.16", "question_text": "domains [BREAK] performance", "context": "Self- Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets . Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains . In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . In particular, we achieve 50% reduction in annotation cost for the in-domain case , yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case . This is the first time that self-training with small labeled datasets is applied successfully to these tasks . We were also able to formulate a characterization of when self-training is valuable.", "tag": "RESULT"}, {"qas_id": "P07-1078.18_P07-1078.22", "question_text": "self-training [BREAK] parser", "context": "Self- Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets . Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains . In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . In particular, we achieve 50% reduction in annotation cost for the in-domain case , yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case . This is the first time that self-training with small labeled datasets is applied successfully to these tasks . We were also able to formulate a characterization of when self-training is valuable.", "tag": "USAGE"}, {"qas_id": "P07-1078.46_P07-1078.48", "question_text": "self-training [BREAK] tasks", "context": "Self- Training for Enhancement and Domain Adaptation of Statistical Parsers Trained on Small Datasets . Creating large amounts of annotated data to train statistical PCFG parsers is expensive, and the performance of such parsers declines when training and test data are taken from different domains . In this paper we use self-training in order to improve the quality of a parser and to adapt it to a different domain , using only small amounts of manually annotated seed data . We report significant improvement both when the seed and test data are in the same domain and in the out-of-domain adaptation scenario . In particular, we achieve 50% reduction in annotation cost for the in-domain case , yielding an improvement of 66% over previous work, and a 20-33% reduction for the domain adaptation case . This is the first time that self-training with small labeled datasets is applied successfully to these tasks . We were also able to formulate a characterization of when self-training is valuable.", "tag": "USAGE"}, {"qas_id": "W05-1104.8_W05-1104.9", "question_text": "language models [BREAK] realizations", "context": "Designing an Extensible API for Integrating Language Modeling and Realization . \"We present an extensible API for integrating language modeling and realization , describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders , promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search . The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies . The n-gram models may be of any order , operate in reverse (\"\"right-to-left\"\"), and selectively replace certain words with their semantic classes . Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form , pitch accent, stem , part of speech , supertag, and semantic class . \"", "tag": "USAGE"}, {"qas_id": "W05-1104.15_W05-1104.16", "question_text": "speed [BREAK] search", "context": "Designing an Extensible API for Integrating Language Modeling and Realization . \"We present an extensible API for integrating language modeling and realization , describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders , promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search . The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies . The n-gram models may be of any order , operate in reverse (\"\"right-to-left\"\"), and selectively replace certain words with their semantic classes . Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form , pitch accent, stem , part of speech , supertag, and semantic class . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-1104.22_W05-1104.23", "question_text": "order [BREAK] n-gram models", "context": "Designing an Extensible API for Integrating Language Modeling and Realization . \"We present an extensible API for integrating language modeling and realization , describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders , promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search . The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies . The n-gram models may be of any order , operate in reverse (\"\"right-to-left\"\"), and selectively replace certain words with their semantic classes . Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form , pitch accent, stem , part of speech , supertag, and semantic class . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-1104.24_W05-1104.25", "question_text": "semantic classes [BREAK] words", "context": "Designing an Extensible API for Integrating Language Modeling and Realization . \"We present an extensible API for integrating language modeling and realization , describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders , promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search . The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies . The n-gram models may be of any order , operate in reverse (\"\"right-to-left\"\"), and selectively replace certain words with their semantic classes . Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form , pitch accent, stem , part of speech , supertag, and semantic class . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-1104.28_W05-1104.29", "question_text": "factors [BREAK] words", "context": "Designing an Extensible API for Integrating Language Modeling and Realization . \"We present an extensible API for integrating language modeling and realization , describing its design and efficient implementation in the OpenCCG surface realizer. With OpenCCG, language models may be used to select realizations with preferred word orders , promote alignment with a conversational partner, avoid repetitive language use, and increase the speed of the best-first anytime search . The API enables a variety of n-gram models to be easily combined and used in conjunction with appropriate edge pruning strategies . The n-gram models may be of any order , operate in reverse (\"\"right-to-left\"\"), and selectively replace certain words with their semantic classes . Factored language models with generalized backoff may also be employed, over words represented as bundles of factors such as form , pitch accent, stem , part of speech , supertag, and semantic class . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1237.5_L08-1237.6", "question_text": "entities [BREAK] corpora", "context": "A Semantically Annotated Swedish Medical Corpus . With the information overload in the life sciences there is an increasing need for annotated corpora , particularly with biological and biomedical entities , which is the driving force for data-driven language processing applications and the empirical approach to language study . Inspired by the work in the GENIA Corpus , MEDLEX Corpus .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1237.10_L08-1237.12", "question_text": "approach [BREAK] study", "context": "A Semantically Annotated Swedish Medical Corpus . With the information overload in the life sciences there is an increasing need for annotated corpora , particularly with biological and biomedical entities , which is the driving force for data-driven language processing applications and the empirical approach to language study . Inspired by the work in the GENIA Corpus , MEDLEX Corpus .", "tag": "USAGE"}, {"qas_id": "L08-1265.2_L08-1265.4", "question_text": "Library [BREAK] Search", "context": "Ontology Search with the OntoSelect Ontology Library . OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size ( number of classes , properties ), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies ) and human languages used for class- and object property-labels . Ontology search in OntoSelect is based on a combined measure of coverage , structure connectedness.", "tag": "USAGE"}, {"qas_id": "L08-1265.10_L08-1265.11", "question_text": "size [BREAK] ontologies", "context": "Ontology Search with the OntoSelect Ontology Library . OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size ( number of classes , properties ), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies ) and human languages used for class- and object property-labels . Ontology search in OntoSelect is based on a combined measure of coverage , structure connectedness.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1265.17_L08-1265.19", "question_text": "number [BREAK] ontologies", "context": "Ontology Search with the OntoSelect Ontology Library . OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size ( number of classes , properties ), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies ) and human languages used for class- and object property-labels . Ontology search in OntoSelect is based on a combined measure of coverage , structure connectedness.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1265.20_L08-1265.23", "question_text": "human languages [BREAK] property-labels", "context": "Ontology Search with the OntoSelect Ontology Library . OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size ( number of classes , properties ), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies ) and human languages used for class- and object property-labels . Ontology search in OntoSelect is based on a combined measure of coverage , structure connectedness.", "tag": "USAGE"}, {"qas_id": "L08-1265.25_L08-1265.27", "question_text": "coverage [BREAK] search", "context": "Ontology Search with the OntoSelect Ontology Library . OntoSelect is a dynamic web-based ontology library that harvests, analyzes and organizes ontologies published on the Semantic Web. OntoSelect allows searching as well as browsing of ontologies according to size ( number of classes , properties ), representation format (DAML, RDFS, OWL), connectedness (score over the number of included and referring ontologies ) and human languages used for class- and object property-labels . Ontology search in OntoSelect is based on a combined measure of coverage , structure connectedness.", "tag": "USAGE"}, {"qas_id": "C88-1071.2_C88-1071.3", "question_text": "Frequency [BREAK] Words", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1071.4_C88-1071.5", "question_text": "Markov Model [BREAK] Natural Language", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "USAGE"}, {"qas_id": "C88-1071.6_C88-1071.7", "question_text": "language model [BREAK] Speech recognition systems", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "PART_WHOLE"}, {"qas_id": "C88-1071.10_C88-1071.12", "question_text": "probability [BREAK] word", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1071.15_C88-1071.16", "question_text": "language models [BREAK] success", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "RESULT"}, {"qas_id": "C88-1071.20_C88-1071.21", "question_text": "probabilities [BREAK] words", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1071.29_C88-1071.31", "question_text": "English [BREAK] Corpus", "context": "Speech Recognition And The Frequency Of Recently Used Words : A Modified Markov Model For Natural Language . Speech recognition systems incorporate a language model which, at each stage of the recognition task , assigns a probability of occurrence to each word in the vocabulary . A class of Markov language models identified by Jelinek has achieved consider able success in this domain . A modification of the Markov approach , which assigns higher probabilities to recently used words , is proposed and tested against a pure Markov model . Parameter calculation and comparison of the two models both involve use of the LOB Corpus of tagged modern English .", "tag": "PART_WHOLE"}, {"qas_id": "A00-3001.10_A00-3001.12", "question_text": "paper [BREAK] interaction", "context": "Experimenting With The Interaction Between Aggregation And Text Structuring . In natural language generation , different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter ( Reiter, 1994 ). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence , and tries to explore what preferences exist among the factors related to the two tasks . The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text .", "tag": "TOPIC"}, {"qas_id": "A00-3001.18_A00-3001.19", "question_text": "factors [BREAK] tasks", "context": "Experimenting With The Interaction Between Aggregation And Text Structuring . In natural language generation , different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter ( Reiter, 1994 ). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence , and tries to explore what preferences exist among the factors related to the two tasks . The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text .", "tag": "MODEL-FEATURE"}, {"qas_id": "A00-3001.21_A00-3001.22", "question_text": "preferences [BREAK] quality", "context": "Experimenting With The Interaction Between Aggregation And Text Structuring . In natural language generation , different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter ( Reiter, 1994 ). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence , and tries to explore what preferences exist among the factors related to the two tasks . The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text .", "tag": "RESULT"}, {"qas_id": "A00-3001.25_A00-3001.26", "question_text": "generation system [BREAK] text", "context": "Experimenting With The Interaction Between Aggregation And Text Structuring . In natural language generation , different generation tasks often interact with each other in a complex way, which is hard to capture in the pipeline architecture described by Reiter ( Reiter, 1994 ). This paper focuses on the interaction between a specific type of aggregation and text planning, in particular, maintaining local coherence , and tries to explore what preferences exist among the factors related to the two tasks . The evaluation result shows that it is these preferences that decide the quality of the generated text and capturing them properly in a generation system could lead to coherent text .", "tag": "RESULT"}, {"qas_id": "H93-1065.1_H93-1065.2", "question_text": "Modeling [BREAK] Duration", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "USAGE"}, {"qas_id": "H93-1065.5_H93-1065.6", "question_text": "durations [BREAK] segments", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "MODEL-FEATURE"}, {"qas_id": "H93-1065.8_H93-1065.9", "question_text": "descriptions [BREAK] effects", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "TOPIC"}, {"qas_id": "H93-1065.14_H93-1065.15", "question_text": "paper [BREAK] system", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "TOPIC"}, {"qas_id": "H93-1065.16_H93-1065.17", "question_text": "text [BREAK] duration", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "USAGE"}, {"qas_id": "H93-1065.18_H93-1065.19", "question_text": "statistical methods [BREAK] construction", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "USAGE"}, {"qas_id": "H93-1065.21_H93-1065.22", "question_text": "experiment [BREAK] results", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "RESULT"}, {"qas_id": "H93-1065.24_H93-1065.27", "question_text": "implementation [BREAK] synthesis", "context": "Quantitative Modeling Of Segmental Duration . In natural speech , durations of phonetic segments are strongly dependent on contextual factors . Quantitative descriptions of these contextual effects have applications in text-to-speech synthesis and in automatic speech recognition . In this paper , we describe a speaker-dependent system for predicting segmental duration from text , with emphasis on the statistical methods used for its construction . We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes .", "tag": "USAGE"}, {"qas_id": "C00-1068.8_C00-1068.10", "question_text": "method [BREAK] dialogue", "context": "Flexible Mixed- Initiative Dialogue Management Using Concept- Level Confidence Measures Of Speech Recognizer Output . We present a method to realize flexible mixed-initiative dialogue , in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors . We define two concept-level CMs, which are on content-words and on semantic-attributes , using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations . Less confident interpretations are given to confirmation process . The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user 's intention and generates system-initiative guidances even when successful interpretation is not obtained.", "tag": "USAGE"}, {"qas_id": "C00-1068.11_C00-1068.12", "question_text": "system [BREAK] confirmation", "context": "Flexible Mixed- Initiative Dialogue Management Using Concept- Level Confidence Measures Of Speech Recognizer Output . We present a method to realize flexible mixed-initiative dialogue , in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors . We define two concept-level CMs, which are on content-words and on semantic-attributes , using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations . Less confident interpretations are given to confirmation process . The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user 's intention and generates system-initiative guidances even when successful interpretation is not obtained.", "tag": "USAGE"}, {"qas_id": "C00-1068.14_C00-1068.16", "question_text": "output [BREAK] confidence", "context": "Flexible Mixed- Initiative Dialogue Management Using Concept- Level Confidence Measures Of Speech Recognizer Output . We present a method to realize flexible mixed-initiative dialogue , in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors . We define two concept-level CMs, which are on content-words and on semantic-attributes , using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations . Less confident interpretations are given to confirmation process . The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user 's intention and generates system-initiative guidances even when successful interpretation is not obtained.", "tag": "USAGE"}, {"qas_id": "C00-1068.29_C00-1068.31", "question_text": "process [BREAK] interpretations", "context": "Flexible Mixed- Initiative Dialogue Management Using Concept- Level Confidence Measures Of Speech Recognizer Output . We present a method to realize flexible mixed-initiative dialogue , in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors . We define two concept-level CMs, which are on content-words and on semantic-attributes , using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations . Less confident interpretations are given to confirmation process . The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user 's intention and generates system-initiative guidances even when successful interpretation is not obtained.", "tag": "USAGE"}, {"qas_id": "C00-1068.32_C00-1068.35", "question_text": "strategy [BREAK] accuracy", "context": "Flexible Mixed- Initiative Dialogue Management Using Concept- Level Confidence Measures Of Speech Recognizer Output . We present a method to realize flexible mixed-initiative dialogue , in which the system can make effective confirmation and guidance using concept-level confidence measures (CMs) derived from speech recognizer output in order to handle speech recognition errors . We define two concept-level CMs, which are on content-words and on semantic-attributes , using 10-best outputs of the speech recognizer and parsing with phrase-level grammars. Content-word CM is useful for selecting plausible interpretations . Less confident interpretations are given to confirmation process . The strategy improved the interpretation accuracy by 11.5%. Moreover, the semantic-attribute CM is used to estimate user 's intention and generates system-initiative guidances even when successful interpretation is not obtained.", "tag": "RESULT"}, {"qas_id": "W93-0109.7_W93-0109.8", "question_text": "frequencies [BREAK] frames", "context": "The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora . We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus . A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus . In an experiment involving the identification of six fixed subcategorization frames , our current system showed more than 80% accuracy . In addition , a new statistical approach substantially improves the accuracy of the frequency estimation .", "tag": "MODEL-FEATURE"}, {"qas_id": "W93-0109.11_W93-0109.12", "question_text": "noun phrases [BREAK] corpus", "context": "The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora . We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus . A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus . In an experiment involving the identification of six fixed subcategorization frames , our current system showed more than 80% accuracy . In addition , a new statistical approach substantially improves the accuracy of the frequency estimation .", "tag": "PART_WHOLE"}, {"qas_id": "W93-0109.14_W93-0109.15", "question_text": "verb [BREAK] corpus", "context": "The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora . We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus . A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus . In an experiment involving the identification of six fixed subcategorization frames , our current system showed more than 80% accuracy . In addition , a new statistical approach substantially improves the accuracy of the frequency estimation .", "tag": "PART_WHOLE"}, {"qas_id": "W93-0109.20_W93-0109.21", "question_text": "system [BREAK] accuracy", "context": "The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora . We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus . A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus . In an experiment involving the identification of six fixed subcategorization frames , our current system showed more than 80% accuracy . In addition , a new statistical approach substantially improves the accuracy of the frequency estimation .", "tag": "RESULT"}, {"qas_id": "W93-0109.24_W93-0109.26", "question_text": "approach [BREAK] accuracy", "context": "The Automatic Acquisition Of Frequencies Of Verb Subcategorization Frames From Tagged Corpora . We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus . A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus . In an experiment involving the identification of six fixed subcategorization frames , our current system showed more than 80% accuracy . In addition , a new statistical approach substantially improves the accuracy of the frequency estimation .", "tag": "RESULT"}, {"qas_id": "C90-3053.3_C90-3053.4", "question_text": "Generation [BREAK] Machine Translation", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "USAGE"}, {"qas_id": "C90-3053.5_C90-3053.8", "question_text": "paper [BREAK] translation", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "TOPIC"}, {"qas_id": "C90-3053.11_C90-3053.12", "question_text": "prepositions [BREAK] languages", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "PART_WHOLE"}, {"qas_id": "C90-3053.18_C90-3053.21", "question_text": "principles [BREAK] process", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "USAGE"}, {"qas_id": "C90-3053.23_C90-3053.24", "question_text": "semantic features [BREAK] translation system", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "USAGE"}, {"qas_id": "C90-3053.26_C90-3053.27", "question_text": "distinctions [BREAK] configurations", "context": "The Semantic Representation Of Spatial Configurations: A Conceptual Motivation For Generation In Machine Translation . This paper deals with the automatic translation of prepositions , which are highly polysemous. Moreover, the same real situation is often expressed by different prepositions in different languages . We proceed from the hypothesis that different usage patterns are due to different conceptualizations of the same real situation . Following cognitive principles of spatial conceptualization, we design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy. Thus we can differentiate subtle distinctions between spatially significant configurations .", "tag": "MODEL-FEATURE"}, {"qas_id": "W97-0210.4_W97-0210.5", "question_text": "approach [BREAK] tagging", "context": "Investigating Complementary Methods For Verb Sense Pruning . We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters . Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method .", "tag": "USAGE"}, {"qas_id": "W97-0210.9_W97-0210.11", "question_text": "alternations [BREAK] method", "context": "Investigating Complementary Methods For Verb Sense Pruning . We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters . Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method .", "tag": "USAGE"}, {"qas_id": "W97-0210.13_W97-0210.16", "question_text": "clusters [BREAK] method", "context": "Investigating Complementary Methods For Verb Sense Pruning . We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters . Initial results indicate that verb senses can be pruned for highly polysemous verbs by up to 74% by the first method and by up to 85% by the second method .", "tag": "USAGE"}, {"qas_id": "W02-0817.1_W02-0817.2", "question_text": "Sense [BREAK] Corpus", "context": "Building A Sense Tagged Corpus With Open Mind Word Expert . Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert . If successful, the collection process can be extended to create the definitive corpus of word sense information .", "tag": "MODEL-FEATURE"}, {"qas_id": "W02-0817.9_W02-0817.11", "question_text": "system [BREAK] tagging", "context": "Building A Sense Tagged Corpus With Open Mind Word Expert . Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert . If successful, the collection process can be extended to create the definitive corpus of word sense information .", "tag": "USAGE"}, {"qas_id": "W02-0817.12_W02-0817.17", "question_text": "system [BREAK] data", "context": "Building A Sense Tagged Corpus With Open Mind Word Expert . Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web. It is available at http://teach-computers.org. We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers. We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert . If successful, the collection process can be extended to create the definitive corpus of word sense information .", "tag": "RESULT"}, {"qas_id": "W98-1126.4_W98-1126.5", "question_text": "paper [BREAK] interactions", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "TOPIC"}, {"qas_id": "W98-1126.6_W98-1126.8", "question_text": "properties [BREAK] features", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "USAGE"}, {"qas_id": "W98-1126.10_W98-1126.14", "question_text": "experiments [BREAK] task", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "USAGE"}, {"qas_id": "W98-1126.15_W98-1126.16", "question_text": "organizations [BREAK] properties", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "USAGE"}, {"qas_id": "W98-1126.17_W98-1126.19", "question_text": "paper [BREAK] analysis", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "TOPIC"}, {"qas_id": "W98-1126.20_W98-1126.22", "question_text": "algorithms [BREAK] results", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "RESULT"}, {"qas_id": "W98-1126.28_W98-1126.29", "question_text": "analysis [BREAK] relationship", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "TOPIC"}, {"qas_id": "W98-1126.30_W98-1126.31", "question_text": "investigation [BREAK] criteria", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "TOPIC"}, {"qas_id": "W98-1126.33_W98-1126.35", "question_text": "properties [BREAK] experiments", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "USAGE"}, {"qas_id": "W98-1126.37_W98-1126.39", "question_text": "properties [BREAK] features", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "USAGE"}, {"qas_id": "W98-1126.40_W98-1126.41", "question_text": "investigations [BREAK] interaction", "context": "Mapping Collocational Properties Into Machine Learning Features . This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning . In experiments performing an event categorization task , Wiebe et al. (1997a) found that different organizations are best for different properties . This paper presents a statistical analysis of the results across different machine learning algorithms . In the experiments , the relationship between property and organization was strikingly consistent across algorithms . This prompted further analysis of this relationship , and an investigation of criteria for recognizing beneficial ways to include collocational properties in machine learning experiments . While many types of collocational properties and methods of organizing them into features have been used in NLP, systematic investigations of their interaction are rare.", "tag": "TOPIC"}, {"qas_id": "L08-1203.3_L08-1203.4", "question_text": "System [BREAK] Interpretation", "context": "KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data . This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .", "tag": "USAGE"}, {"qas_id": "L08-1203.8_L08-1203.9", "question_text": "paper [BREAK] architecture", "context": "KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data . This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .", "tag": "TOPIC"}, {"qas_id": "L08-1203.12_L08-1203.14", "question_text": "information [BREAK] scientific literature", "context": "KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data . This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1203.16_L08-1203.17", "question_text": "techniques [BREAK] abstracts", "context": "KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data . This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .", "tag": "USAGE"}, {"qas_id": "L08-1203.27_L08-1203.28", "question_text": "standards [BREAK] fusion", "context": "KnoFusius: a New Knowledge Fusion System for Interpretation of Gene Expression Data . This paper introduces a new architecture that aims at combining molecular biology data with information automatically extracted from relevant scientific literature ( using text mining techniques on PubMed abstracts and fulltext papers ) to help biomedical experts to interpret experimental results in hand . The infrastructural level bears on semantic-web technologies and standards that facilitate the actual fusion of the multi-source knowledge .", "tag": "USAGE"}, {"qas_id": "L08-1204.20_L08-1204.21", "question_text": "paper [BREAK] vector-based models", "context": "Modelling Word Similarity : an Evaluation of Automatic Synonymy Extraction Algorithms . . Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts . Despite their increasing popularity , it is unclear which kind of semantic similarity they actually capture and for which kind of words . In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic properties of the nouns influence the results . In particular, we compare results from a dependency-based model with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns ' frequency , semantic speficity and semantic class . We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence .", "tag": "TOPIC"}, {"qas_id": "L08-1204.24_L08-1204.27", "question_text": "properties [BREAK] results", "context": "Modelling Word Similarity : an Evaluation of Automatic Synonymy Extraction Algorithms . . Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts . Despite their increasing popularity , it is unclear which kind of semantic similarity they actually capture and for which kind of words . In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic properties of the nouns influence the results . In particular, we compare results from a dependency-based model with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns ' frequency , semantic speficity and semantic class . We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence .", "tag": "RESULT"}, {"qas_id": "L08-1204.28_L08-1204.29", "question_text": "dependency-based model [BREAK] results", "context": "Modelling Word Similarity : an Evaluation of Automatic Synonymy Extraction Algorithms . . Vector-based models of lexical semantics retrieve semantically related words automatically from large corpora by exploiting the property that words with a similar meaning tend to occur in similar contexts . Despite their increasing popularity , it is unclear which kind of semantic similarity they actually capture and for which kind of words . In this paper , we use three vector-based models to retrieve semantically related words for a set of Dutch nouns and we analyse whether three linguistic properties of the nouns influence the results . In particular, we compare results from a dependency-based model with those from a 1st and 2nd order bag-of-words model and we examine the effect of the nouns ' frequency , semantic speficity and semantic class . We find that all three models find more synonyms for high-frequency nouns and those belonging to abstract semantic classses. Semantic specificty does not have a clear influence .", "tag": "RESULT"}, {"qas_id": "L08-1205.10_L08-1205.12", "question_text": "speech [BREAK] database", "context": "Childrens Oral Reading Corpus (CHOREC): Description and Assessment of Annotator Agreement . Within the scope of the SPACE project , the CHildren's Oral REading Corpus (CHOREC) is developed . This database contains recorded , transcribed and annotated read speech (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties . Analyses of inter- and intra-annotator agreement are carried out in order to investigate the consistency with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10% resp. 30% of the corpus ) together, % agreement varies between 86.4% and 98.6%, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement . To conclude, an analysis of the annotation differences with respect to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling behaviour ), phoneme labels, reading strategy and error labels is given.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1205.14_L08-1205.17", "question_text": "Analyses [BREAK] consistency", "context": "Childrens Oral Reading Corpus (CHOREC): Description and Assessment of Annotator Agreement . Within the scope of the SPACE project , the CHildren's Oral REading Corpus (CHOREC) is developed . This database contains recorded , transcribed and annotated read speech (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties . Analyses of inter- and intra-annotator agreement are carried out in order to investigate the consistency with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10% resp. 30% of the corpus ) together, % agreement varies between 86.4% and 98.6%, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement . To conclude, an analysis of the annotation differences with respect to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling behaviour ), phoneme labels, reading strategy and error labels is given.", "tag": "TOPIC"}, {"qas_id": "L08-1205.32_L08-1205.33", "question_text": "type [BREAK] differences", "context": "Childrens Oral Reading Corpus (CHOREC): Description and Assessment of Annotator Agreement . Within the scope of the SPACE project , the CHildren's Oral REading Corpus (CHOREC) is developed . This database contains recorded , transcribed and annotated read speech (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties . Analyses of inter- and intra-annotator agreement are carried out in order to investigate the consistency with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10% resp. 30% of the corpus ) together, % agreement varies between 86.4% and 98.6%, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement . To conclude, an analysis of the annotation differences with respect to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling behaviour ), phoneme labels, reading strategy and error labels is given.", "tag": "RESULT"}, {"qas_id": "L08-1205.38_L08-1205.39", "question_text": "analysis [BREAK] differences", "context": "Childrens Oral Reading Corpus (CHOREC): Description and Assessment of Annotator Agreement . Within the scope of the SPACE project , the CHildren's Oral REading Corpus (CHOREC) is developed . This database contains recorded , transcribed and annotated read speech (42 GB or 130 hours) of 400 Dutch speaking elementary school children with or without reading difficulties . Analyses of inter- and intra-annotator agreement are carried out in order to investigate the consistency with which reading errors are detected, orthographic and phonetic transcriptions are made, and reading errors and reading strategies are labeled. Percentage agreement scores and kappa values both show that agreement between annotations, and therefore the quality of the annotations, is high. Taken all double or triple annotations (for 10% resp. 30% of the corpus ) together, % agreement varies between 86.4% and 98.6%, whereas kappa varies between 0.72 and 0.97 depending on the annotation tier that is being assessed. School type and reading type seem to account for systematic differences in % agreement , but these differences disappear when kappa values are calculated that correct for chance agreement . To conclude, an analysis of the annotation differences with respect to the '*s' label (i.e. a label that is used to annotate undistinguishable spelling behaviour ), phoneme labels, reading strategy and error labels is given.", "tag": "TOPIC"}, {"qas_id": "L08-1206.1_L08-1206.2", "question_text": "Events [BREAK] Corpus", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1206.3_L08-1206.4", "question_text": "paper [BREAK] creation", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "TOPIC"}, {"qas_id": "L08-1206.5_L08-1206.6", "question_text": "events [BREAK] corpus", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1206.14_L08-1206.15", "question_text": "analysis [BREAK] phenomena", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "TOPIC"}, {"qas_id": "L08-1206.20_L08-1206.21", "question_text": "analysis [BREAK] texts", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "TOPIC"}, {"qas_id": "L08-1206.23_L08-1206.24", "question_text": "methodology [BREAK] construction", "context": "A Bilingual Corpus of Inter-linked Events . This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English . Linkage is accomplished through the Inter- Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource , on the one hand , enables contrastive analysis of the linguistic phenomena surrounding events in both languages , and on the other hand , can be used to perform multilingual temporal analysis of texts . In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense .", "tag": "USAGE"}, {"qas_id": "L08-1207.18_L08-1207.19", "question_text": "translation [BREAK] documents", "context": "New Resources for Document Classification , Analysis and Translation Technologies . The goal of the DARPA MADCAT (Multilingual Automatic Document Classification Analysis and Translation ) Program is to automatically convert foreign language text images into English transcripts , for use by humans and downstream applications . The first phase the program focuses on translation of handwritten Arabic documents . Linguistic Data Consortium (LDC) is creating publicly available linguistic resources for MADCAT technologies , on a scale and richness not previously available. Corpora will consist of existing LDC corpora and data donations from MADCAT partners, plus new data collection to provide high quality material for evaluation and to address strategic gaps (for genre , dialect , image quality , etc.) in the existing resources . Training and test data properties will expand over time to encompass a wide range of topics and genres : letters, diaries, training manuals , brochures, signs, ledgers, memos, instructions , post cards and forms among others. Data will be ground truthed, with line, word and token segmentation and zoning, and translations and word alignments will be produced for a subset. Evaluation data will be carefully selected from the available data pools and high quality references will be produced, which can be used to compare MADCAT system performance against the human-produced gold standard .", "tag": "USAGE"}, {"qas_id": "L08-1207.59_L08-1207.60", "question_text": "performance [BREAK] gold standard", "context": "New Resources for Document Classification , Analysis and Translation Technologies . The goal of the DARPA MADCAT (Multilingual Automatic Document Classification Analysis and Translation ) Program is to automatically convert foreign language text images into English transcripts , for use by humans and downstream applications . The first phase the program focuses on translation of handwritten Arabic documents . Linguistic Data Consortium (LDC) is creating publicly available linguistic resources for MADCAT technologies , on a scale and richness not previously available. Corpora will consist of existing LDC corpora and data donations from MADCAT partners, plus new data collection to provide high quality material for evaluation and to address strategic gaps (for genre , dialect , image quality , etc.) in the existing resources . Training and test data properties will expand over time to encompass a wide range of topics and genres : letters, diaries, training manuals , brochures, signs, ledgers, memos, instructions , post cards and forms among others. Data will be ground truthed, with line, word and token segmentation and zoning, and translations and word alignments will be produced for a subset. Evaluation data will be carefully selected from the available data pools and high quality references will be produced, which can be used to compare MADCAT system performance against the human-produced gold standard .", "tag": "COMPARE"}, {"qas_id": "L08-1208.15_L08-1208.17", "question_text": "question [BREAK] performance", "context": "Approximating Learning Curves for Active- Learning- Driven Annotation . Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks . A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts . This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance , this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.", "tag": "RESULT"}, {"qas_id": "L08-1208.32_L08-1208.33", "question_text": "method [BREAK] corpus", "context": "Approximating Learning Curves for Active- Learning- Driven Annotation . Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks . A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts . This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance , this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.", "tag": "USAGE"}, {"qas_id": "L08-1208.38_L08-1208.43", "question_text": "approach [BREAK] recognition", "context": "Approximating Learning Curves for Active- Learning- Driven Annotation . Active learning (AL) is getting more and more popular as a methodology to considerably reduce the annotation effort when building training material for statistical learning methods for various NLP tasks . A crucial issue rarely addressed, however, is when to actually stop the annotation process to profit from the savings in efforts . This question is tightly related to estimating the classifier performance after a certain amount of data has already been annotated. While learning curves are the default means to monitor the progress of the annotation process in terms of classifier performance , this requires a labeled gold standard which - in realistic annotation settings, at least - is often unavailable. We here propose a method for committee-based AL to approximate the progression of the learning curve based on the disagreement among the committee members. This method relies on a separate, unlabeled corpus and is thus well suited for situations where a labeled gold standard is not available or would be too expensive to obtain. Considering named entity recognition as a test case we provide empirical evidence that this approach works well under simulation as well as under real-world annotation conditions.", "tag": "USAGE"}, {"qas_id": "L08-1209.6_L08-1209.7", "question_text": "paper [BREAK] schemas", "context": "Lexicon Schemas and Related Data Models : when Standards Meet Users . Lexicon schemas and their use are discussed in this paper from the perspective of lexicographers and field linguists . A variety of lexicon schemas have been developed , with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and compared to each other in terms of conversion and usability for this particular user group, using a common lexicon entry and providing examples for each schema under consideration . The formats are assessed and the final recommendation is given for the potential users , namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards , lexicographers and field linguists .", "tag": "TOPIC"}, {"qas_id": "L08-1209.13_L08-1209.17", "question_text": "schemas [BREAK] archiving", "context": "Lexicon Schemas and Related Data Models : when Standards Meet Users . Lexicon schemas and their use are discussed in this paper from the perspective of lexicographers and field linguists . A variety of lexicon schemas have been developed , with goals ranging from computational lexicography (DATR) through archiving (LIFT, TEI) to standardization (LMF, FSR). A number of requirements for lexicon schemas are given. The lexicon schemas are introduced and compared to each other in terms of conversion and usability for this particular user group, using a common lexicon entry and providing examples for each schema under consideration . The formats are assessed and the final recommendation is given for the potential users , namely to request standard compliance from the developers of the tools used. This paper should foster a discussion between authors of standards , lexicographers and field linguists .", "tag": "USAGE"}, {"qas_id": "L08-1211.2_L08-1211.3", "question_text": "Inference [BREAK] Extensions", "context": "Arabic WordNet: Semi-automatic Extensions using Bayesian Inference . This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .", "tag": "USAGE"}, {"qas_id": "L08-1211.4_L08-1211.7", "question_text": "presentation [BREAK] extension", "context": "Arabic WordNet: Semi-automatic Extensions using Bayesian Inference . This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .", "tag": "TOPIC"}, {"qas_id": "L08-1211.24_L08-1211.27", "question_text": "rules [BREAK] generation", "context": "Arabic WordNet: Semi-automatic Extensions using Bayesian Inference . This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .", "tag": "USAGE"}, {"qas_id": "L08-1211.36_L08-1211.38", "question_text": "Network [BREAK] mechanism", "context": "Arabic WordNet: Semi-automatic Extensions using Bayesian Inference . This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .", "tag": "USAGE"}, {"qas_id": "L08-1211.43_L08-1211.45", "question_text": "approach [BREAK] results", "context": "Arabic WordNet: Semi-automatic Extensions using Bayesian Inference . This presentation focuses on the semi-automatic extension of Arabic WordNet (AWN) using lexical and morphological rules and applying Bayesian inference . We briefly report on the current status of AWN and propose a way of extending its coverage by taking advantage of a limited set of highly productive Arabic morphological rules for deriving a range of semantically related word forms from verb entries . The application of this set of rules , combined with the use of bilingual Arabic- English resources and Princeton's WordNet, allows the generation of a graph representing the semantic neighbourhood of the original word . In previous work, a set of associations between the hypothesized Arabic words and English synsets was proposed on the basis of this graph. Here, a novel approach to extending AWN is presented whereby a Bayesian Network is automatically built from the graph and then the net is used as an inferencing mechanism for scoring the set of candidate associations . Both on its own and in combination with the previous technique , this new approach has led to improved results .", "tag": "RESULT"}, {"qas_id": "L08-1212.4_L08-1212.6", "question_text": "paper [BREAK] process", "context": "Subjective Evaluation of an Emotional Speech Database for Basque ///q. This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis . The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors. The test results show that every emotion is readily recognized far above chance level for both speakers. Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.", "tag": "TOPIC"}, {"qas_id": "L08-1212.16_L08-1212.17", "question_text": "sentences [BREAK] corpus", "context": "Subjective Evaluation of an Emotional Speech Database for Basque ///q. This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis . The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors. The test results show that every emotion is readily recognized far above chance level for both speakers. Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1212.24_L08-1212.28", "question_text": "database [BREAK] purposes", "context": "Subjective Evaluation of an Emotional Speech Database for Basque ///q. This paper describes the evaluation process of an emotional speech database recorded for standard Basque, in order to determine its adequacy for the analysis of emotional models and its use in speech synthesis . The corpus consists of seven hundred semantically neutral sentences that were recorded for the Big Six emotions and neutral style, by two professional actors. The test results show that every emotion is readily recognized far above chance level for both speakers. Therefore the database is a valid linguistic resource for the research and development purposes it was designed for.", "tag": "USAGE"}, {"qas_id": "L08-1213.46_L08-1213.47", "question_text": "schemes [BREAK] constructions", "context": "How to Compare Treebanks . Recent years have seen an increasing interest in developing standards for linguistic annotation, with a focus on the interoperability of the resources . This effort , however, requires a profound knowledge of the advantages and disadvantages of linguistic annotation schemes in order to avoid importing the flaws and weaknesses of existing encoding schemes into the new standards . This paper addresses the question how to compare syntactically annotated corpora and gain insights into the usefulness of specific design decisions . We present an exhaustive evaluation of two German treebanks with crucially different encoding schemes . We evaluate three different parsers trained on the two treebanks and compare results using EvalB, the Leaf-Ancestor metric , and a dependency-based evaluation . Furthermore, we present TePaCoC, a new testsuite for the evaluation of parsers on complex German grammatical constructions . The testsuite provides a well thought-out error classification , which enables us to compare parser output for parsers trained on treebanks with different encoding schemes and provides interesting insights into the impact of treebank annotation schemes on specific constructions like PP attachment or non-constituent coordination .", "tag": "RESULT"}, {"qas_id": "L08-1214.14_L08-1214.15", "question_text": "collection [BREAK] corpus", "context": "The INFILE Project : a Crosslingual Filtering Systems Evaluation Campaign . The InFile project ( INformation , FILtering, Evaluation ) is a cross-language adaptive filtering evaluation campaign, sponsored by the French National Research Agency. The campaign is organized by the CEA LIST , ELDA and the University of Lille3-GERiiCO. It has an international scope as it is a pilot track of the CLEF 2008 campaigns. The corpus is built from a collection of about 1,4 millions newswires (10 GB) in three languages , Arabic, English and French provided by Agence France Press (AFP) and selected from a 3 years period. The profiles corpus is made of 50 profiles from which 30 concern general news and events (national and international affairs, politics, sports ...) and 20 concern scientific and technical subjects.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1214.19_L08-1214.22", "question_text": "events [BREAK] corpus", "context": "The INFILE Project : a Crosslingual Filtering Systems Evaluation Campaign . The InFile project ( INformation , FILtering, Evaluation ) is a cross-language adaptive filtering evaluation campaign, sponsored by the French National Research Agency. The campaign is organized by the CEA LIST , ELDA and the University of Lille3-GERiiCO. It has an international scope as it is a pilot track of the CLEF 2008 campaigns. The corpus is built from a collection of about 1,4 millions newswires (10 GB) in three languages , Arabic, English and French provided by Agence France Press (AFP) and selected from a 3 years period. The profiles corpus is made of 50 profiles from which 30 concern general news and events (national and international affairs, politics, sports ...) and 20 concern scientific and technical subjects.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1215.5_L08-1215.6", "question_text": "string [BREAK] language", "context": "DIAC+: a Professional Diacritics Recovering System . In languages that use diacritical characters, if these special signs are stripped-off from a word , the resulted string of characters may not exist in the language , and therefore its normative form is, in general, easy to recover. However, this is not always the case , as presence or absence of a diacritical sign attached to a base letter of a word which exists in both variants , may change its grammatical properties or even the meaning, making the recovery of the missing diacritics a difficult task , not only for a program but sometimes even for a human reader. We describe and evaluate an accurate knowledge-based system for automatic recovering the missing diacritics in MS-Office documents written in Romanian. For the rare cases when the system is not able to reliably make a decision ,it either provides the user a list of words with their recovery suggestions , or probabilistically choose one of the possible changes, but leaves a trace (a highlighted comment) on each word the modification of which was uncertain.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1216.3_L08-1216.4", "question_text": "paper [BREAK] project", "context": "Annotating an Arabic Learner Corpus for Error . This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.", "tag": "TOPIC"}, {"qas_id": "L08-1216.10_L08-1216.11", "question_text": "Error Analysis [BREAK] data", "context": "Annotating an Arabic Learner Corpus for Error . This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.", "tag": "USAGE"}, {"qas_id": "L08-1216.18_L08-1216.20", "question_text": "difficulty [BREAK] languages", "context": "Annotating an Arabic Learner Corpus for Error . This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1216.22_L08-1216.24", "question_text": "advanced-level [BREAK] collection", "context": "Annotating an Arabic Learner Corpus for Error . This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1216.25_L08-1216.26", "question_text": "data [BREAK] corpora", "context": "Annotating an Arabic Learner Corpus for Error . This paper describes an ongoing project in which we are collecting a learner corpus of Arabic, developing a tagset for error annotation and performing Computer-aided Error Analysis (CEA) on the data . We adapted the French Interlanguage Database FRIDA tagset ( Granger, 2003a ) to the data . We chose FRIDA in order to follow a known standard and to see whether the changes needed to move from a French to an Arabic tagset would give us a measure of the distance between the two languages with respect to learner difficulty . The current collection of texts , which is constantly growing, contains intermediate and advanced-level student writings . We describe the need for such corpora , the learner data we have collected and the tagset we have developed . We also describe the error frequency distribution of both proficiency levels and the ongoing work.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1217.1_L08-1217.2", "question_text": "Correction [BREAK] OCR-Error", "context": "All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation . Some time in the future, some spelling error correction system will correct all the errors , and only the errors . We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal . We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision , as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path . We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one's evaluations . We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .", "tag": "USAGE"}, {"qas_id": "L08-1217.8_L08-1217.9", "question_text": "system [BREAK] errors", "context": "All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation . Some time in the future, some spelling error correction system will correct all the errors , and only the errors . We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal . We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision , as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path . We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one's evaluations . We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .", "tag": "USAGE"}, {"qas_id": "L08-1217.14_L08-1217.17", "question_text": "evaluation [BREAK] current", "context": "All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation . Some time in the future, some spelling error correction system will correct all the errors , and only the errors . We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal . We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision , as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path . We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one's evaluations . We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .", "tag": "TOPIC"}, {"qas_id": "L08-1217.40_L08-1217.41", "question_text": "metrics [BREAK] Accuracy", "context": "All, and only, the Errors: more Complete and Consistent Spelling and OCR-Error Correction Evaluation . Some time in the future, some spelling error correction system will correct all the errors , and only the errors . We need evaluation metrics that will tell us when this has been achieved and that can help guide us there. We survey the current practice in the form of the evaluation scheme of the latest major publication on spelling correction in a leading journal . We are forced to conclude that while the metric used there can tell us exactly when the ultimate goal of spelling correction research has been achieved, it offers little in the way of directions to be followed to eventually get there. We propose to consistently use the well-known metrics Recall and Precision , as combined in the F score, on 5 possible levels of measurement that should guide us more informedly along that path . We describe briefly what is then measured or measurable at these levels and propose a framework that should allow for concisely stating what it is one performs in one's evaluations . We finally contrast our preferred metrics to Accuracy , which is widely used in this field to this day and to the Area- Under-the-Curve, which is increasingly finding acceptance in other fields .", "tag": "COMPARE"}, {"qas_id": "L08-1218.3_L08-1218.4", "question_text": "paper [BREAK] method", "context": "Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora . This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate .", "tag": "TOPIC"}, {"qas_id": "L08-1218.6_L08-1218.7", "question_text": "database [BREAK] corpus", "context": "Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora . This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1218.10_L08-1218.15", "question_text": "algorithm [BREAK] algorithm", "context": "Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora . This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate .", "tag": "USAGE"}, {"qas_id": "L08-1218.16_L08-1218.18", "question_text": "information [BREAK] algorithm", "context": "Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora . This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate .", "tag": "USAGE"}, {"qas_id": "L08-1218.26_L08-1218.27", "question_text": "time [BREAK] alignment", "context": "Using Movie Subtitles for Creating a Large- Scale Bilingual Corpora . This paper presents a method for compiling a large-scale bilingual corpus from a database of movie subtitles . To create the corpus , we propose an algorithm based on Gale and Church's sentence alignment algorithm (1993). However, our algorithm not only relies on character length information , but also uses subtitle-timing information , which is encoded in the subtitle files. Timing is highly correlated between subtitles in different versions (for the same movie), since subtitles that match should be displayed at the same time . However, the absolute time values can't be used for alignment , since the timing is usually specified by frame numbers and not by real time , and converting it to real time values is not always possible, hence we use normalized subtitle duration instead . This results in a significant reduction in the alignment error rate .", "tag": "USAGE"}, {"qas_id": "L08-1219.12_L08-1219.13", "question_text": "speech [BREAK] Corpora", "context": "The IFADV Corpus : a Free Dialog Video Corpus . \" Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures , gaze, and facial expression . Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions . A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech . Within the bounds of the law, everything has been done to remove copyright and use restrictions . Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software . From our experiences we would like to advocate the formulation of \"\"best practises\"\" for both legal handling and database storage of recordings and annotations. \"", "tag": "PART_WHOLE"}, {"qas_id": "L08-1219.16_L08-1219.18", "question_text": "video [BREAK] corpus", "context": "The IFADV Corpus : a Free Dialog Video Corpus . \" Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures , gaze, and facial expression . Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions . A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech . Within the bounds of the law, everything has been done to remove copyright and use restrictions . Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software . From our experiences we would like to advocate the formulation of \"\"best practises\"\" for both legal handling and database storage of recordings and annotations. \"", "tag": "PART_WHOLE"}, {"qas_id": "L08-1219.30_L08-1219.32", "question_text": "recordings [BREAK] database", "context": "The IFADV Corpus : a Free Dialog Video Corpus . \" Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures , gaze, and facial expression . Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions . A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech . Within the bounds of the law, everything has been done to remove copyright and use restrictions . Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software . From our experiences we would like to advocate the formulation of \"\"best practises\"\" for both legal handling and database storage of recordings and annotations. \"", "tag": "PART_WHOLE"}, {"qas_id": "L08-1220.3_L08-1220.4", "question_text": "paper [BREAK] data", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "TOPIC"}, {"qas_id": "L08-1220.11_L08-1220.12", "question_text": "interface [BREAK] control", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "USAGE"}, {"qas_id": "L08-1220.14_L08-1220.16", "question_text": "acquisition [BREAK] signal <entity id=\"L08-1220.17\">processing", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "USAGE"}, {"qas_id": "L08-1220.19_L08-1220.20", "question_text": "database [BREAK] pre-processing", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "USAGE"}, {"qas_id": "L08-1220.22_L08-1220.25", "question_text": "scenarios [BREAK] experiments", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "USAGE"}, {"qas_id": "L08-1220.35_L08-1220.37", "question_text": "events [BREAK] database", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1220.38_L08-1220.39", "question_text": "video [BREAK] compute", "context": "WOZ Acoustic Data Collection for Interactive TV . This paper describes a multichannel acoustic data collection recorded under the European DICIT project , during the Wizard of Oz (WOZ) experiments carried out at FAU and FBK-irst laboratories . The scenario is a distant-talking interface for interactive control of a TV. The experiments involve the acquisition of multichannel data for signal <entity id=\"L08-1220.17\">processing </entity> front-end and were carried out due to the need to collect a database for testing acoustic pre-processing algorithms . In this way, realistic scenarios can be simulated at a preliminary stage, instead of real-time implementations , allowing for repeatable experiments . To match the project requirements , the WOZ experiments were recorded in three languages : English , German and Italian. Besides the user inputs , the database also contains non-speech related acoustic events , room impulse response measurements and video data, the latter used to compute 3D labels . Sessions were manually transcribed and segmented at word level , introducing also specific labels for acoustic events .", "tag": "USAGE"}, {"qas_id": "L08-1221.1_L08-1221.5", "question_text": "Process [BREAK] Corpora", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "USAGE"}, {"qas_id": "L08-1221.6_L08-1221.7", "question_text": "model [BREAK] corpus", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "USAGE"}, {"qas_id": "L08-1221.10_L08-1221.11", "question_text": "corpora [BREAK] research", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "USAGE"}, {"qas_id": "L08-1221.13_L08-1221.14", "question_text": "text [BREAK] corpus", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1221.24_L08-1221.25", "question_text": "software [BREAK] extracting", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "USAGE"}, {"qas_id": "L08-1221.30_L08-1221.31", "question_text": "paper [BREAK] project", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "TOPIC"}, {"qas_id": "L08-1221.34_L08-1221.35", "question_text": "overview [BREAK] qualities", "context": "Process Model for Composing High-quality Text Corpora . The Teko corpus composing model offers a decentralized, dynamic way of collecting high-quality text corpora for linguistic research . The resulting corpus consists of independent text sets. The sets are composed in cooperation with linguistic research projects , so each of them responds to a specific research need. The corpora are morphologically annotated and XML-based, with in-built compatibilty with the Kaino user interface used in the corpus server of the Research Institute for the Languages of Finland. Furthermore, software for extracting standard quantitative reports from the text sets has been created during the project . The paper describes the project , and estimates its benefits and problems . It also gives an overview of the technical qualities of the corpora and corpus interface connected to the Teko project .", "tag": "TOPIC"}, {"qas_id": "L08-1222.3_L08-1222.5", "question_text": "words [BREAK] corpus", "context": "AnCora: Multilevel Annotated Corpora for Catalan and Spanish . This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic. ub. edu/ancora. The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description : morphological (PoS and lemmas ), syntactic ( constituents and functions ), and semantic ( argument structures , thematic roles , semantic verb classes , named entities , and WordNet nominal senses). All resulting layers are independent of each other, thus making easier the data management . The annotation was performed manually, semiautomatically, or fully automatically, depending on the encoded linguistic information . The development of these basic resources constituted a primary objective , since there was a lack of such resources for these languages . A second goal was the definition of a consistent methodology that can be followed in further annotations. The current versions of AnCora have been used in several international evaluation competitions", "tag": "PART_WHOLE"}, {"qas_id": "L08-1222.6_L08-1222.7", "question_text": "languages [BREAK] corpus", "context": "AnCora: Multilevel Annotated Corpora for Catalan and Spanish . This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic. ub. edu/ancora. The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description : morphological (PoS and lemmas ), syntactic ( constituents and functions ), and semantic ( argument structures , thematic roles , semantic verb classes , named entities , and WordNet nominal senses). All resulting layers are independent of each other, thus making easier the data management . The annotation was performed manually, semiautomatically, or fully automatically, depending on the encoded linguistic information . The development of these basic resources constituted a primary objective , since there was a lack of such resources for these languages . A second goal was the definition of a consistent methodology that can be followed in further annotations. The current versions of AnCora have been used in several international evaluation competitions", "tag": "PART_WHOLE"}, {"qas_id": "L08-1222.8_L08-1222.10", "question_text": "texts [BREAK] corpora", "context": "AnCora: Multilevel Annotated Corpora for Catalan and Spanish . This paper presents AnCora, a multilingual corpus annotated at different linguistic levels consisting of 500,000 words in Catalan (AnCora-Ca) and in Spanish (AnCora-Es). At present AnCora is the largest multilayer annotated corpus of these languages freely available from http://clic. ub. edu/ancora. The two corpora consist mainly of newspaper texts annotated at different levels of linguistic description : morphological (PoS and lemmas ), syntactic ( constituents and functions ), and semantic ( argument structures , thematic roles , semantic verb classes , named entities , and WordNet nominal senses). All resulting layers are independent of each other, thus making easier the data management . The annotation was performed manually, semiautomatically, or fully automatically, depending on the encoded linguistic information . The development of these basic resources constituted a primary objective , since there was a lack of such resources for these languages . A second goal was the definition of a consistent methodology that can be followed in further annotations. The current versions of AnCora have been used in several international evaluation competitions", "tag": "PART_WHOLE"}, {"qas_id": "W08-0213.10_W08-0213.11", "question_text": "environment [BREAK] instruction", "context": "Studying Discourse and Dialogue with SIDGrid . Teaching Computational Linguistics is inherently multi-disciplinary and frequently poses challenges and provides opportunities in teaching to a student body with diverse educational backgrounds and goals . This paper describes the use of a computational environment (SIDGrid) that facilitates interdisciplinary instruction by providing support for students with little computational background as well as extending the scale of projects accessible to students with more advanced computational skills. The environment facilitates the use of hands-on exercises and is being applied to interdisciplinary instruction in Discourse and Dialogue .", "tag": "USAGE"}, {"qas_id": "W08-0213.20_W08-0213.22", "question_text": "environment [BREAK] instruction", "context": "Studying Discourse and Dialogue with SIDGrid . Teaching Computational Linguistics is inherently multi-disciplinary and frequently poses challenges and provides opportunities in teaching to a student body with diverse educational backgrounds and goals . This paper describes the use of a computational environment (SIDGrid) that facilitates interdisciplinary instruction by providing support for students with little computational background as well as extending the scale of projects accessible to students with more advanced computational skills. The environment facilitates the use of hands-on exercises and is being applied to interdisciplinary instruction in Discourse and Dialogue .", "tag": "USAGE"}, {"qas_id": "W08-2128.5_W08-2128.6", "question_text": "system [BREAK] challenge", "context": "A Combined Memory- Based Semantic Role Labeler of English . We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies . Syntactic dependencies are processed with the Malt- Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers . The system achieves 78.43 labeled macro Fl for the complete problem , 86.07 labeled attachment score for syntactic dependencies , and 70.51 labeled Fl for semantic dependencies .", "tag": "USAGE"}, {"qas_id": "W08-2128.8_W08-2128.11", "question_text": "parsing [BREAK] dependencies", "context": "A Combined Memory- Based Semantic Role Labeler of English . We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies . Syntactic dependencies are processed with the Malt- Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers . The system achieves 78.43 labeled macro Fl for the complete problem , 86.07 labeled attachment score for syntactic dependencies , and 70.51 labeled Fl for semantic dependencies .", "tag": "USAGE"}, {"qas_id": "W08-2128.13_W08-2128.15", "question_text": "Parser [BREAK] dependencies", "context": "A Combined Memory- Based Semantic Role Labeler of English . We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies . Syntactic dependencies are processed with the Malt- Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers . The system achieves 78.43 labeled macro Fl for the complete problem , 86.07 labeled attachment score for syntactic dependencies , and 70.51 labeled Fl for semantic dependencies .", "tag": "USAGE"}, {"qas_id": "W08-2128.17_W08-2128.21", "question_text": "classifiers [BREAK] dependencies", "context": "A Combined Memory- Based Semantic Role Labeler of English . We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies . Syntactic dependencies are processed with the Malt- Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers . The system achieves 78.43 labeled macro Fl for the complete problem , 86.07 labeled attachment score for syntactic dependencies , and 70.51 labeled Fl for semantic dependencies .", "tag": "USAGE"}, {"qas_id": "W08-2128.22_W08-2128.23", "question_text": "system [BREAK] problem", "context": "A Combined Memory- Based Semantic Role Labeler of English . We describe the system submitted to the closed challenge of the CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies . Syntactic dependencies are processed with the Malt- Parser 0.4. Semantic dependencies are processed with a combination of memory-based classifiers . The system achieves 78.43 labeled macro Fl for the complete problem , 86.07 labeled attachment score for syntactic dependencies , and 70.51 labeled Fl for semantic dependencies .", "tag": "USAGE"}, {"qas_id": "N07-1047.2_N07-1047.4", "question_text": "Models [BREAK] Conversion", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "USAGE"}, {"qas_id": "N07-1047.9_N07-1047.10", "question_text": "phonemes [BREAK] data", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "PART_WHOLE"}, {"qas_id": "N07-1047.15_N07-1047.16", "question_text": "alignments [BREAK] training", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "USAGE"}, {"qas_id": "N07-1047.18_N07-1047.19", "question_text": "prediction [BREAK] phonemes", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "USAGE"}, {"qas_id": "N07-1047.24_N07-1047.26", "question_text": "classification model [BREAK] sequence", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "USAGE"}, {"qas_id": "N07-1047.28_N07-1047.31", "question_text": "alignments [BREAK] approach", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "COMPARE"}, {"qas_id": "N07-1047.32_N07-1047.33", "question_text": "system [BREAK] performance", "context": "Applying Many-to-Many Alignments and Hidden Markov Models to Letter-to- Phoneme Conversion . Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes . Typically, the alignments are limited to one-to-one alignments . We present a novel technique of training with many-to-many alignments . A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists . We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word . The many-to-many alignments result in significant improvements over the traditional one-to-one approach . Our system achieves state-of-the-art performance on several languages and data sets.", "tag": "RESULT"}, {"qas_id": "N07-2003.4_N07-2003.7", "question_text": "dialog system [BREAK] information", "context": "Conquest---An Open- Source Dialog System for Conferences . We describe ConQuest, an open-source , reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure , and aims to enable applied research in spoken language interfaces . The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base . In this paper , we describe the system 's functionality , overall architecture , and we discuss two initial deployments .", "tag": "USAGE"}, {"qas_id": "N07-2003.8_N07-2003.9", "question_text": "infrastructure [BREAK] system", "context": "Conquest---An Open- Source Dialog System for Conferences . We describe ConQuest, an open-source , reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure , and aims to enable applied research in spoken language interfaces . The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base . In this paper , we describe the system 's functionality , overall architecture , and we discuss two initial deployments .", "tag": "USAGE"}, {"qas_id": "N07-2003.11_N07-2003.13", "question_text": "research [BREAK] interfaces", "context": "Conquest---An Open- Source Dialog System for Conferences . We describe ConQuest, an open-source , reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure , and aims to enable applied research in spoken language interfaces . The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base . In this paper , we describe the system 's functionality , overall architecture , and we discuss two initial deployments .", "tag": "TOPIC"}, {"qas_id": "N07-2003.14_N07-2003.17", "question_text": "domain [BREAK] research", "context": "Conquest---An Open- Source Dialog System for Conferences . We describe ConQuest, an open-source , reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure , and aims to enable applied research in spoken language interfaces . The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base . In this paper , we describe the system 's functionality , overall architecture , and we discuss two initial deployments .", "tag": "USAGE"}, {"qas_id": "N07-2003.20_N07-2003.22", "question_text": "paper [BREAK] functionality", "context": "Conquest---An Open- Source Dialog System for Conferences . We describe ConQuest, an open-source , reusable spoken dialog system that provides technical program information during conferences. The system uses a transparent, modular and open infrastructure , and aims to enable applied research in spoken language interfaces . The conference domain is a good platform for applied research since it permits periodical redeployments and evaluations with a real user-base . In this paper , we describe the system 's functionality , overall architecture , and we discuss two initial deployments .", "tag": "TOPIC"}, {"qas_id": "P04-1006.3_P04-1006.7", "question_text": "technique [BREAK] parsing", "context": "Attention Shifting For Parsing Speech . We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice . The parser 's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model .", "tag": "USAGE"}, {"qas_id": "P04-1006.10_P04-1006.12", "question_text": "parser [BREAK] technique", "context": "Attention Shifting For Parsing Speech . We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice . The parser 's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model .", "tag": "USAGE"}, {"qas_id": "P04-1006.16_P04-1006.18", "question_text": "word-lattice [BREAK] parser", "context": "Attention Shifting For Parsing Speech . We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice . The parser 's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model .", "tag": "USAGE"}, {"qas_id": "P04-1006.20_P04-1006.22", "question_text": "technique [BREAK] increase", "context": "Attention Shifting For Parsing Speech . We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word-lattice . The parser 's attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited. This attention-shifting technique provides a six-times increase in speed (measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model .", "tag": "RESULT"}, {"qas_id": "P06-3010.4_P06-3010.5", "question_text": "approach [BREAK] Word Sense Disambiguation", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "USAGE"}, {"qas_id": "P06-3010.7_P06-3010.8", "question_text": "formalism [BREAK] instances", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-3010.11_P06-3010.12", "question_text": "techniques [BREAK] evidence", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "USAGE"}, {"qas_id": "P06-3010.14_P06-3010.16", "question_text": "process [BREAK] model", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "RESULT"}, {"qas_id": "P06-3010.18_P06-3010.20", "question_text": "approach [BREAK] verbs", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "USAGE"}, {"qas_id": "P06-3010.24_P06-3010.25", "question_text": "approach [BREAK] accuracy", "context": "A Hybrid Relational Approach For WSD - First Results . We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge . It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based WSD model . We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation . Results showed that the approach is promising, achieving an average accuracy of 75%, which outperforms the other machine learning techniques investigated (66%).", "tag": "RESULT"}, {"qas_id": "C82-1028.5_C82-1028.7", "question_text": "approach [BREAK] approach", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "COMPARE"}, {"qas_id": "C82-1028.8_C82-1028.10", "question_text": "process [BREAK] approach", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "PART_WHOLE"}, {"qas_id": "C82-1028.14_C82-1028.16", "question_text": "analysis [BREAK] approach", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "PART_WHOLE"}, {"qas_id": "C82-1028.20_C82-1028.22", "question_text": "paper [BREAK] approach", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "TOPIC"}, {"qas_id": "C82-1028.28_C82-1028.30", "question_text": "principle [BREAK] rules", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "USAGE"}, {"qas_id": "C82-1028.31_C82-1028.32", "question_text": "operation [BREAK] rule", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "C82-1028.33_C82-1028.34", "question_text": "operation [BREAK] rules", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "C82-1028.37_C82-1028.39", "question_text": "trees [BREAK] structure", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "C82-1028.43_C82-1028.44", "question_text": "section [BREAK] concepts", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "TOPIC"}, {"qas_id": "C82-1028.46_C82-1028.47", "question_text": "section [BREAK] version", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "TOPIC"}, {"qas_id": "C82-1028.52_C82-1028.53", "question_text": "section [BREAK] design", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "TOPIC"}, {"qas_id": "C82-1028.54_C82-1028.56", "question_text": "approach [BREAK] translation system", "context": "Machine Translation Based On Logically Isomorphic Montague Grammars . \"Usually two approaches to machine translation are distinguished: the interlingual approach and the transfer approach ( cf. Hutchins [1]). In the interlingual approach translation is a two-stage process : from source language to interlingua and from interlingua to target language . In the transfer approach there are three stages: source language analysis , transfer and target language generation . The approach advanced in this paper is a variant of the interlingual one. It requires that 'logically isomorphic grammars' are written for the languages ,under consideration . The syntactic rules of these grammars must correspond with logical operations , in accordance with the compositionality principle of Montague grammar. Moreover, the grammars must be attuned to each other as follows: if one grammar contains a rule corresponding with a particular logical operation , the other grammars must contain rules corresponding with the same operation . Syntactically, these rules may differ considerably. If the grammars are attuned to each other in this way, 'logical derivation trees ', representations of both the syntactical and the logical structure of sentences , can be used as intermediate expressions . The paper is organized as follows. In section 2 the relevant concepts of Montague grammar and the notion 'logically isomorphic grammars' are introduced. In section 3 a version of Montague grammar is described, called M-grammar, which is more suitable for computational use than Montague 's original proposals . The property of logical isomorphy is then defined for M-grammars. In section 4 the design of the Rosetta translation system , based on this approach , is outlined , followed by a brief discussion in section 5. \"", "tag": "USAGE"}, {"qas_id": "C86-1082.3_C86-1082.5", "question_text": "paper [BREAK] semantics", "context": "A Compositional Semantics For Directional Modifiers - Locative Case Reopened . This paper presents a model-theoretic semantics for directional modifiers in English . The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers . Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature .", "tag": "TOPIC"}, {"qas_id": "C86-1082.6_C86-1082.7", "question_text": "modifiers [BREAK] English", "context": "A Compositional Semantics For Directional Modifiers - Locative Case Reopened . This paper presents a model-theoretic semantics for directional modifiers in English . The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers . Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature .", "tag": "PART_WHOLE"}, {"qas_id": "C86-1082.9_C86-1082.10", "question_text": "theory [BREAK] analysis", "context": "A Compositional Semantics For Directional Modifiers - Locative Case Reopened . This paper presents a model-theoretic semantics for directional modifiers in English . The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers . Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature .", "tag": "USAGE"}, {"qas_id": "C86-1082.14_C86-1082.18", "question_text": "treatment [BREAK] treatments", "context": "A Compositional Semantics For Directional Modifiers - Locative Case Reopened . This paper presents a model-theoretic semantics for directional modifiers in English . The semantic theory presupposed for the analysis is that of Montague Grammar (cf. Montague 1970 , 1973) which makes it possible to develop a strongly compositional treatment of directional modifiers . Such a treatment has significant computational advantages over case-based treatments of directional modifiers that are advocated in the Al literature .", "tag": "COMPARE"}, {"qas_id": "C88-1031.8_C88-1031.13", "question_text": "model [BREAK] realizations", "context": "Stylistic Grammars In Language Translation . We are developing stylistic grammars to provide the basis for a French and English stylistic parser . Our stylistic grammar is a branching stratificational model , built upon a foundation dealing with lexical , syntactic , and semantic stylistic realizations . Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals , such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to- English translation . We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output .", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1031.14_C88-1031.15", "question_text": "vocabulary [BREAK] level", "context": "Stylistic Grammars In Language Translation . We are developing stylistic grammars to provide the basis for a French and English stylistic parser . Our stylistic grammar is a branching stratificational model , built upon a foundation dealing with lexical , syntactic , and semantic stylistic realizations . Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals , such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to- English translation . We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output .", "tag": "USAGE"}, {"qas_id": "C88-1031.24_C88-1031.26", "question_text": "schema [BREAK] translation", "context": "Stylistic Grammars In Language Translation . We are developing stylistic grammars to provide the basis for a French and English stylistic parser . Our stylistic grammar is a branching stratificational model , built upon a foundation dealing with lexical , syntactic , and semantic stylistic realizations . Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals , such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to- English translation . We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output .", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1031.27_C88-1031.28", "question_text": "analysis [BREAK] machine translation systems", "context": "Stylistic Grammars In Language Translation . We are developing stylistic grammars to provide the basis for a French and English stylistic parser . Our stylistic grammar is a branching stratificational model , built upon a foundation dealing with lexical , syntactic , and semantic stylistic realizations . Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals , such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to- English translation . We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output .", "tag": "PART_WHOLE"}, {"qas_id": "C88-1031.31_C88-1031.32", "question_text": "systems' [BREAK] quality", "context": "Stylistic Grammars In Language Translation . We are developing stylistic grammars to provide the basis for a French and English stylistic parser . Our stylistic grammar is a branching stratificational model , built upon a foundation dealing with lexical , syntactic , and semantic stylistic realizations . Its central level uses a vocabulary of constituent stylistic elements common to both English and French, while the top level correlates stylistic goals , such as clarity and concreteness, with patterns of these elements. Overall, we are implementing a computational schema of stylistics in French-to- English translation . We believe that the incorporation of stylistic analysis into machine translation systems will significantly reduce the current reliance on human post-editing and improve the quality of the systems' output .", "tag": "RESULT"}, {"qas_id": "C94-1036.2_C94-1036.3", "question_text": "Information [BREAK] Words", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "MODEL-FEATURE"}, {"qas_id": "C94-1036.6_C94-1036.7", "question_text": "words [BREAK] sentences", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "PART_WHOLE"}, {"qas_id": "C94-1036.10_C94-1036.11", "question_text": "information [BREAK] information", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "COMPARE"}, {"qas_id": "C94-1036.12_C94-1036.13", "question_text": "paper [BREAK] method", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "TOPIC"}, {"qas_id": "C94-1036.15_C94-1036.18", "question_text": "sentences [BREAK] result", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "RESULT"}, {"qas_id": "C94-1036.20_C94-1036.21", "question_text": "system [BREAK] result", "context": "Segmenting A Sentence Into Morphemes Using Statistic Information Between Words . This paper is on dividing non-separated language sentences (whose words are not separated from each other with a space or other separater ) into morphemes using statistical information , not grammatical information which is often used in NLP. In this paper we describe our method and experimental result on Japanese and Chinese sentences . As will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences .", "tag": "RESULT"}, {"qas_id": "C94-2106.8_C94-2106.10", "question_text": "paper [BREAK] system", "context": "A System Of Verbal Semantic Attributes Focused On The Syntactic Correspondence Between Japanese And English . This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases . These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures .", "tag": "TOPIC"}, {"qas_id": "C94-2106.14_C94-2106.15", "question_text": "characteristics [BREAK] verbs", "context": "A System Of Verbal Semantic Attributes Focused On The Syntactic Correspondence Between Japanese And English . This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases . These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures .", "tag": "MODEL-FEATURE"}, {"qas_id": "C94-2106.28_C94-2106.32", "question_text": "patterns [BREAK] dictionary", "context": "A System Of Verbal Semantic Attributes Focused On The Syntactic Correspondence Between Japanese And English . This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases . These attribute values are used to disambiguate the meanings of all Japanese and English pattern pairs in a Japanese to English transfer pattern dictionary consisting of 15,000 pairs of Japanese valence patterns and equivalent English syntactic structures .", "tag": "PART_WHOLE"}, {"qas_id": "C86-1046.5_C86-1046.6", "question_text": "component [BREAK] language processing system", "context": "Dependency Unification Grammar . This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars. The principles of Dependency Unification Grammar (DUG) are discussed. The computer language DRL ( Dependency Representation Language ) is introduced in which DUGs can be formulated. A unification-based parsing procedure is part of the formalism . PLAIN is implemented at. the universities of Heidelberg, Bonn, Flensburg, Kiel, Zurich and Cambridge U.K.", "tag": "PART_WHOLE"}, {"qas_id": "C86-1046.17_C86-1046.19", "question_text": "procedure [BREAK] formalism", "context": "Dependency Unification Grammar . This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars. The principles of Dependency Unification Grammar (DUG) are discussed. The computer language DRL ( Dependency Representation Language ) is introduced in which DUGs can be formulated. A unification-based parsing procedure is part of the formalism . PLAIN is implemented at. the universities of Heidelberg, Bonn, Flensburg, Kiel, Zurich and Cambridge U.K.", "tag": "PART_WHOLE"}, {"qas_id": "D08-1100.3_D08-1100.6", "question_text": "Learning [BREAK] Information", "context": "Acquiring Domain- Specific Dialog Information from Task- Oriented Human-Human Interaction through an Unsupervised Learning . We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data . The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs .", "tag": "USAGE"}, {"qas_id": "D08-1100.10_D08-1100.13", "question_text": "knowledge [BREAK] dialog system", "context": "Acquiring Domain- Specific Dialog Information from Task- Oriented Human-Human Interaction through an Unsupervised Learning . We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data . The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs .", "tag": "USAGE"}, {"qas_id": "D08-1100.18_D08-1100.21", "question_text": "design [BREAK] representation", "context": "Acquiring Domain- Specific Dialog Information from Task- Oriented Human-Human Interaction through an Unsupervised Learning . We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data . The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs .", "tag": "MODEL-FEATURE"}, {"qas_id": "D08-1100.22_D08-1100.25", "question_text": "approach [BREAK] information", "context": "Acquiring Domain- Specific Dialog Information from Task- Oriented Human-Human Interaction through an Unsupervised Learning . We describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data . The key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs .", "tag": "USAGE"}, {"qas_id": "L08-1189.2_L08-1189.3", "question_text": "Environment [BREAK] Architecture", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1189.4_L08-1189.9", "question_text": "Information extraction [BREAK] solutions", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "USAGE"}, {"qas_id": "L08-1189.14_L08-1189.16", "question_text": "characteristics [BREAK] resources", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1189.18_L08-1189.20", "question_text": "process [BREAK] evaluating", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1189.23_L08-1189.24", "question_text": "architectures [BREAK] text processing", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "USAGE"}, {"qas_id": "L08-1189.31_L08-1189.32", "question_text": "components [BREAK] pipeline", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1189.35_L08-1189.36", "question_text": "status [BREAK] tool", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1189.45_L08-1189.50", "question_text": "tool [BREAK] cycle", "context": "A Development Environment for Configurable Meta-Annotators in a Pipelined NLP Architecture . Information extraction from large data repositories is critical to Information Management solutions . In addition to prerequisite corpus analysis , to determine domain-specific characteristics of text resources , developing , refining and evaluating analytics entails a complex and lengthy process , typically requiring more than just domain expertise. Modern architectures for text processing , while facilitating reuse and (re-)composition of analytical pipelines , place additional constraints upon the analytics development , as domain experts need not only configure individual annotator components , but situate these within a fully functional annotator pipeline . We present the design , and current status , of a tool for configuring model-driven annotators, which abstracts away from annotator implementation details , pipeline composition constraints , and data management . Instead, the tool embodies support for all stages of ontology-centric model development cycle - from corpus analysis and concept definition , to model development and testing, to large scale evaluation , to easy and rapid composition of text applications deploying these concept models . With our design , we aim to meet the needs of domain experts , who are not necessarily expert NLP practitioners.", "tag": "USAGE"}, {"qas_id": "L08-1002.2_L08-1002.4", "question_text": "Models [BREAK] Information Retrieval", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "USAGE"}, {"qas_id": "L08-1002.5_L08-1002.14", "question_text": "method [BREAK] task", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "USAGE"}, {"qas_id": "L08-1002.19_L08-1002.22", "question_text": "system [BREAK] data", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "USAGE"}, {"qas_id": "L08-1002.23_L08-1002.26", "question_text": "task [BREAK] spontaneous speech", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "USAGE"}, {"qas_id": "L08-1002.29_L08-1002.30", "question_text": "topics [BREAK] information", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1002.31_L08-1002.32", "question_text": "Information Retrieval systems [BREAK] results", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "RESULT"}, {"qas_id": "L08-1002.33_L08-1002.36", "question_text": "summaries [BREAK] data", "context": "Combining Multiple Models for Speech Information Retrieval . In this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a Speech Information Retrieval task . The formulas for combining the models are tuned on training data . Then the system is evaluated on test data . The task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain good results on this data set, except for the case when manual summaries are included .", "tag": "PART_WHOLE"}, {"qas_id": "L08-1022.2_L08-1022.3", "question_text": "Corpora [BREAK] Parsing", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "USAGE"}, {"qas_id": "L08-1022.10_L08-1022.13", "question_text": "statistics [BREAK] parsers", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "USAGE"}, {"qas_id": "L08-1022.16_L08-1022.17", "question_text": "paper [BREAK] method", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "TOPIC"}, {"qas_id": "L08-1022.19_L08-1022.20", "question_text": "sensitivity [BREAK] parser", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1022.22_L08-1022.24", "question_text": "techniques [BREAK] parsers", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "USAGE"}, {"qas_id": "L08-1022.25_L08-1022.26", "question_text": "methods [BREAK] predictions", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "USAGE"}, {"qas_id": "L08-1022.29_L08-1022.30", "question_text": "corpora [BREAK] sensitivity", "context": "Subdomain Sensitive Statistical Parsing using Raw Corpora . Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports , politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser . We employ statistical techniques for creating an ensemble of domain sensitive parsers , and explore methods for amalgamating their predictions . Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline.", "tag": "USAGE"}, {"qas_id": "E99-1027.7_E99-1027.9", "question_text": "agreement [BREAK] representation", "context": "An Experiment On The Upper Bound Of Interjudge Agreement : The Case Of Tagging . We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation . Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories . Our experiments with 55kW data give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement .", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1027.10_E99-1027.11", "question_text": "words [BREAK] text", "context": "An Experiment On The Upper Bound Of Interjudge Agreement : The Case Of Tagging . We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation . Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories . Our experiments with 55kW data give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement .", "tag": "PART_WHOLE"}, {"qas_id": "E99-1027.18_E99-1027.19", "question_text": "training [BREAK] linguists", "context": "An Experiment On The Upper Bound Of Interjudge Agreement : The Case Of Tagging . We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation . Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories . Our experiments with 55kW data give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement .", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1027.21_E99-1027.22", "question_text": "agreement [BREAK] tags", "context": "An Experiment On The Upper Bound Of Interjudge Agreement : The Case Of Tagging . We investigate the controversial issue about the upper bound of interjudge agreement in the use of a low-level grammatical representation . Pessimistic views suggest that several percent of words in running text are undecidable in terms of part-of-speech categories . Our experiments with 55kW data give reason for optimism: linguists with only 30 hours' training apply the EngCG-2 morphological tags with almost 100% interjudge agreement .", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1049.2_E99-1049.4", "question_text": "research [BREAK] anaphora resolution", "context": "Pointing To Events . Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected. This paper describes the results of an empirical study regarding event reference . The experiment investigated event anaphora in narrative discourse via a sentence completion task .", "tag": "TOPIC"}, {"qas_id": "E99-1049.6_E99-1049.7", "question_text": "paper [BREAK] results", "context": "Pointing To Events . Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected. This paper describes the results of an empirical study regarding event reference . The experiment investigated event anaphora in narrative discourse via a sentence completion task .", "tag": "TOPIC"}, {"qas_id": "E99-1049.8_E99-1049.10", "question_text": "study [BREAK] reference", "context": "Pointing To Events . Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected. This paper describes the results of an empirical study regarding event reference . The experiment investigated event anaphora in narrative discourse via a sentence completion task .", "tag": "TOPIC"}, {"qas_id": "E99-1049.11_E99-1049.16", "question_text": "task [BREAK] experiment", "context": "Pointing To Events . Although there is an extensive body of research concerned with anaphora resolution (e.g. ( Fox, 1987 ; Grosz et al., 1995 )), event anaphora has been widely neglected. This paper describes the results of an empirical study regarding event reference . The experiment investigated event anaphora in narrative discourse via a sentence completion task .", "tag": "USAGE"}, {"qas_id": "E03-1051.8_E03-1051.9", "question_text": "attachment [BREAK] prepositional phrases", "context": "Learning PP Attachment For Filtering Prosodic Phrasing . We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun . Using cross-validated parameter and feature selection , we train two learning algorithms , iBl and RIPPER, on making this distinction , based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment , since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun . On noun attachment , IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.", "tag": "MODEL-FEATURE"}, {"qas_id": "E03-1051.14_E03-1051.16", "question_text": "feature selection [BREAK] algorithms", "context": "Learning PP Attachment For Filtering Prosodic Phrasing . We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun . Using cross-validated parameter and feature selection , we train two learning algorithms , iBl and RIPPER, on making this distinction , based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment , since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun . On noun attachment , IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.", "tag": "USAGE"}, {"qas_id": "E03-1051.17_E03-1051.19", "question_text": "lexical features [BREAK] distinction", "context": "Learning PP Attachment For Filtering Prosodic Phrasing . We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun . Using cross-validated parameter and feature selection , we train two learning algorithms , iBl and RIPPER, on making this distinction , based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment , since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun . On noun attachment , IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.", "tag": "USAGE"}, {"qas_id": "E03-1051.32_E03-1051.34", "question_text": "decisions [BREAK] improvement", "context": "Learning PP Attachment For Filtering Prosodic Phrasing . We explore learning prepositional-phrase attachment in Dutch, to use it as a filter in prosodic phrasing. From a syntactic treebank of spoken Dutch we extract instances of the attachment of prepositional phrases to either a governing verb or noun . Using cross-validated parameter and feature selection , we train two learning algorithms , iBl and RIPPER, on making this distinction , based on unigram and bigram lexical features and a cooccurrence feature derived from WWW counts. We optimize the learning on noun attachment , since in a second stage we use the attachment decision for blocking the incorrect placement of phrase boundaries before prepositional phrases attached to the preceding noun . On noun attachment , IBl attains an F-score of 82; RIPPER an F-score of 78. When used as a filter for prosodic phrasing, using attachment decisions from IBl yields the best improvement on precision (by six points to 71) on phrase boundary placement.", "tag": "RESULT"}, {"qas_id": "C96-2210.2_C96-2210.3", "question_text": "Architecture [BREAK] Text Analysis", "context": "A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing . Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers . When one works with a general language ; and not a sublanguage, there are different cases of ambiguities at different classical levels ; and more particularly when one works on complex language phenomena analysis ( coordination , ellipsis , negation ...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion . So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics ( morphology , syntax , semantic ) or to complex language phenomena analysis .", "tag": "USAGE"}, {"qas_id": "C96-2210.7_C96-2210.9", "question_text": "layers [BREAK] Natural Language Processing systems", "context": "A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing . Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers . When one works with a general language ; and not a sublanguage, there are different cases of ambiguities at different classical levels ; and more particularly when one works on complex language phenomena analysis ( coordination , ellipsis , negation ...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion . So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics ( morphology , syntax , semantic ) or to complex language phenomena analysis .", "tag": "PART_WHOLE"}, {"qas_id": "C96-2210.10_C96-2210.12", "question_text": "ambiguities [BREAK] language", "context": "A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing . Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers . When one works with a general language ; and not a sublanguage, there are different cases of ambiguities at different classical levels ; and more particularly when one works on complex language phenomena analysis ( coordination , ellipsis , negation ...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion . So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics ( morphology , syntax , semantic ) or to complex language phenomena analysis .", "tag": "PART_WHOLE"}, {"qas_id": "C96-2210.23_C96-2210.24", "question_text": "explosion [BREAK] approach", "context": "A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing . Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers . When one works with a general language ; and not a sublanguage, there are different cases of ambiguities at different classical levels ; and more particularly when one works on complex language phenomena analysis ( coordination , ellipsis , negation ...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion . So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics ( morphology , syntax , semantic ) or to complex language phenomena analysis .", "tag": "MODEL-FEATURE"}, {"qas_id": "C96-2210.25_C96-2210.27", "question_text": "agents [BREAK] architecture", "context": "A Distributed Architecture For Text Analysis In French: An Application To Complex Linguistic Phenomena Processing . Most Natural Language Processing systems use a sequential architecture embodying classical linguistic layers . When one works with a general language ; and not a sublanguage, there are different cases of ambiguities at different classical levels ; and more particularly when one works on complex language phenomena analysis ( coordination , ellipsis , negation ...) it is difficult to take into account all the different types of these constructions with a general grammar. Indeed, the inconvenience of this approach is the possible risk of a combinatory explosion . So, we have defined the TALISMAN architecture that includes linguistic agents that correspond either to classical levels in linguistics ( morphology , syntax , semantic ) or to complex language phenomena analysis .", "tag": "PART_WHOLE"}, {"qas_id": "C00-1058.4_C00-1058.6", "question_text": "paper [BREAK] method", "context": "Automatic Thesaurus Generation Through Multiple Filtering . In this paper , we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora . The method combines morphological and lexical processing , bilingual word alignment , and graph-theoretic cluster generation . An experiment shows that the method is promising.", "tag": "TOPIC"}, {"qas_id": "C00-1058.12_C00-1058.14", "question_text": "processing [BREAK] method", "context": "Automatic Thesaurus Generation Through Multiple Filtering . In this paper , we propose a method of generating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora . The method combines morphological and lexical processing , bilingual word alignment , and graph-theoretic cluster generation . An experiment shows that the method is promising.", "tag": "PART_WHOLE"}, {"qas_id": "C02-1086.7_C02-1086.8", "question_text": "paper [BREAK] method", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "TOPIC"}, {"qas_id": "C02-1086.9_C02-1086.10", "question_text": "clustering [BREAK] ambiguities", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "USAGE"}, {"qas_id": "C02-1086.16_C02-1086.20", "question_text": "dictionary [BREAK] query", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "USAGE"}, {"qas_id": "C02-1086.21_C02-1086.25", "question_text": "retrieval [BREAK] documents", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "USAGE"}, {"qas_id": "C02-1086.29_C02-1086.32", "question_text": "clusters [BREAK] documents", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "MODEL-FEATURE"}, {"qas_id": "C02-1086.33_C02-1086.35", "question_text": "clusters [BREAK] weight", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "USAGE"}, {"qas_id": "C02-1086.39_C02-1086.40", "question_text": "method [BREAK] performance improvement", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "RESULT"}, {"qas_id": "C02-1086.52_C02-1086.53", "question_text": "method [BREAK] retrieval", "context": "Implicit Ambiguity Resolution Using Incremental Clustering In Korean-To- English Cross- Language Information Retrieval . This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to- English cross-language information retrieval . In the framework we propose , a query in Korean is first translated into English by looking up Korean- English dictionary , then documents are retrieved based on the vector space retrieval for the translated query terms . For the top-ranked retrieved documents , query-oriented document clusters are incrementally created and the weight of each retrieved document is re-calculated by using clusters . In experiment on TREC-6 CLIR test collection , our method achieved 28.29% performance improvement for translated queries without ambiguity resolution for queries . This corresponds to 97.27% of the monolingual performance for original queries . When we combine our method with query ambiguity resolution , our method even outperforms the monolingual retrieval .", "tag": "COMPARE"}, {"qas_id": "C04-1091.1_C04-1091.3", "question_text": "Framework [BREAK] Problem", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "USAGE"}, {"qas_id": "C04-1091.10_C04-1091.12", "question_text": "framework [BREAK] problem", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "USAGE"}, {"qas_id": "C04-1091.19_C04-1091.20", "question_text": "maximization [BREAK] problem", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1091.25_C04-1091.27", "question_text": "techniques [BREAK] algorithms", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "RESULT"}, {"qas_id": "C04-1091.29_C04-1091.31", "question_text": "search algorithm [BREAK] algorithm", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1091.32_C04-1091.33", "question_text": "algorithm [BREAK] algorithms", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "USAGE"}, {"qas_id": "C04-1091.34_C04-1091.36", "question_text": "algorithms [BREAK] significance", "context": "An Algorithmic Framework For Solving The Decoding Problem In Statistical Machine Translation . The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem . In this paper , we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility . In the new algorithmic framework , the decoding problem can be solved both exactly and approximately. The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems. We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem . A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations. Our first algorithm is a prov-ably linear time search algorithm . We use this algorithm as a subroutine in the other algorithms . We believe that decoding algorithms derived from our framework can be of practical significance .", "tag": "RESULT"}, {"qas_id": "C04-1155.10_C04-1155.13", "question_text": "schema [BREAK] base", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "USAGE"}, {"qas_id": "C04-1155.20_C04-1155.22", "question_text": "syntactic structure [BREAK] sentence", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1155.24_C04-1155.29", "question_text": "tree [BREAK] string", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1155.32_C04-1155.33", "question_text": "node [BREAK] representation", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "PART_WHOLE"}, {"qas_id": "C04-1155.34_C04-1155.36", "question_text": "structure [BREAK] target languages", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1155.39_C04-1155.41", "question_text": "examples [BREAK] database", "context": "A Flexible Example Annotation Schema : Translation Corresponding Tree Representation . This paper presents work on the task of constructing an example base from a given bilingual corpus based on the annotation schema of Translation Corresponding Tree (TCT). Each TCT describes a translation example (a pair of bilingual sentences ). It represents the syntactic structure of source language sentence , and more importantly is the facility to specify the correspondences between string (both the source and target sentences ) and the representation tree . Furthermore, syntax transformation clues are also encapsulated at each node in the TCT representation to capture the differentiation of grammatical structure between the source and target languages . With this annotation schema , translation examples are effectively represented and organized in the bilingual knowledge database that we need for the Portuguese to Chinese machine translation system .", "tag": "PART_WHOLE"}, {"qas_id": "C04-1204.1_C04-1204.4", "question_text": "Linguistic Analysis [BREAK] Relations", "context": "Deep Linguistic Analysis For The Accurate Identification Of Predicate- Argument Relations . This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations . We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model , an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "tag": "USAGE"}, {"qas_id": "C04-1204.7_C04-1204.8", "question_text": "accuracy [BREAK] parsing", "context": "Deep Linguistic Analysis For The Accurate Identification Of Predicate- Argument Relations . This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations . We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model , an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1204.13_C04-1204.16", "question_text": "output [BREAK] Bank", "context": "Deep Linguistic Analysis For The Accurate Identification Of Predicate- Argument Relations . This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations . We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model , an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "tag": "COMPARE"}, {"qas_id": "C04-1204.17_C04-1204.18", "question_text": "mapping [BREAK] semantic representation", "context": "Deep Linguistic Analysis For The Accurate Identification Of Predicate- Argument Relations . This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations . We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model , an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "tag": "MODEL-FEATURE"}, {"qas_id": "C04-1204.22_C04-1204.24", "question_text": "parser [BREAK] studies", "context": "Deep Linguistic Analysis For The Accurate Identification Of Predicate- Argument Relations . This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations . We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation. Even though PropBank was not used for the training of a disambiguation model , an HPSG parser achieved the accuracy competitive with existing studies on the task of identifying PropBank annotations.", "tag": "COMPARE"}, {"qas_id": "E91-1004.2_E91-1004.5", "question_text": "paper [BREAK] algorithm", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "TOPIC"}, {"qas_id": "E91-1004.9_E91-1004.10", "question_text": "parse [BREAK] sentence", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "MODEL-FEATURE"}, {"qas_id": "E91-1004.13_E91-1004.15", "question_text": "prediction [BREAK] parser", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "USAGE"}, {"qas_id": "E91-1004.18_E91-1004.21", "question_text": "interpretation [BREAK] extent", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "MODEL-FEATURE"}, {"qas_id": "E91-1004.22_E91-1004.23", "question_text": "parser [BREAK] parsers", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "COMPARE"}, {"qas_id": "E91-1004.25_E91-1004.27", "question_text": "context [BREAK] conditional probabilities", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "USAGE"}, {"qas_id": "E91-1004.30_E91-1004.37", "question_text": "results [BREAK] tool", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "USAGE"}, {"qas_id": "E91-1004.46_E91-1004.47", "question_text": "categories [BREAK] unknown words", "context": "Pearl: A Probabilistic Chart Parser . This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the \"\"best\"\" parse of a sentence . The parser , Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context , of the sentence predicts that interpretation . This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context , to predict, likelihood. 'Pearl also provides a framework for incorporating the results of previous work in pait-ol'- speech assignment , unknown word models , and other probabilistic models of linguistic features into one parsing tool , interleaving these techniques instead of using the traditional pipeline architecture . In preliminary tests , 'Pearl has been successful at resolving part-of-speech and word (in speech processing ) ambiguity , determining categories for unknown words , and selecting correct parses first using a very loosely fitting covering grammar.\"", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1227.7_L08-1227.8", "question_text": "analysis [BREAK] features", "context": "Relationships between Nursing Converstaions and Activities . In this paper , we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy . For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora . Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper , we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.", "tag": "TOPIC"}, {"qas_id": "L08-1227.9_L08-1227.10", "question_text": "conversation [BREAK] corpora", "context": "Relationships between Nursing Converstaions and Activities . In this paper , we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy . For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora . Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper , we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1227.12_L08-1227.13", "question_text": "information [BREAK] conversations", "context": "Relationships between Nursing Converstaions and Activities . In this paper , we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy . For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora . Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper , we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.", "tag": "MODEL-FEATURE"}, {"qas_id": "L08-1227.22_L08-1227.24", "question_text": "paper [BREAK] approach", "context": "Relationships between Nursing Converstaions and Activities . In this paper , we determine the relationships between nursing activities and nurseing conversations based on the principle of maximum entropy . For analysis of the features of nursing activities, we built nursing corpora from actual nursing conversation sets collected in hospitals that involve various information about nursing activities. Ex-nurses manually assigned nursing activity information to the nursing conversations in the corpora . Since it is inefficient and too expensive to attach all information manually, we introduced an automatic nursing activity determination method for which we built models of relationships between nursing conversations and activities. In this paper , we adopted a maximum entropy approach for learning. Even though the conversation data set is not large enough for learning, acceptable results were obtained.", "tag": "TOPIC"}, {"qas_id": "D08-1022.7_D08-1022.9", "question_text": "extraction [BREAK] machine <entity id=\"D08-1022.10\">translation", "context": "Forest-based Translation Rule Extraction . Translation rule extraction is a fundamental problem in machine <entity id=\"D08-1022.10\">translation </entity> , especially for linguistically syntax-based packed forest", "tag": "PART_WHOLE"}, {"qas_id": "D08-1048.7_D08-1048.8", "question_text": "paper [BREAK] applicability", "context": "Automatic induction of FrameNet lexical units . Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage . In this paper , we investigate the applicability of distributional and WordNet-based <entity id=\"D08-1048.10\">models </entity> on the task of lexical unit induction ,", "tag": "TOPIC"}, {"qas_id": "D08-1048.9_D08-1048.14", "question_text": "WordNet-based <entity id=\"D08-1048.10\">models [BREAK] induction", "context": "Automatic induction of FrameNet lexical units . Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage . In this paper , we investigate the applicability of distributional and WordNet-based <entity id=\"D08-1048.10\">models </entity> on the task of lexical unit induction ,", "tag": "USAGE"}, {"qas_id": "I05-1038.1_I05-1038.3", "question_text": "Classification [BREAK] Questions", "context": "Classification of Multiple- Sentence Questions . Abstract .", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-1067.1_I05-1067.3", "question_text": "Structure [BREAK] Computing", "context": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness . Abstract . proper", "tag": "USAGE"}, {"qas_id": "E99-1033.2_E99-1033.4", "question_text": "paper [BREAK] methodology", "context": "Investigating NLG Architectures: Taking Style Into Consideration . In this paper we propose a methodology for investigating the relationship between architectures of natural language generation (NLG)", "tag": "TOPIC"}, {"qas_id": "E03-1010.1_E03-1010.3", "question_text": "Automatic <entity id=\"E03-1010.2\">Evaluation [BREAK] Translation <entity id=\"E03-1010.4\">System", "context": "Automatic <entity id=\"E03-1010.2\">Evaluation </entity> For A Palpable Measure Of A Speech Translation <entity id=\"E03-1010.4\">System </entity> 's Capability . The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation <entity id=\"E03-1010.25\">result </entity> , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation <entity id=\"E03-1010.40\">methods </entity> are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation <entity id=\"E03-1010.56\">results </entity> of the translation paired comparison method .", "tag": "USAGE"}, {"qas_id": "E03-1010.8_E03-1010.11", "question_text": "paper [BREAK] schemes", "context": "Automatic <entity id=\"E03-1010.2\">Evaluation </entity> For A Palpable Measure Of A Speech Translation <entity id=\"E03-1010.4\">System </entity> 's Capability . The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation <entity id=\"E03-1010.25\">result </entity> , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation <entity id=\"E03-1010.40\">methods </entity> are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation <entity id=\"E03-1010.56\">results </entity> of the translation paired comparison method .", "tag": "TOPIC"}, {"qas_id": "E03-1010.16_E03-1010.19", "question_text": "method [BREAK] speech translation", "context": "Automatic <entity id=\"E03-1010.2\">Evaluation </entity> For A Palpable Measure Of A Speech Translation <entity id=\"E03-1010.4\">System </entity> 's Capability . The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation <entity id=\"E03-1010.25\">result </entity> , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation <entity id=\"E03-1010.40\">methods </entity> are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation <entity id=\"E03-1010.56\">results </entity> of the translation paired comparison method .", "tag": "USAGE"}, {"qas_id": "E03-1010.22_E03-1010.24", "question_text": "method [BREAK] evaluation <entity id=\"E03-1010.25\">result", "context": "Automatic <entity id=\"E03-1010.2\">Evaluation </entity> For A Palpable Measure Of A Speech Translation <entity id=\"E03-1010.4\">System </entity> 's Capability . The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation <entity id=\"E03-1010.25\">result </entity> , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation <entity id=\"E03-1010.40\">methods </entity> are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation <entity id=\"E03-1010.56\">results </entity> of the translation paired comparison method .", "tag": "RESULT"}, {"qas_id": "E03-1010.39_E03-1010.45", "question_text": "evaluation <entity id=\"E03-1010.40\">methods [BREAK] method", "context": "Automatic <entity id=\"E03-1010.2\">Evaluation </entity> For A Palpable Measure Of A Speech Translation <entity id=\"E03-1010.4\">System </entity> 's Capability . The main goal of this paper is to propose automatic schemes for the translation paired comparison method . This method was proposed to precisely evaluate a speech translation system 's capability . Furthermore, the method gives an objective evaluation <entity id=\"E03-1010.25\">result </entity> , i.e., a score of the Test of English for International Communication (TOEIC). The TOEIC score is used as a measure of one's speech translation capability . However, this method requires tremendous evaluation costs . Accordingly, automatization of this method is an important subject for study . In the proposed method , currently available automatic evaluation <entity id=\"E03-1010.40\">methods </entity> are applied to automate the translation paired comparison method . In the experiments , several automatic evaluation methods (BLEU, NIST, DP-based method ) are applied . The experimental results of these automatic measures show a good correlation with evaluation <entity id=\"E03-1010.56\">results </entity> of the translation paired comparison method .", "tag": "USAGE"}, {"qas_id": "E03-1014.4_E03-1014.9", "question_text": "research [BREAK] transformation", "context": "Arabic Syntactic Trees: From Constituency To Dependency . This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=\"E03-1014.33\">tree </entity> . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.", "tag": "TOPIC"}, {"qas_id": "E03-1014.14_E03-1014.15", "question_text": "Guidelines [BREAK] descriptions", "context": "Arabic Syntactic Trees: From Constituency To Dependency . This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=\"E03-1014.33\">tree </entity> . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.", "tag": "USAGE"}, {"qas_id": "E03-1014.24_E03-1014.27", "question_text": "translating [BREAK] tree", "context": "Arabic Syntactic Trees: From Constituency To Dependency . This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=\"E03-1014.33\">tree </entity> . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.", "tag": "USAGE"}, {"qas_id": "E03-1014.31_E03-1014.32", "question_text": "nodes [BREAK] dependency <entity id=\"E03-1014.33\">tree", "context": "Arabic Syntactic Trees: From Constituency To Dependency . This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=\"E03-1014.33\">tree </entity> . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.", "tag": "PART_WHOLE"}, {"qas_id": "E03-1014.38_E03-1014.40", "question_text": "model [BREAK] transformation", "context": "Arabic Syntactic Trees: From Constituency To Dependency . This research note reports on the work in progress which regards automatic transformation of phrase-structure syntactic trees of Arabic into dependency-driven analytical ones. Guidelines for these descriptions have been developed at the Linguistic Data Consortium, University of Pennsylvania, and at the Faculty of Mathematics and Physics and the Faculty of Arts, Charles University in Prague, respectively. The transformation consists of (i) a recursive function translating the topology of a phrase tree into a corresponding dependency tree , and (ii) a procedure assigning analytical functions to the nodes of the dependency <entity id=\"E03-1014.33\">tree </entity> . Apart from an outline of the annotation schemes and a deeper insight into these procedures , model application of the transformation is given herein.", "tag": "MODEL-FEATURE"}, {"qas_id": "N03-1008.4_N03-1008.5", "question_text": "Models [BREAK] Speech <entity id=\"N03-1008.6\">Recognition", "context": "Latent Semantic Information In Maximum Entropy Language Models For Conversational Speech <entity id=\"N03-1008.6\">Recognition </entity> . Latent <entity id=\"N03-1008.8\">semantic analysis </entity> (LSA), first exploited in indexing documents for information <entity id=\"N03-1008.12\">retrieval </entity> , has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal . In this paper we present an investigation into the use of LSA in language <entity id=\"N03-1008.23\">modeling </entity> for conversational speech <entity id=\"N03-1008.25\">recognition </entity> . We find that previously proposed methods of combining an LSA-based unigram model with an N- feature", "tag": "USAGE"}, {"qas_id": "N03-1008.7_N03-1008.11", "question_text": "Latent <entity id=\"N03-1008.8\">semantic analysis [BREAK] information <entity id=\"N03-1008.12\">retrieval", "context": "Latent Semantic Information In Maximum Entropy Language Models For Conversational Speech <entity id=\"N03-1008.6\">Recognition </entity> . Latent <entity id=\"N03-1008.8\">semantic analysis </entity> (LSA), first exploited in indexing documents for information <entity id=\"N03-1008.12\">retrieval </entity> , has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal . In this paper we present an investigation into the use of LSA in language <entity id=\"N03-1008.23\">modeling </entity> for conversational speech <entity id=\"N03-1008.25\">recognition </entity> . We find that previously proposed methods of combining an LSA-based unigram model with an N- feature", "tag": "USAGE"}, {"qas_id": "N03-1008.20_N03-1008.21", "question_text": "paper [BREAK] investigation", "context": "Latent Semantic Information In Maximum Entropy Language Models For Conversational Speech <entity id=\"N03-1008.6\">Recognition </entity> . Latent <entity id=\"N03-1008.8\">semantic analysis </entity> (LSA), first exploited in indexing documents for information <entity id=\"N03-1008.12\">retrieval </entity> , has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal . In this paper we present an investigation into the use of LSA in language <entity id=\"N03-1008.23\">modeling </entity> for conversational speech <entity id=\"N03-1008.25\">recognition </entity> . We find that previously proposed methods of combining an LSA-based unigram model with an N- feature", "tag": "TOPIC"}, {"qas_id": "N03-1008.22_N03-1008.24", "question_text": "language <entity id=\"N03-1008.23\">modeling [BREAK] speech <entity id=\"N03-1008.25\">recognition", "context": "Latent Semantic Information In Maximum Entropy Language Models For Conversational Speech <entity id=\"N03-1008.6\">Recognition </entity> . Latent <entity id=\"N03-1008.8\">semantic analysis </entity> (LSA), first exploited in indexing documents for information <entity id=\"N03-1008.12\">retrieval </entity> , has since been used by several researchers to demonstrate impressive reductions in the perplexity of statistical language models on text corpora such as the Wall Street Journal . In this paper we present an investigation into the use of LSA in language <entity id=\"N03-1008.23\">modeling </entity> for conversational speech <entity id=\"N03-1008.25\">recognition </entity> . We find that previously proposed methods of combining an LSA-based unigram model with an N- feature", "tag": "USAGE"}, {"qas_id": "N03-1013.8_N03-1013.10", "question_text": "variations [BREAK] database", "context": "A Categorial Variation Database For English . \"We describe our approach to the construction and evaluation of a large-scale database called \"\"CatVar\"\" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . Thus, the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold <entity id=\"N03-1013.38\">standard </entity> . This evaluation reveals that the categorial database achieves a high degree of precision and recall . Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%. \"", "tag": "PART_WHOLE"}, {"qas_id": "N03-1013.17_N03-1013.20", "question_text": "resource [BREAK] applications", "context": "A Categorial Variation Database For English . \"We describe our approach to the construction and evaluation of a large-scale database called \"\"CatVar\"\" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . Thus, the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold <entity id=\"N03-1013.38\">standard </entity> . This evaluation reveals that the categorial database achieves a high degree of precision and recall . Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%. \"", "tag": "PART_WHOLE"}, {"qas_id": "N03-1013.33_N03-1013.37", "question_text": "accuracy [BREAK] gold <entity id=\"N03-1013.38\">standard", "context": "A Categorial Variation Database For English . \"We describe our approach to the construction and evaluation of a large-scale database called \"\"CatVar\"\" which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . Thus, the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold <entity id=\"N03-1013.38\">standard </entity> . This evaluation reveals that the categorial database achieves a high degree of precision and recall . Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%. \"", "tag": "COMPARE"}, {"qas_id": "M91-1007.10_M91-1007.11", "question_text": "analysis [BREAK] result", "context": "GE NLTOOLSET: MUC-3 Test Results And Analysis . This paper reports on the GE NLTOOLSET customization effort for MUC-3, and analyzes the results of the TST2 run. Although our own tests had shown steady improvement between TST1 and TST2, our official scores on TST2 were lo wer than on TST1. The analysis of this unexpected result explains some of the details of th e MUC-3 test , and we propose ways of looking at the scores to distinguish different aspects of system performance .", "tag": "TOPIC"}, {"qas_id": "M92-1004.7_M92-1004.8", "question_text": "paper [BREAK] text", "context": "Text Filtering In MUC-3 And MUC-4 . \"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"\" text filtering\"\" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=\"M92-1004.27\">systems </entity> in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. \"", "tag": "TOPIC"}, {"qas_id": "M92-1004.22_M92-1004.23", "question_text": "components [BREAK] systems", "context": "Text Filtering In MUC-3 And MUC-4 . \"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"\" text filtering\"\" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=\"M92-1004.27\">systems </entity> in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. \"", "tag": "PART_WHOLE"}, {"qas_id": "M92-1004.24_M92-1004.26", "question_text": "scheme [BREAK] extraction <entity id=\"M92-1004.27\">systems", "context": "Text Filtering In MUC-3 And MUC-4 . \"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"\" text filtering\"\" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=\"M92-1004.27\">systems </entity> in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "M92-1004.34_M92-1004.35", "question_text": "approaches [BREAK] evaluation", "context": "Text Filtering In MUC-3 And MUC-4 . \"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"\" text filtering\"\" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=\"M92-1004.27\">systems </entity> in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. \"", "tag": "USAGE"}, {"qas_id": "M92-1004.58_M92-1004.60", "question_text": "generality [BREAK] precision", "context": "Text Filtering In MUC-3 And MUC-4 . \"One of the changes from the Third (MUC-3) to the Fourth (MUC-4) Message Understanding Conference was the emergence of text filtering as an explicit topic of discussion . In this paper we examine text filtering in MUC systems with three goals in mind. First, we clarify the difference between two uses of the term \"\" text filtering\"\" in the context of data extraction systems , and put these phenomena in the context of prior research on information retrieval (IR). Secondly, we discuss the use of text filtering components in MUC-3 and MUC-4 systems , and present a preliminary scheme for classifying data extraction <entity id=\"M92-1004.27\">systems </entity> in terms of the features over which they do text filtering. Finally, we examine the text filtering effectiveness of MUC-3 and MUC-4 systems , and introduce some approaches to the evaluation of text filtering systems which may be of interest themselves. Two questions of crucial interest are whether sites improved their system level text filtering effectiveness from MUC-3 to MUC-4, and what the effectiveness of MUC systems would be on real world data streams . Because of changes in both test set and system design since MUC-3 we were not able to address the first question . However, with respect to the second question , we present preliminary evidence suggesting that the text filtering precision of MUC systems declines with the generality of the data stream they process , i.e. the proportion of relevant documents . The ramifications of this for future research and for operational systems are discussed. \"", "tag": "RESULT"}, {"qas_id": "X93-1015.2_X93-1015.3", "question_text": "Design [BREAK] Information <entity id=\"X93-1015.4\">Extraction", "context": "Template Design For Information <entity id=\"X93-1015.4\">Extraction </entity> . The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text . This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case <entity id=\"X93-1015.24\">Study </entity> in the last section of this paper .", "tag": "USAGE"}, {"qas_id": "X93-1015.6_X93-1015.8", "question_text": "template [BREAK] application", "context": "Template Design For Information <entity id=\"X93-1015.4\">Extraction </entity> . The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text . This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case <entity id=\"X93-1015.24\">Study </entity> in the last section of this paper .", "tag": "USAGE"}, {"qas_id": "X93-1015.15_X93-1015.19", "question_text": "paper [BREAK] principles", "context": "Template Design For Information <entity id=\"X93-1015.4\">Extraction </entity> . The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text . This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case <entity id=\"X93-1015.24\">Study </entity> in the last section of this paper .", "tag": "TOPIC"}, {"qas_id": "X93-1015.23_X93-1015.25", "question_text": "section [BREAK] Case <entity id=\"X93-1015.24\">Study", "context": "Template Design For Information <entity id=\"X93-1015.4\">Extraction </entity> . The design of the template for an information extraction application (or exercise) reflects the nature of the task and therefore crucially affects the success of the attempt to capture information from text . This paper addresses the template design requirement by discussing the general principles or template definition effort which is explicitly discussed in a Case <entity id=\"X93-1015.24\">Study </entity> in the last section of this paper .", "tag": "TOPIC"}, {"qas_id": "W90-0111.6_W90-0111.7", "question_text": "paper [BREAK] issues", "context": "Selection : Salience, Relevance And The Coupling Between Domain- Level Tasks And Text Planning . In this paper we examine some issues pertaining to the task of selection in text planning. We attempt to distinguish salience and relevance , and characterize their role as important fundamental notions governing selection . We also formulate the problem of selection of text content in terms of the coupling between domain-level tasks and text planning tasks . We describe our research on generating bus route descriptions .Keywords: Natural Language Generation , Text Planning, Selection , Salience, Relevance , Coupling, Route Descriptions", "tag": "TOPIC"}, {"qas_id": "W90-0111.24_W90-0111.27", "question_text": "research [BREAK] descriptions", "context": "Selection : Salience, Relevance And The Coupling Between Domain- Level Tasks And Text Planning . In this paper we examine some issues pertaining to the task of selection in text planning. We attempt to distinguish salience and relevance , and characterize their role as important fundamental notions governing selection . We also formulate the problem of selection of text content in terms of the coupling between domain-level tasks and text planning tasks . We describe our research on generating bus route descriptions .Keywords: Natural Language Generation , Text Planning, Selection , Salience, Relevance , Coupling, Route Descriptions", "tag": "TOPIC"}, {"qas_id": "W93-0213.5_W93-0213.6", "question_text": "approaches [BREAK] discourse <entity id=\"W93-0213.7\">analysis", "context": "Using Cue Phrases To Determine Rhetorical Relations . ' Relation based ' approaches to discourse <entity id=\"W93-0213.7\">analysis </entity> and text generation suffer from a common problem : there is considerable disagreement between researchers over the set of relations which is proposed . Few researchers use identical sets of relations , and many use relations not found in any other sets. This proliferation of relations has been pointed out before (eg Hovy [1]), and several methods for justifying a standard set of relations have been proposed : this paper reviews some of these, and presents a new method of justification which overcomes some awkward problems .", "tag": "USAGE"}, {"qas_id": "W93-0213.23_W93-0213.25", "question_text": "paper [BREAK] method", "context": "Using Cue Phrases To Determine Rhetorical Relations . ' Relation based ' approaches to discourse <entity id=\"W93-0213.7\">analysis </entity> and text generation suffer from a common problem : there is considerable disagreement between researchers over the set of relations which is proposed . Few researchers use identical sets of relations , and many use relations not found in any other sets. This proliferation of relations has been pointed out before (eg Hovy [1]), and several methods for justifying a standard set of relations have been proposed : this paper reviews some of these, and presents a new method of justification which overcomes some awkward problems .", "tag": "TOPIC"}, {"qas_id": "W93-0231.5_W93-0231.6", "question_text": "structure [BREAK] text", "context": "Domain Structure , Rhetorical Structure , And Text Structure . It is generally agreed that text has structure (at least, coherent text does). Therefore, an understanding and appreciation of text <entity id=\"W93-0231.10\">structure </entity> must play some role in building computational systems that are capable of using text as people do. What is less clear is what are necessary and sufficient sources of structure for a text-using system , and further, what such a system needs to know about and do with these structures in the process of using text . By using text , I mean understanding it or producing it; speaking it, writing it, or thinking about it. Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.", "tag": "MODEL-FEATURE"}, {"qas_id": "W93-0231.8_W93-0231.9", "question_text": "understanding [BREAK] text <entity id=\"W93-0231.10\">structure", "context": "Domain Structure , Rhetorical Structure , And Text Structure . It is generally agreed that text has structure (at least, coherent text does). Therefore, an understanding and appreciation of text <entity id=\"W93-0231.10\">structure </entity> must play some role in building computational systems that are capable of using text as people do. What is less clear is what are necessary and sufficient sources of structure for a text-using system , and further, what such a system needs to know about and do with these structures in the process of using text . By using text , I mean understanding it or producing it; speaking it, writing it, or thinking about it. Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.", "tag": "TOPIC"}, {"qas_id": "W93-0231.22_W93-0231.23", "question_text": "paper [BREAK] case", "context": "Domain Structure , Rhetorical Structure , And Text Structure . It is generally agreed that text has structure (at least, coherent text does). Therefore, an understanding and appreciation of text <entity id=\"W93-0231.10\">structure </entity> must play some role in building computational systems that are capable of using text as people do. What is less clear is what are necessary and sufficient sources of structure for a text-using system , and further, what such a system needs to know about and do with these structures in the process of using text . By using text , I mean understanding it or producing it; speaking it, writing it, or thinking about it. Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.", "tag": "TOPIC"}, {"qas_id": "W93-0231.26_W93-0231.28", "question_text": "structure [BREAK] text", "context": "Domain Structure , Rhetorical Structure , And Text Structure . It is generally agreed that text has structure (at least, coherent text does). Therefore, an understanding and appreciation of text <entity id=\"W93-0231.10\">structure </entity> must play some role in building computational systems that are capable of using text as people do. What is less clear is what are necessary and sufficient sources of structure for a text-using system , and further, what such a system needs to know about and do with these structures in the process of using text . By using text , I mean understanding it or producing it; speaking it, writing it, or thinking about it. Li this paper , I present a case for the importance of domain structure in structuring text , and discuss the role of rhetorical structure and intentionality.", "tag": "RESULT"}, {"qas_id": "W93-0311.3_W93-0311.5", "question_text": "Adaptation [BREAK] Disambiguation", "context": "Corpus- Based Adaptation Mechanisms For Chinese Homophone Disambiguation . Based on the concepts of bidirectional conversion and automatic evaluation , we propose two nser-adapiation mechanisms , character-preference learning and pstlido-word learning , for resolving Chinese homophone ambiguities in syllable-to-character conversion . The 1991 United Daily corpus oj approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character bi-grams. Experiments show that 20.5 percent (testing sets) to 71.8 percent ( training sets ) of conversion errors can be eliminated through the proposed mechanisms . These concepts are thus very useful m applications such as Chinese input methods and speech recognition systems", "tag": "USAGE"}, {"qas_id": "W93-0311.17_W93-0311.18", "question_text": "ambiguities [BREAK] conversion", "context": "Corpus- Based Adaptation Mechanisms For Chinese Homophone Disambiguation . Based on the concepts of bidirectional conversion and automatic evaluation , we propose two nser-adapiation mechanisms , character-preference learning and pstlido-word learning , for resolving Chinese homophone ambiguities in syllable-to-character conversion . The 1991 United Daily corpus oj approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character bi-grams. Experiments show that 20.5 percent (testing sets) to 71.8 percent ( training sets ) of conversion errors can be eliminated through the proposed mechanisms . These concepts are thus very useful m applications such as Chinese input methods and speech recognition systems", "tag": "MODEL-FEATURE"}, {"qas_id": "W93-0311.19_W93-0311.22", "question_text": "databases [BREAK] corpus", "context": "Corpus- Based Adaptation Mechanisms For Chinese Homophone Disambiguation . Based on the concepts of bidirectional conversion and automatic evaluation , we propose two nser-adapiation mechanisms , character-preference learning and pstlido-word learning , for resolving Chinese homophone ambiguities in syllable-to-character conversion . The 1991 United Daily corpus oj approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character bi-grams. Experiments show that 20.5 percent (testing sets) to 71.8 percent ( training sets ) of conversion errors can be eliminated through the proposed mechanisms . These concepts are thus very useful m applications such as Chinese input methods and speech recognition systems", "tag": "PART_WHOLE"}, {"qas_id": "W95-0109.5_W95-0109.6", "question_text": "paper [BREAK] approach", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "TOPIC"}, {"qas_id": "W95-0109.16_W95-0109.18", "question_text": "dictionary [BREAK] corpus", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "PART_WHOLE"}, {"qas_id": "W95-0109.19_W95-0109.22", "question_text": "information [BREAK] corpus", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "PART_WHOLE"}, {"qas_id": "W95-0109.24_W95-0109.26", "question_text": "technique [BREAK] model", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "USAGE"}, {"qas_id": "W95-0109.32_W95-0109.34", "question_text": "language <entity id=\"W95-0109.35\">model [BREAK] process", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "USAGE"}, {"qas_id": "W95-0109.36_W95-0109.37", "question_text": "parameters [BREAK] tagging", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "USAGE"}, {"qas_id": "W95-0109.41_W95-0109.42", "question_text": "classifier [BREAK] n-gram", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "USAGE"}, {"qas_id": "W95-0109.56_W95-0109.60", "question_text": "module [BREAK] configurations", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "PART_WHOLE"}, {"qas_id": "W95-0109.75_W95-0109.76", "question_text": "identification [BREAK] precision", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "RESULT"}, {"qas_id": "W95-0109.79_W95-0109.82", "question_text": "classifier [BREAK] list", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "USAGE"}, {"qas_id": "W95-0109.86_W95-0109.91", "question_text": "part of <entity id=\"W95-0109.87\">speech [BREAK] rates", "context": "Automatic Construction Of A Chinese Electronic Dictionary . In this paper , an unsupervised approach for constructing a large-scale Chinese electronic dictionary is surveyed . The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from a large untagged text corpus with the aid of the information in a small tagged seed corpus . The basic model is based on a Viterbi reestimation technique . During the dictionary construction process , it tries to optimize the automatic segmentation and tagging process by repeatedly refining the set of parameters of the underlying language <entity id=\"W95-0109.35\">model </entity> . The refined parameters are then used to further get a better tagging result . In addition , a two-class classifier , which is capable of classifying an n-gram either as a word or a non-word , is used in combination with the Viterbi training module to improve the system performance . Two different system configurations had been developed to construct the dictionary . The configurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging module and (2) a two-class classification module as the postfilter for the above Viterbi word identification module . With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences , the performance for bigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier is applied to the word list suggested by the Viterbi word identification module . The Viterbi part of <entity id=\"W95-0109.87\">speech tag </entity> reestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and 73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences .", "tag": "RESULT"}, {"qas_id": "W96-0201.1_W96-0201.2", "question_text": "Approach [BREAK] Mapping", "context": "A Geometric Approach To Mapping Bitext Correspondence . The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation . Several automatic methods for this task have been proposed in recent years. Yet even the best of these methods can err by several typeset pages . The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm . SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4. Its robustness has enabled new commercial-quality applications . The greedy nature of the algorithm makes it independent of memory resources . Unlike other bitext mapping algorithms , SIMR allows crossing correspondences to account for word order differences . Its output can be converted quickly and easily into a sentence alignment . SIMR's output has been used to align more than 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium.", "tag": "USAGE"}, {"qas_id": "W96-0201.8_W96-0201.9", "question_text": "map [BREAK] correspondence", "context": "A Geometric Approach To Mapping Bitext Correspondence . The first step in most corpus-based multilingual NLP work is to construct a detailed map of the correspondence between a text and its translation . Several automatic methods for this task have been proposed in recent years. Yet even the best of these methods can err by several typeset pages . The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm . SIMR's errors are smaller than those of the previous front-runner by more than a factor of 4. Its robustness has enabled new commercial-quality applications . The greedy nature of the algorithm makes it independent of memory resources . Unlike other bitext mapping algorithms , SIMR allows crossing correspondences to account for word order differences . Its output can be converted quickly and easily into a sentence alignment . SIMR's output has been used to align more than 200 megabytes of the Canadian Hansards for publication by the Linguistic Data Consortium.", "tag": "MODEL-FEATURE"}, {"qas_id": "W96-0210.4_W96-0210.6", "question_text": "classifier [BREAK] test set", "context": "The Measure Of A Model . This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set . These determinants are the appropriateness , for the test set , of the results of (1) feature selection , (2) formulation of the parametric form of the model , and (3) parameter estimation . These are part of any model formulation procedure , even if not broken out as separate steps , so the tradeoffs explored in this paper are relevant to a wide variety of methods . The measures are demonstrated in a large experiment , in which they are used to analyze the results of roughly 300 classifiers that perform <entity id=\"W96-0210.28\">word-sense disambiguation </entity> .", "tag": "USAGE"}, {"qas_id": "W96-0210.7_W96-0210.9", "question_text": "appropriateness [BREAK] results", "context": "The Measure Of A Model . This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set . These determinants are the appropriateness , for the test set , of the results of (1) feature selection , (2) formulation of the parametric form of the model , and (3) parameter estimation . These are part of any model formulation procedure , even if not broken out as separate steps , so the tradeoffs explored in this paper are relevant to a wide variety of methods . The measures are demonstrated in a large experiment , in which they are used to analyze the results of roughly 300 classifiers that perform <entity id=\"W96-0210.28\">word-sense disambiguation </entity> .", "tag": "MODEL-FEATURE"}, {"qas_id": "W96-0210.25_W96-0210.27", "question_text": "classifiers [BREAK] <entity id=\"W96-0210.28\">word-sense", "context": "The Measure Of A Model . This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set . These determinants are the appropriateness , for the test set , of the results of (1) feature selection , (2) formulation of the parametric form of the model , and (3) parameter estimation . These are part of any model formulation procedure , even if not broken out as separate steps , so the tradeoffs explored in this paper are relevant to a wide variety of methods . The measures are demonstrated in a large experiment , in which they are used to analyze the results of roughly 300 classifiers that perform <entity id=\"W96-0210.28\">word-sense disambiguation </entity> .", "tag": "USAGE"}, {"qas_id": "W96-0404.2_W96-0404.3", "question_text": "paper [BREAK] technique", "context": "Approximate Generation From Non-Hierarchical Representations . This paper presents a technique for sentence generation . We argue that the input to generators should have a non-hierarchical nature . This allows us to investigate a more general version of the sentence generation problem where one is not pre-committed to a choice of the syntactically prominent elements in the initial semantics . We also consider that a generator can happen to convey more (or less) information than is originally specified in its semantic input . In order to constrain this approximate matching of the input we impose additional restrictions on the semantics of the generated sentence . Our technique provides flexibility to address cases where the entire input cannot be precisely expressed in a single sentence . Thus the generator does not rely on the strategic component having linguistic knowledge . We show clearly how the semantic <entity id=\"W96-0404.33\">structure </entity> is declaratively related to linguistically motivated syntactic <entity id=\"W96-0404.35\">representation </entity> .", "tag": "TOPIC"}, {"qas_id": "W96-0404.32_W96-0404.34", "question_text": "semantic <entity id=\"W96-0404.33\">structure [BREAK] syntactic <entity id=\"W96-0404.35\">representation", "context": "Approximate Generation From Non-Hierarchical Representations . This paper presents a technique for sentence generation . We argue that the input to generators should have a non-hierarchical nature . This allows us to investigate a more general version of the sentence generation problem where one is not pre-committed to a choice of the syntactically prominent elements in the initial semantics . We also consider that a generator can happen to convey more (or less) information than is originally specified in its semantic input . In order to constrain this approximate matching of the input we impose additional restrictions on the semantics of the generated sentence . Our technique provides flexibility to address cases where the entire input cannot be precisely expressed in a single sentence . Thus the generator does not rely on the strategic component having linguistic knowledge . We show clearly how the semantic <entity id=\"W96-0404.33\">structure </entity> is declaratively related to linguistically motivated syntactic <entity id=\"W96-0404.35\">representation </entity> .", "tag": "MODEL-FEATURE"}, {"qas_id": "W96-0405.5_W96-0405.8", "question_text": "paper [BREAK] language", "context": "Input Specification In The WAG Sentence Generation System . This paper describes the input specification language of the WAG Sentence Generation system . The input is described in terms of Halliday 's (1978) three meaning components , ideational meaning (the propositional content to be expressed), interactional meaning (what the speaker intends the listener to do in making the utterance ), and textual meaning (how the content is structured as a message , in terms of theme, reference , etc.).", "tag": "TOPIC"}, {"qas_id": "W97-0406.8_W97-0406.10", "question_text": "interfaces [BREAK] translation <entity id=\"W97-0406.11\">systems", "context": "Dealing With Multilinguality In A Spoken Language Query Translator . Robustness is an important issue for multilingual speech interfaces for spoken language translation <entity id=\"W97-0406.11\">systems </entity> . We have studied three aspects of robustness in such a system : accent differences , mixed language input , and the use of common feature sets for HMM-based speech recognizers for English and Cantonese. The results of our preliminary experiments show that accent differences cause recognizer performance to degrade . A rather surprising finding is that for mixed language input , a straight forward implementation of a mixed language <entity id=\"W97-0406.32\">model-based </entity> speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .", "tag": "USAGE"}, {"qas_id": "W97-0406.25_W97-0406.26", "question_text": "differences [BREAK] performance", "context": "Dealing With Multilinguality In A Spoken Language Query Translator . Robustness is an important issue for multilingual speech interfaces for spoken language translation <entity id=\"W97-0406.11\">systems </entity> . We have studied three aspects of robustness in such a system : accent differences , mixed language input , and the use of common feature sets for HMM-based speech recognizers for English and Cantonese. The results of our preliminary experiments show that accent differences cause recognizer performance to degrade . A rather surprising finding is that for mixed language input , a straight forward implementation of a mixed language <entity id=\"W97-0406.32\">model-based </entity> speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .", "tag": "RESULT"}, {"qas_id": "W97-0406.31_W97-0406.35", "question_text": "language <entity id=\"W97-0406.32\">model-based [BREAK] language", "context": "Dealing With Multilinguality In A Spoken Language Query Translator . Robustness is an important issue for multilingual speech interfaces for spoken language translation <entity id=\"W97-0406.11\">systems </entity> . We have studied three aspects of robustness in such a system : accent differences , mixed language input , and the use of common feature sets for HMM-based speech recognizers for English and Cantonese. The results of our preliminary experiments show that accent differences cause recognizer performance to degrade . A rather surprising finding is that for mixed language input , a straight forward implementation of a mixed language <entity id=\"W97-0406.32\">model-based </entity> speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .", "tag": "COMPARE"}, {"qas_id": "W97-0406.38_W97-0406.44", "question_text": "common [BREAK] output", "context": "Dealing With Multilinguality In A Spoken Language Query Translator . Robustness is an important issue for multilingual speech interfaces for spoken language translation <entity id=\"W97-0406.11\">systems </entity> . We have studied three aspects of robustness in such a system : accent differences , mixed language input , and the use of common feature sets for HMM-based speech recognizers for English and Cantonese. The results of our preliminary experiments show that accent differences cause recognizer performance to degrade . A rather surprising finding is that for mixed language input , a straight forward implementation of a mixed language <entity id=\"W97-0406.32\">model-based </entity> speech recognizer performs less well than the concatenation of pure language recognizers. Our experimental results also show that a common feature set , parameter set, and common algorithm lead to different performance output for Cantonese and English speech recognition modules .", "tag": "RESULT"}, {"qas_id": "W97-0408.3_W97-0408.6", "question_text": "model [BREAK] translation <entity id=\"W97-0408.7\">system", "context": "English- To-Mandarin Speech Translation With Head Transducers . We describe the head transducer model used in an experimental English-to- Mandarin speech translation <entity id=\"W97-0408.7\">system </entity> . Head transduction is a translation method in which weighted finite state transducers are associated with source-target word pairs . The method is suitable for speech <entity id=\"W97-0408.16\">translation </entity> because it allows efficient bottom up processing . The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency .", "tag": "USAGE"}, {"qas_id": "W97-0408.14_W97-0408.15", "question_text": "method [BREAK] speech <entity id=\"W97-0408.16\">translation", "context": "English- To-Mandarin Speech Translation With Head Transducers . We describe the head transducer model used in an experimental English-to- Mandarin speech translation <entity id=\"W97-0408.7\">system </entity> . Head transduction is a translation method in which weighted finite state transducers are associated with source-target word pairs . The method is suitable for speech <entity id=\"W97-0408.16\">translation </entity> because it allows efficient bottom up processing . The head transducers in the experimental system have a wider range of output positions than input positions. This asymmetry is motivated by a tradeoff between model complexity and search efficiency .", "tag": "USAGE"}, {"qas_id": "W97-0615.5_W97-0615.7", "question_text": "language <entity id=\"W97-0615.6\">processing [BREAK] dialogue <entity id=\"W97-0615.8\">systems", "context": "Filtering Errors And Repairing Linguistic Anomalies For Spoken Dialogue Systems . Our work addresses the integration of speech recognition and language <entity id=\"W97-0615.6\">processing </entity> for whole spoken dialogue <entity id=\"W97-0615.8\">systems </entity> .To filter ill-recognized words , we design an on-line computing of word confidence scores based on the recognizer output hypothesis . To infer as much information as possible from the retained sequence of words , we propose a bottom-up syntactico-semantic robust parsing relying on a lexi-calized tree grammar and on integrated repairing strategies .", "tag": "USAGE"}, {"qas_id": "W97-0615.23_W97-0615.26", "question_text": "strategies [BREAK] parsing", "context": "Filtering Errors And Repairing Linguistic Anomalies For Spoken Dialogue Systems . Our work addresses the integration of speech recognition and language <entity id=\"W97-0615.6\">processing </entity> for whole spoken dialogue <entity id=\"W97-0615.8\">systems </entity> .To filter ill-recognized words , we design an on-line computing of word confidence scores based on the recognizer output hypothesis . To infer as much information as possible from the retained sequence of words , we propose a bottom-up syntactico-semantic robust parsing relying on a lexi-calized tree grammar and on integrated repairing strategies .", "tag": "USAGE"}, {"qas_id": "W97-0618.1_W97-0618.4", "question_text": "Architecture [BREAK] Systems", "context": "A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems . In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.", "tag": "USAGE"}, {"qas_id": "W97-0618.8_W97-0618.10", "question_text": "components [BREAK] tasks", "context": "A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems . In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.", "tag": "USAGE"}, {"qas_id": "W97-0618.32_W97-0618.34", "question_text": "variables [BREAK] rules", "context": "A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems . In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.", "tag": "PART_WHOLE"}, {"qas_id": "W97-0618.37_W97-0618.39", "question_text": "representations [BREAK] information", "context": "A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems . In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.", "tag": "MODEL-FEATURE"}, {"qas_id": "W97-0618.41_W97-0618.42", "question_text": "specificity [BREAK] rule", "context": "A Programmable Multi-Blackboard Architecture For Dialogue Processing Systems . In current Natural Language Processing Systems , different components for different processing tasks and input / output modalities have to be integrated. Once integrated, the interactions between the components have to be specified. Interactions in dialogue systems can be complex due in part to the many states the system can be in. When porting the system to another domain , parts of the integration process have to be repeated. To overcome these difficulties , we propose a multi-blackboard architecture that is controlled by a set of expert-system like rules . These rules may contain typed variables . Variables can be substituted by representations with an appropriate type stored in the blackboards. Furthermore, the representations in the blackboards allow to represent partial information and to leave disjunctions unresolved. Moreover, the conditions of the rule may depend on the specificity of the representations with which the variables are instantiated. For this reason , the interaction is information-driven . The described system has been implemented and has been integrated with the speech recognizer JANUS.", "tag": "USAGE"}, {"qas_id": "J82-1001.2_J82-1001.3", "question_text": "paper [BREAK] results", "context": "Phrase Structure Trees Bear More Fruit Than You Would Have Thought . In this paper we will present several results concerning phrase structure trees . These results show that phrase structure trees , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints . We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen . We have also presented some results on skeletons ( phrase structure trees without labels) which show that phrase <entity id=\"J82-1001.20\">structure trees </entity> , even when deprived of the labels, retain in a certain sense all the structural information . This result has implications for grammatical inference procedures .", "tag": "TOPIC"}, {"qas_id": "J82-1001.11_J82-1001.12", "question_text": "theorem [BREAK] constraints", "context": "Phrase Structure Trees Bear More Fruit Than You Would Have Thought . In this paper we will present several results concerning phrase structure trees . These results show that phrase structure trees , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints . We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen . We have also presented some results on skeletons ( phrase structure trees without labels) which show that phrase <entity id=\"J82-1001.20\">structure trees </entity> , even when deprived of the labels, retain in a certain sense all the structural information . This result has implications for grammatical inference procedures .", "tag": "TOPIC"}, {"qas_id": "J82-1001.14_J82-1001.16", "question_text": "approach [BREAK] framework", "context": "Phrase Structure Trees Bear More Fruit Than You Would Have Thought . In this paper we will present several results concerning phrase structure trees . These results show that phrase structure trees , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints . We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen . We have also presented some results on skeletons ( phrase structure trees without labels) which show that phrase <entity id=\"J82-1001.20\">structure trees </entity> , even when deprived of the labels, retain in a certain sense all the structural information . This result has implications for grammatical inference procedures .", "tag": "COMPARE"}, {"qas_id": "J82-1001.19_J82-1001.24", "question_text": "phrase <entity id=\"J82-1001.20\">structure [BREAK] information", "context": "Phrase Structure Trees Bear More Fruit Than You Would Have Thought . In this paper we will present several results concerning phrase structure trees . These results show that phrase structure trees , when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints . We have compared the local constraints approach to some aspects of Gazdar 's framework and that of Peters and Ritchie and of Karttunen . We have also presented some results on skeletons ( phrase structure trees without labels) which show that phrase <entity id=\"J82-1001.20\">structure trees </entity> , even when deprived of the labels, retain in a certain sense all the structural information . This result has implications for grammatical inference procedures .", "tag": "MODEL-FEATURE"}, {"qas_id": "J82-2002.8_J82-2002.10", "question_text": "languages [BREAK] computers", "context": "Natural- Language Interface . A major problem faced by would-be users of computer systems is that computers generally make use of special-purpose languages familiar only to those trained in computer science . For a large number of applications requiring interaction between humans and computer systems , it would be highly desirable for machines to converse in English or other natural languages familiar to their human users . Over the last decade, in laboratories around the world, several computer systems have been developed that support at least elementary levels of natural-language interaction . Among these are such systems as those described in the several references at the end of this paper .", "tag": "USAGE"}, {"qas_id": "J82-2002.32_J82-2002.33", "question_text": "paper [BREAK] references", "context": "Natural- Language Interface . A major problem faced by would-be users of computer systems is that computers generally make use of special-purpose languages familiar only to those trained in computer science . For a large number of applications requiring interaction between humans and computer systems , it would be highly desirable for machines to converse in English or other natural languages familiar to their human users . Over the last decade, in laboratories around the world, several computer systems have been developed that support at least elementary levels of natural-language interaction . Among these are such systems as those described in the several references at the end of this paper .", "tag": "TOPIC"}, {"qas_id": "P98-1099.3_P98-1099.4", "question_text": "Lexicon [BREAK] Natural <entity id=\"P98-1099.5\">Language Generation", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "USAGE"}, {"qas_id": "P98-1099.6_P98-1099.8", "question_text": "lexicon [BREAK] generation <entity id=\"P98-1099.9\">system", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "PART_WHOLE"}, {"qas_id": "P98-1099.15_P98-1099.16", "question_text": "paper [BREAK] lexicon", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "TOPIC"}, {"qas_id": "P98-1099.26_P98-1099.27", "question_text": "lexicon [BREAK] lexical <entity id=\"P98-1099.28\">choice", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "USAGE"}, {"qas_id": "P98-1099.31_P98-1099.35", "question_text": "architecture [BREAK] generation", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "USAGE"}, {"qas_id": "P98-1099.36_P98-1099.41", "question_text": "integration [BREAK] errors", "context": "Combining Multiple, Large- Scale Resources in a Reusable Lexicon for Natural <entity id=\"P98-1099.5\">Language Generation </entity> . A lexicon is an essential component in a generation <entity id=\"P98-1099.9\">system </entity> but few efforts have been made to build a rich, large-scale lexicon and make it reusable for different generation applications . In this paper , we describe our work to build such a lexicon by combining multiple, heterogeneous linguistic resources which have been developed for other purposes . Novel transformation and integration of resources is required to reuse them for generation . We also applied the lexicon to the lexical <entity id=\"P98-1099.28\">choice </entity> and realization component of a practical generation application by using a multi-level feedback architecture . The integration of the lexicon and the architecture is able to effectively improve the system paraphrasing power, minimize the chance of grammatical errors , and simplify the development process substantially.", "tag": "RESULT"}, {"qas_id": "P98-1107.2_P98-1107.6", "question_text": "Co-occurrence [BREAK] Speech <entity id=\"P98-1107.3\">Recognition", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "USAGE"}, {"qas_id": "P98-1107.7_P98-1107.8", "question_text": "errors [BREAK] results", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-1107.11_P98-1107.12", "question_text": "translation system [BREAK] performance", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "RESULT"}, {"qas_id": "P98-1107.15_P98-1107.16", "question_text": "method [BREAK] errors", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "USAGE"}, {"qas_id": "P98-1107.23_P98-1107.24", "question_text": "processes [BREAK] method", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "PART_WHOLE"}, {"qas_id": "P98-1107.25_P98-1107.26", "question_text": "pairs [BREAK] process", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "USAGE"}, {"qas_id": "P98-1107.28_P98-1107.30", "question_text": "speech <entity id=\"P98-1107.31\">recognition [BREAK] string", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "RESULT"}, {"qas_id": "P98-1107.32_P98-1107.34", "question_text": "string [BREAK] utterance", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "MODEL-FEATURE"}, {"qas_id": "P98-1107.35_P98-1107.37", "question_text": "pairs [BREAK] database", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "PART_WHOLE"}, {"qas_id": "P98-1107.39_P98-1107.40", "question_text": "string [BREAK] process", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "USAGE"}, {"qas_id": "P98-1107.42_P98-1107.45", "question_text": "errors [BREAK] string", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "PART_WHOLE"}, {"qas_id": "P98-1107.50_P98-1107.51", "question_text": "post-processor [BREAK] speech <entity id=\"P98-1107.52\">recognition", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "USAGE"}, {"qas_id": "P98-1107.56_P98-1107.58", "question_text": "method [BREAK] segments", "context": "A Method for Correcting Errors in Speech <entity id=\"P98-1107.3\">Recognition </entity> using the Statistical Features of Character Co-occurrence . It is important to correct the errors in the results of speech recognition to increase the performance of a speech translation system . This paper proposes a method for correcting errors using the statistical features of character co-occurrence , and evaluates the method . The proposed method comprises two successive correcting processes . The first process uses pairs of strings : the first string is an erroneous substring of the utterance predicted by speech <entity id=\"P98-1107.31\">recognition </entity> , the second string is the corresponding section of the actual utterance . Errors are detected and corrected according to the database learned from erroneous-correct utterance pairs . The remaining errors are passed to the posterior process which uses a string in the corpus that is similar to the string including recognition errors . The results of our evaluation show that the use of our proposed method as a post-processor for speech <entity id=\"P98-1107.52\">recognition </entity> is likely to make a significant contribution to the performance of speech translation systems . method also obtains reliably recognized partial segments of an utterance by cooperatively using both grammatical and n-gram based statistical language constraints , and uses a robust parsing technique to apply the grammatical constraints described by context-free grammar ( Tsukada", "tag": "RESULT"}, {"qas_id": "P98-2134.7_P98-2134.10", "question_text": "alignment [BREAK] segments", "context": "Bitext Correspondences through Rich Mark-up. Rich mark-up can considerably benefit the process of establishing bitext correspondences , that is, the task of providing correct identification and alignment methods for text segments that are translation equivalences of each other in a parallel corpus . We present a sentence alignment algorithm that, by taking advantage of previously annotated texts , obtains accuracy rates close to 100%. The algorithm evaluates the similarity of the linguistic and extra-linguistic mark-up in both sides of a bitext. Given that annotations are neutral with respect to typological, grammatical and orthographical differences between languages , rich mark-up becomes an optimal foundation to support bitext correspondences . The main originality of this approach is that it makes maximal use of annotations, which is a very sensible and efficient method for the exploitation of parallel corpora when annotations exist.", "tag": "USAGE"}, {"qas_id": "P98-2134.16_P98-2134.19", "question_text": "algorithm [BREAK] accuracy", "context": "Bitext Correspondences through Rich Mark-up. Rich mark-up can considerably benefit the process of establishing bitext correspondences , that is, the task of providing correct identification and alignment methods for text segments that are translation equivalences of each other in a parallel corpus . We present a sentence alignment algorithm that, by taking advantage of previously annotated texts , obtains accuracy rates close to 100%. The algorithm evaluates the similarity of the linguistic and extra-linguistic mark-up in both sides of a bitext. Given that annotations are neutral with respect to typological, grammatical and orthographical differences between languages , rich mark-up becomes an optimal foundation to support bitext correspondences . The main originality of this approach is that it makes maximal use of annotations, which is a very sensible and efficient method for the exploitation of parallel corpora when annotations exist.", "tag": "RESULT"}, {"qas_id": "P98-2154.2_P98-2154.4", "question_text": "paper [BREAK] method", "context": "Translating a Unification Grammar with Disjunctions into Logical Constraints . This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions , which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.", "tag": "TOPIC"}, {"qas_id": "P98-2154.7_P98-2154.10", "question_text": "representation [BREAK] information", "context": "Translating a Unification Grammar with Disjunctions into Logical Constraints . This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions , which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.", "tag": "PART_WHOLE"}, {"qas_id": "P98-2154.12_P98-2154.16", "question_text": "lists [BREAK] formalisms", "context": "Translating a Unification Grammar with Disjunctions into Logical Constraints . This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions , which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.", "tag": "USAGE"}, {"qas_id": "P98-2154.21_P98-2154.24", "question_text": "Parsing [BREAK] parsing", "context": "Translating a Unification Grammar with Disjunctions into Logical Constraints . This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions , which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.", "tag": "COMPARE"}, {"qas_id": "P98-2154.28_P98-2154.32", "question_text": "lists [BREAK] formalism", "context": "Translating a Unification Grammar with Disjunctions into Logical Constraints . This paper proposes a method for generating a logical-constraint-based internal representation from a unification grammar formalism with disjunctive information . Unification grammar formalisms based on path equations and lists of pairs of labels and values are better than those based on first-order terms in that the former is easier to describe and to understand. Parsing with term-based internal representations is more efficient than parsing with graph-based representations . Therefore, it is effective to translate unification grammar formalism based on path equations and lists of pairs of labels and values into a term-based internal representation . Previous translation methods cannot deal with disjunctive feature descriptions , which reduce redundancies in the grammar and make parsing efficient. Since the proposed method translates a formalism without expanding disjunctions , parsing with the resulting representation is efficient.", "tag": "USAGE"}, {"qas_id": "C08-1013.17_C08-1013.20", "question_text": "alignments [BREAK] sentences", "context": "ParaMetric: An Automatic Evaluation Metric for Paraphrasing . We present ParaMetric, an automatic evaluation metric for data-driven approaches to paraphrasing. ParaMetric provides an objective measure of quality using a collection of multiple translations whose paraphrases have been manually annotated. ParaMetric calculates precision and recall scores by comparing the paraphrases discovered by automatic paraphrasing techniques against gold standard alignments of words and phrases within equivalent sentences . We report scores for several established paraphrasing techniques .", "tag": "MODEL-FEATURE"}, {"qas_id": "C08-1025.4_C08-1025.6", "question_text": "lexical <entity id=\"C08-1025.5\">information [BREAK] data", "context": "Re-estimation of Lexical Parameters for Treebank PCFGs . We present procedures which pool lexical <entity id=\"C08-1025.5\">information </entity> estimated from unlabeled data via the Inside-Outside algorithm , with lexical information from a treebank PCFG. The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG. Even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing /scores on Wall Street Journal parsing , and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs .", "tag": "PART_WHOLE"}, {"qas_id": "C08-1025.9_C08-1025.10", "question_text": "procedures [BREAK] improvements", "context": "Re-estimation of Lexical Parameters for Treebank PCFGs . We present procedures which pool lexical <entity id=\"C08-1025.5\">information </entity> estimated from unlabeled data via the Inside-Outside algorithm , with lexical information from a treebank PCFG. The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG. Even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing /scores on Wall Street Journal parsing , and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs .", "tag": "RESULT"}, {"qas_id": "C08-1025.14_C08-1025.18", "question_text": "frames [BREAK] Penn Treebank-trained", "context": "Re-estimation of Lexical Parameters for Treebank PCFGs . We present procedures which pool lexical <entity id=\"C08-1025.5\">information </entity> estimated from unlabeled data via the Inside-Outside algorithm , with lexical information from a treebank PCFG. The procedures produce substantial improvements (up to 31.6% error reduction ) on the task of determining subcategorization frames of novel verbs , relative to a smoothed Penn Treebank-trained PCFG. Even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing /scores on Wall Street Journal parsing , and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs .", "tag": "PART_WHOLE"}, {"qas_id": "C08-1082.5_C08-1082.7", "question_text": "kernel <entity id=\"C08-1082.6\">methods [BREAK] classification", "context": "Semantic Classification with Distributional Kernels . Distributional measures of lexical similarity and kernel <entity id=\"C08-1082.6\">methods </entity> for classification are well-known tools in Natural Language Processing . We bring these two methods together by introducing distributional kernels", "tag": "USAGE"}, {"qas_id": "C08-1125.6_C08-1125.10", "question_text": "text [BREAK] Statistical <entity id=\"C08-1125.7\">machine translation systems", "context": "Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora . Statistical <entity id=\"C08-1125.7\">machine translation systems </entity> are usually trained on large amounts of bilingual text and monolingual text . In this paper , we propose a method to perform domain adaptation for statistical machine translation , where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance . We propose an algorithm to combine these different resources in a unified framework . Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-of-domain corpora .", "tag": "USAGE"}, {"qas_id": "C08-1125.12_C08-1125.14", "question_text": "paper [BREAK] method", "context": "Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora . Statistical <entity id=\"C08-1125.7\">machine translation systems </entity> are usually trained on large amounts of bilingual text and monolingual text . In this paper , we propose a method to perform domain adaptation for statistical machine translation , where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance . We propose an algorithm to combine these different resources in a unified framework . Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-of-domain corpora .", "tag": "TOPIC"}, {"qas_id": "C08-1125.20_C08-1125.22", "question_text": "corpora [BREAK] method", "context": "Domain Adaptation for Statistical Machine Translation with Domain Dictionary and Monolingual Corpora . Statistical <entity id=\"C08-1125.7\">machine translation systems </entity> are usually trained on large amounts of bilingual text and monolingual text . In this paper , we propose a method to perform domain adaptation for statistical machine translation , where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance . We propose an algorithm to combine these different resources in a unified framework . Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-of-domain corpora .", "tag": "USAGE"}, {"qas_id": "C08-1137.4_C08-1137.6", "question_text": "Reordering <entity id=\"C08-1137.5\">Model [BREAK] Statistical <entity id=\"C08-1137.7\">Machine Translation", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "USAGE"}, {"qas_id": "C08-1137.12_C08-1137.14", "question_text": "information [BREAK] source <entity id=\"C08-1137.15\">sentence", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C08-1137.16_C08-1137.18", "question_text": "paper [BREAK] reordering <entity id=\"C08-1137.19\">models", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "TOPIC"}, {"qas_id": "C08-1137.27_C08-1137.29", "question_text": "classifier [BREAK] sentences", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "USAGE"}, {"qas_id": "C08-1137.35_C08-1137.39", "question_text": "types [BREAK] reordering <entity id=\"C08-1137.36\">models", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "USAGE"}, {"qas_id": "C08-1137.41_C08-1137.43", "question_text": "reordering <entity id=\"C08-1137.42\">models [BREAK] improvement", "context": "Sentence Type Based Reordering <entity id=\"C08-1137.5\">Model </entity> for Statistical <entity id=\"C08-1137.7\">Machine Translation </entity> . Many reordering approaches have been proposed for the statistical machine translation (SMT) system . However, the information about the type of source <entity id=\"C08-1137.15\">sentence </entity> is ignored in the previous works. In this paper , we propose a group of novel reordering <entity id=\"C08-1137.19\">models </entity> based on the source sentence type for Chinese-to- English translation . In our approach , an SVM-based classifier is employed to classify the given Chinese sentences into three types : special interrogative sentences , other interrogative sentences , and non-question sentences . The different reordering <entity id=\"C08-1137.36\">models </entity> are developed oriented to the different sentence types . Our experiments show that the novel reordering <entity id=\"C08-1137.42\">models </entity> have obtained an improvement of more than 2.65% in BLEU for a phrase-based spoken language translation system .", "tag": "RESULT"}, {"qas_id": "C80-1002.5_C80-1002.6", "question_text": "processor [BREAK] language", "context": "Automatic Processing Of Written French Language . An automatic processor of written French language is described. This processor uses syntactic and semantic <entity id=\"C80-1002.10\">informations </entity> about words in order to construct a semantic net representing the meaning of the sentences . The structure of the network and the principles of the parser are explained. An application to the processing of the medical records is then discussed.", "tag": "USAGE"}, {"qas_id": "C80-1002.7_C80-1002.9", "question_text": "semantic <entity id=\"C80-1002.10\">informations [BREAK] processor", "context": "Automatic Processing Of Written French Language . An automatic processor of written French language is described. This processor uses syntactic and semantic <entity id=\"C80-1002.10\">informations </entity> about words in order to construct a semantic net representing the meaning of the sentences . The structure of the network and the principles of the parser are explained. An application to the processing of the medical records is then discussed.", "tag": "USAGE"}, {"qas_id": "C80-1002.21_C80-1002.22", "question_text": "processing [BREAK] records", "context": "Automatic Processing Of Written French Language . An automatic processor of written French language is described. This processor uses syntactic and semantic <entity id=\"C80-1002.10\">informations </entity> about words in order to construct a semantic net representing the meaning of the sentences . The structure of the network and the principles of the parser are explained. An application to the processing of the medical records is then discussed.", "tag": "USAGE"}, {"qas_id": "C80-1005.2_C80-1005.3", "question_text": "Tagging [BREAK] English", "context": "Computer- Aided Grammatical Tagging Of Spoken English .", "tag": "USAGE"}, {"qas_id": "C80-1008.19_C80-1008.20", "question_text": "paper [BREAK] proposal", "context": "A Rule- Based Approach To Ill- Formed Input . Though natural language understanding systems have improved markedly in recent years, they have only begun to consider a major problem of truly natural input : ill-formedness. Quite often natural language input is ill-formed in the sense of being misspelled, ungrammatical, or not entirely meaningful. A requirement for any successful natural language interface must be that the system either intelligently guesses at a user 's intent, requests direct clarification , or at the very least, accurately identifies the ill-formedness. This paper presents a proposal for the proper treatment of ill-formed input . Our conjecture is that ill-formedness should be treated as rule-based . Violation of the rules of normal processing should be used to signal ill-formedness. Meta-rules modifying the rules of normal processing should be used for error identification and recovery . These meta-rules correspond to types of errors . Evidence for this conjecture is presented as well as some open questions .", "tag": "TOPIC"}, {"qas_id": "C80-1008.27_C80-1008.30", "question_text": "rules [BREAK] identification", "context": "A Rule- Based Approach To Ill- Formed Input . Though natural language understanding systems have improved markedly in recent years, they have only begun to consider a major problem of truly natural input : ill-formedness. Quite often natural language input is ill-formed in the sense of being misspelled, ungrammatical, or not entirely meaningful. A requirement for any successful natural language interface must be that the system either intelligently guesses at a user 's intent, requests direct clarification , or at the very least, accurately identifies the ill-formedness. This paper presents a proposal for the proper treatment of ill-formed input . Our conjecture is that ill-formedness should be treated as rule-based . Violation of the rules of normal processing should be used to signal ill-formedness. Meta-rules modifying the rules of normal processing should be used for error identification and recovery . These meta-rules correspond to types of errors . Evidence for this conjecture is presented as well as some open questions .", "tag": "USAGE"}, {"qas_id": "C80-1022.1_C80-1022.3", "question_text": "Knowledge <entity id=\"C80-1022.2\">Representation [BREAK] Understanding", "context": "The Knowledge <entity id=\"C80-1022.2\">Representation </entity> For A Story Understanding And Simulation System . TOYONAKA, OSAKA 560, JAPAN !!!! MATSUSHITA ELECTRIC INDUSTRIAL CO.,LTD. KADOMA, OSAKA 571, JAPAN Abstruet", "tag": "USAGE"}, {"qas_id": "L08-1232.4_L08-1232.6", "question_text": "papers [BREAK] information", "context": "Language Resources and Chemical Informatics . Chemistry research papers are a primary source of information about chemistry, as in any scientific field . The presentation of the data is, predominantly, unstructured information , and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques . At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain . However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels . The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research . This relies on the cooperation of several journal publishers to provide papers in an appropriate form . The work is carried out as a collaboration involving the Computer Laboratory , Chemistry Department and eScience Centre at Cambridge University , and is funded under the UK eScience programme.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1232.19_L08-1232.21", "question_text": "information [BREAK] papers", "context": "Language Resources and Chemical Informatics . Chemistry research papers are a primary source of information about chemistry, as in any scientific field . The presentation of the data is, predominantly, unstructured information , and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques . At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain . However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels . The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research . This relies on the cooperation of several journal publishers to provide papers in an appropriate form . The work is carried out as a collaboration involving the Computer Laboratory , Chemistry Department and eScience Centre at Cambridge University , and is funded under the UK eScience programme.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1232.28_L08-1232.29", "question_text": "information [BREAK] papers", "context": "Language Resources and Chemical Informatics . Chemistry research papers are a primary source of information about chemistry, as in any scientific field . The presentation of the data is, predominantly, unstructured information , and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques . At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain . However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels . The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research . This relies on the cooperation of several journal publishers to provide papers in an appropriate form . The work is carried out as a collaboration involving the Computer Laboratory , Chemistry Department and eScience Centre at Cambridge University , and is funded under the UK eScience programme.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1232.40_L08-1232.41", "question_text": "research [BREAK] corpus", "context": "Language Resources and Chemical Informatics . Chemistry research papers are a primary source of information about chemistry, as in any scientific field . The presentation of the data is, predominantly, unstructured information , and so not immediately susceptible to processes developed within chemical informatics for carrying out chemistry research by information processing techniques . At one level , extracting the relevant information from research papers is a text mining task , requiring both extensive language resources and specialised knowledge of the subject domain . However, the papers also encode information about the way the research is conducted and the structure of the field itself. Applying language technology to research papers in chemistry can facilitate eScience on several different levels . The SciBorg project sets out to provide an extensive, analysed corpus of published chemistry research . This relies on the cooperation of several journal publishers to provide papers in an appropriate form . The work is carried out as a collaboration involving the Computer Laboratory , Chemistry Department and eScience Centre at Cambridge University , and is funded under the UK eScience programme.", "tag": "PART_WHOLE"}, {"qas_id": "L08-1236.3_L08-1236.5", "question_text": "data [BREAK] repositories", "context": "MeSH: from a Controlled Vocabulary to a Processable Resource . Large repositories of life science data in the form of domain-specific literature , textual databases and other large specialised textual collections ( corpora ) in electronic form increase on a daily basis to a level beyond the human mind can grasp and interpret. As the volume of data continues to increase , substantial support from new information technologies and computational techniques grounded in the form of the ever increasing applications of the mining paradigm", "tag": "PART_WHOLE"}, {"qas_id": "L08-1378.1_L08-1378.3", "question_text": "Evaluation [BREAK] Question <entity id=\"L08-1378.4\">Answering System", "context": "An Evaluation of Spoken and Textual Interaction in the RITEL Interactive Question <entity id=\"L08-1378.4\">Answering System </entity> . The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system . In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output . Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue .", "tag": "TOPIC"}, {"qas_id": "L08-1378.19_L08-1378.20", "question_text": "paper [BREAK] evaluation", "context": "An Evaluation of Spoken and Textual Interaction in the RITEL Interactive Question <entity id=\"L08-1378.4\">Answering System </entity> . The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system . In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output . Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue .", "tag": "TOPIC"}, {"qas_id": "L08-1378.21_L08-1378.22", "question_text": "system [BREAK] performance", "context": "An Evaluation of Spoken and Textual Interaction in the RITEL Interactive Question <entity id=\"L08-1378.4\">Answering System </entity> . The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system . In this paper , we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output . Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue .", "tag": "RESULT"}, {"qas_id": "D08-1093.18_D08-1093.20", "question_text": "paper [BREAK] technique", "context": "Automatic Prediction of Parser Accuracy . Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications . However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees . In this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains . As a result , we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain .", "tag": "TOPIC"}, {"qas_id": "D08-1093.28_D08-1093.30", "question_text": "parser [BREAK] performance", "context": "Automatic Prediction of Parser Accuracy . Statistical parsers have become increasingly accurate, to the point where they are useful in many natural language applications . However, estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees . In this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains . As a result , we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain .", "tag": "RESULT"}, {"qas_id": "I05-1062.2_I05-1062.4", "question_text": "Terminology [BREAK] Corpora", "context": "French- English Terminology Extraction from Comparable Corpora . Abstract .", "tag": "PART_WHOLE"}, {"qas_id": "I05-3010.15_I05-3010.16", "question_text": "variations [BREAK] pitch", "context": "Turn-taking in Mandarin Dialogue : Interactions of Tone and Intonation . Fluent dialogue requires that speakers successfully negotiate and signal turn-taking. While many cues to turn change have been proposed , especially in multi-modal frameworks , here we focus on the use ofprosodic cues to these functions . In particular, we consider the use of prosodic cues in a tone language , Mandarin Chinese , where variations in pitch height and slope additionally serve to determine word meaning . Within a corpus of spontaneous Chinese dialogues , we find that turn-unit final syllables are significantly lower in average pitch and intensity than turn-unit initial syllables in both smooth turn changes and segments ended by speaker overlap. Interruptions are characterized by significant prosodic differences from smooth turn initiations. Furthermore, we demonstrate that these contrasts correspond to an overall lowering across all tones in final position, which largely preserves the relative heights of the lexical tones. In classification tasks , we contrast the use of text and prosodic features . Finally, we demonstrate that, on balanced training and test sets , we can distinguish turn-unit final words from other words at 93% accuracy and interruptions from smooth turn unit initiations at 62% accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-3010.18_I05-3010.20", "question_text": "dialogues [BREAK] corpus", "context": "Turn-taking in Mandarin Dialogue : Interactions of Tone and Intonation . Fluent dialogue requires that speakers successfully negotiate and signal turn-taking. While many cues to turn change have been proposed , especially in multi-modal frameworks , here we focus on the use ofprosodic cues to these functions . In particular, we consider the use of prosodic cues in a tone language , Mandarin Chinese , where variations in pitch height and slope additionally serve to determine word meaning . Within a corpus of spontaneous Chinese dialogues , we find that turn-unit final syllables are significantly lower in average pitch and intensity than turn-unit initial syllables in both smooth turn changes and segments ended by speaker overlap. Interruptions are characterized by significant prosodic differences from smooth turn initiations. Furthermore, we demonstrate that these contrasts correspond to an overall lowering across all tones in final position, which largely preserves the relative heights of the lexical tones. In classification tasks , we contrast the use of text and prosodic features . Finally, we demonstrate that, on balanced training and test sets , we can distinguish turn-unit final words from other words at 93% accuracy and interruptions from smooth turn unit initiations at 62% accuracy .", "tag": "PART_WHOLE"}, {"qas_id": "E03-1036.2_E03-1036.3", "question_text": "paper [BREAK] Categorial Grammar", "context": "Multi-Modal Combinatory Categorial Grammar . The paper shows how Combinatory Categorial Grammar (CCG) can be adapted to take advantage of the extra resource-sensitivity provided by the Categorial Type Logic framework . The resulting reformulation, Multi-Modal CCG, supports lexically specified control over the applicability of combinatory rules , permitting a universal rale component and shedding the need for language-specific restrictions on rules . We discuss some of the linguistic motivation for these changes, define the Multi-Modal CCG system and demonstrate how it works on some basic examples . We furthermore outline some possible extensions and address computational aspects of Multi-Modal CCG.", "tag": "TOPIC"}, {"qas_id": "E03-1061.4_E03-1061.6", "question_text": "Knowledge [BREAK] Collection", "context": "Automatic Acquisition Of Script Knowledge From A Text Collection . In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .", "tag": "PART_WHOLE"}, {"qas_id": "E03-1061.7_E03-1061.8", "question_text": "paper [BREAK] method", "context": "Automatic Acquisition Of Script Knowledge From A Text Collection . In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .", "tag": "TOPIC"}, {"qas_id": "E03-1061.12_E03-1061.15", "question_text": "knowledge [BREAK] collection", "context": "Automatic Acquisition Of Script Knowledge From A Text Collection . In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .", "tag": "PART_WHOLE"}, {"qas_id": "E03-1061.22_E03-1061.29", "question_text": "sequences [BREAK] collection", "context": "Automatic Acquisition Of Script Knowledge From A Text Collection . In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .", "tag": "PART_WHOLE"}, {"qas_id": "E03-1061.35_E03-1061.36", "question_text": "frequency [BREAK] occurrence", "context": "Automatic Acquisition Of Script Knowledge From A Text Collection . In this paper , we describe a method for automatic acquisition of script knowledge from a Japanese text collection . Script knowledge represents a typical sequence of actions that occur in a particular situation . We extracted sequences ( pairs ) of actions occurring in time order from a Japanese text collection and then chose those that were typical of certain situations by ranking these sequences ( pairs ) in terms of the frequency of their occurrence . To extract sequences of actions occurring in time order , we constructed a text collection in which texts describing facts relating to a similar situation were clustered together and arranged in time order . We also describe a preliminary experiment with our acquisition system and discuss the results .", "tag": "MODEL-FEATURE"}, {"qas_id": "E06-1040.2_E06-1040.3", "question_text": "Evaluation [BREAK] Systems", "context": "Comparing Automatic And Human Evaluation Of NLG Systems . We consider the evaluation problem in Natural <entity id=\"E06-1040.7\">Language Generation </entity> (nlg) and present results for evaluating several nlg systems with similar functionality , including a knowledge-based generator and several statistical systems . We compare evaluation results for these systems by human domain experts , human non-experts, and several automatic evaluation metrics , including nist, bleu, and rouge . We find that nist scores correlate best (&gt; 0.8) with human judgments , but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic <entity id=\"E06-1040.32\">evaluation </entity> of nlg systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations , or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain .", "tag": "TOPIC"}, {"qas_id": "E06-1040.4_E06-1040.6", "question_text": "evaluation [BREAK] Natural <entity id=\"E06-1040.7\">Language Generation", "context": "Comparing Automatic And Human Evaluation Of NLG Systems . We consider the evaluation problem in Natural <entity id=\"E06-1040.7\">Language Generation </entity> (nlg) and present results for evaluating several nlg systems with similar functionality , including a knowledge-based generator and several statistical systems . We compare evaluation results for these systems by human domain experts , human non-experts, and several automatic evaluation metrics , including nist, bleu, and rouge . We find that nist scores correlate best (&gt; 0.8) with human judgments , but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic <entity id=\"E06-1040.32\">evaluation </entity> of nlg systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations , or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain .", "tag": "PART_WHOLE"}, {"qas_id": "E06-1040.26_E06-1040.27", "question_text": "biased [BREAK] metrics", "context": "Comparing Automatic And Human Evaluation Of NLG Systems . We consider the evaluation problem in Natural <entity id=\"E06-1040.7\">Language Generation </entity> (nlg) and present results for evaluating several nlg systems with similar functionality , including a knowledge-based generator and several statistical systems . We compare evaluation results for these systems by human domain experts , human non-experts, and several automatic evaluation metrics , including nist, bleu, and rouge . We find that nist scores correlate best (&gt; 0.8) with human judgments , but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic <entity id=\"E06-1040.32\">evaluation </entity> of nlg systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations , or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain .", "tag": "MODEL-FEATURE"}, {"qas_id": "E06-1040.31_E06-1040.33", "question_text": "automatic <entity id=\"E06-1040.32\">evaluation [BREAK] systems", "context": "Comparing Automatic And Human Evaluation Of NLG Systems . We consider the evaluation problem in Natural <entity id=\"E06-1040.7\">Language Generation </entity> (nlg) and present results for evaluating several nlg systems with similar functionality , including a knowledge-based generator and several statistical systems . We compare evaluation results for these systems by human domain experts , human non-experts, and several automatic evaluation metrics , including nist, bleu, and rouge . We find that nist scores correlate best (&gt; 0.8) with human judgments , but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone. We conclude that automatic <entity id=\"E06-1040.32\">evaluation </entity> of nlg systems has considerable potential, in particular where high-quality reference texts and only a small number of human evalua-tors are available. However, in general it is probably best for automatic evaluations to be supported by human-based evaluations , or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain .", "tag": "TOPIC"}, {"qas_id": "N03-2017.1_N03-2017.3", "question_text": "Cohesion [BREAK] Word <entity id=\"N03-2017.2\">Alignment", "context": "Word <entity id=\"N03-2017.2\">Alignment </entity> With Cohesion Constraint . We present a syntax-based constraint for word alignment , known as the cohesion constraint . It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence . We evaluate the utility of this constraint in two different algorithms . The results show that it can provide a significant improvement in alignment quality .", "tag": "USAGE"}, {"qas_id": "N03-2027.2_N03-2027.3", "question_text": "Categorization [BREAK] Words", "context": "Bayesian Nets For Syntactic Categorization Of Novel Words . This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS <entity id=\"N03-2027.18\">tags </entity> depend on a variety of morphological and contextual <entity id=\"N03-2027.21\">features </entity> . Representing these dependencies in a DBN results into an elegant and effective PoS tagger.", "tag": "USAGE"}, {"qas_id": "N03-2027.4_N03-2027.5", "question_text": "paper [BREAK] application", "context": "Bayesian Nets For Syntactic Categorization Of Novel Words . This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS <entity id=\"N03-2027.18\">tags </entity> depend on a variety of morphological and contextual <entity id=\"N03-2027.21\">features </entity> . Representing these dependencies in a DBN results into an elegant and effective PoS tagger.", "tag": "TOPIC"}, {"qas_id": "N03-2027.10_N03-2027.11", "question_text": "tags [BREAK] text", "context": "Bayesian Nets For Syntactic Categorization Of Novel Words . This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS <entity id=\"N03-2027.18\">tags </entity> depend on a variety of morphological and contextual <entity id=\"N03-2027.21\">features </entity> . Representing these dependencies in a DBN results into an elegant and effective PoS tagger.", "tag": "USAGE"}, {"qas_id": "N03-2027.17_N03-2027.20", "question_text": "contextual <entity id=\"N03-2027.21\">features [BREAK] PoS <entity id=\"N03-2027.18\">tags", "context": "Bayesian Nets For Syntactic Categorization Of Novel Words . This paper presents an application of a Dynamic Bayesian Network (DBN) to the task of assigning Part-of- Speech (PoS) tags to novel text . This task is particularly challenging for non-standard corpora , such as Internet lingo, where a large proportion of words are unknown. Previous work reveals that PoS <entity id=\"N03-2027.18\">tags </entity> depend on a variety of morphological and contextual <entity id=\"N03-2027.21\">features </entity> . Representing these dependencies in a DBN results into an elegant and effective PoS tagger.", "tag": "RESULT"}, {"qas_id": "N04-1018.12_N04-1018.13", "question_text": "paper [BREAK] system", "context": "Detecting Structural Metadata With Decision Trees And Transformation- Based Learning . The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech . Detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts . This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information <entity id=\"N04-1018.20\">sources </entity> . Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events , transformation-based learning is used to detect edit disfluencies and conversational fillers . Results are reported on human and automatic transcripts of conversational telephone speech .", "tag": "TOPIC"}, {"qas_id": "N04-1018.17_N04-1018.19", "question_text": "cues [BREAK] information <entity id=\"N04-1018.20\">sources", "context": "Detecting Structural Metadata With Decision Trees And Transformation- Based Learning . The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech . Detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts . This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information <entity id=\"N04-1018.20\">sources </entity> . Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events , transformation-based learning is used to detect edit disfluencies and conversational fillers . Results are reported on human and automatic transcripts of conversational telephone speech .", "tag": "PART_WHOLE"}, {"qas_id": "N04-1018.32_N04-1018.34", "question_text": "transcripts [BREAK] speech", "context": "Detecting Structural Metadata With Decision Trees And Transformation- Based Learning . The regular occurrence of disfluencies is a distinguishing characteristic of spontaneous speech . Detecting and removing such disfluencies can substantially improve the usefulness of spontaneous speech transcripts . This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information <entity id=\"N04-1018.20\">sources </entity> . Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events , transformation-based learning is used to detect edit disfluencies and conversational fillers . Results are reported on human and automatic transcripts of conversational telephone speech .", "tag": "PART_WHOLE"}, {"qas_id": "N04-4021.4_N04-4021.5", "question_text": "Modeling [BREAK] Speech <entity id=\"N04-4021.6\">Recognition", "context": "Feature- Based Pronunciation Modeling For Speech <entity id=\"N04-4021.6\">Recognition </entity> . We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values , rather than phone substitutions , insertions , and deletions . We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks . In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus . The experimental results , as well as the model 's qualitative behavior , suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech .", "tag": "USAGE"}, {"qas_id": "N04-4021.26_N04-4021.27", "question_text": "networks [BREAK] model", "context": "Feature- Based Pronunciation Modeling For Speech <entity id=\"N04-4021.6\">Recognition </entity> . We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values , rather than phone substitutions , insertions , and deletions . We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks . In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus . The experimental results , as well as the model 's qualitative behavior , suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech .", "tag": "USAGE"}, {"qas_id": "N04-4021.28_N04-4021.29", "question_text": "paper [BREAK] approach", "context": "Feature- Based Pronunciation Modeling For Speech <entity id=\"N04-4021.6\">Recognition </entity> . We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values , rather than phone substitutions , insertions , and deletions . We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks . In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus . The experimental results , as well as the model 's qualitative behavior , suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech .", "tag": "TOPIC"}, {"qas_id": "N04-4021.33_N04-4021.35", "question_text": "transcriptions [BREAK] corpus", "context": "Feature- Based Pronunciation Modeling For Speech <entity id=\"N04-4021.6\">Recognition </entity> . We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented. This differs from phone-based models in that pronunciation variation is viewed as the result of feature asynchrony and changes in feature values , rather than phone substitutions , insertions , and deletions . We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks . In this paper , we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus . The experimental results , as well as the model 's qualitative behavior , suggest that this is a promising way of accounting for the types of pronunciation variation often seen in spontaneous speech .", "tag": "PART_WHOLE"}, {"qas_id": "N04-4028.7_N04-4028.9", "question_text": "databases [BREAK] sources", "context": "Confidence Estimation For Information Extraction . Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect. For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information <entity id=\"N04-4028.21\">extraction system </entity> we evaluate is based on a linear-chain conditional random <entity id=\"N04-4028.26\">field </entity> (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .", "tag": "PART_WHOLE"}, {"qas_id": "N04-4028.20_N04-4028.25", "question_text": "conditional random <entity id=\"N04-4028.26\">field [BREAK] information <entity id=\"N04-4028.21\">extraction system", "context": "Confidence Estimation For Information Extraction . Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect. For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information <entity id=\"N04-4028.21\">extraction system </entity> we evaluate is based on a linear-chain conditional random <entity id=\"N04-4028.26\">field </entity> (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .", "tag": "USAGE"}, {"qas_id": "N04-4028.31_N04-4028.32", "question_text": "features [BREAK] input", "context": "Confidence Estimation For Information Extraction . Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect. For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information <entity id=\"N04-4028.21\">extraction system </entity> we evaluate is based on a linear-chain conditional random <entity id=\"N04-4028.26\">field </entity> (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .", "tag": "MODEL-FEATURE"}, {"qas_id": "N04-4028.35_N04-4028.41", "question_text": "techniques [BREAK] precision", "context": "Confidence Estimation For Information Extraction . Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect. For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information <entity id=\"N04-4028.21\">extraction system </entity> we evaluate is based on a linear-chain conditional random <entity id=\"N04-4028.26\">field </entity> (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records .", "tag": "RESULT"}, {"qas_id": "M91-1029.1_M91-1029.2", "question_text": "Description [BREAK] System", "context": "PRC Inc: Description Of The PAKTUS System Used For MUC-3 . The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "tag": "MODEL-FEATURE"}, {"qas_id": "M91-1029.11_M91-1029.16", "question_text": "lexicon [BREAK] system", "context": "PRC Inc: Description Of The PAKTUS System Used For MUC-3 . The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "tag": "PART_WHOLE"}, {"qas_id": "M91-1029.21_M91-1029.23", "question_text": "systems [BREAK] understanding", "context": "PRC Inc: Description Of The PAKTUS System Used For MUC-3 . The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "tag": "USAGE"}, {"qas_id": "M91-1029.32_M91-1029.35", "question_text": "Input [BREAK] stream", "context": "PRC Inc: Description Of The PAKTUS System Used For MUC-3 . The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "tag": "PART_WHOLE"}, {"qas_id": "M91-1029.56_M91-1029.59", "question_text": "system [BREAK] discourses", "context": "PRC Inc: Description Of The PAKTUS System Used For MUC-3 . The PRC Adaptive Knowledge-based Text Understanding System (PAKTUS) has been under development as an Independent Research and Development project at PRC since 1984. The objective is a generic system of tools , including a core English lexicon , grammar, and concept representations , for building natural language processing (NLP) systems for text understanding . Systems built with PAKTUS are intended to generate input to knowledge based systems or data base systems . Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire. PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts, etc., by acquiring sublanguage and domain-specific grammar, words , conceptual mappings , and discourse patterns . The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "tag": "USAGE"}, {"qas_id": "A92-1035.1_A92-1035.2", "question_text": "Modeling [BREAK] NLP <entity id=\"A92-1035.3\">Applications", "context": "Practical World Modeling For NLP <entity id=\"A92-1035.3\">Applications </entity> .", "tag": "USAGE"}, {"qas_id": "A92-1041.4_A92-1041.5", "question_text": "paper [BREAK] models", "context": "Lexicon Design Using A Paradigmatic Approach . The paper describes models for representation and methods to handle lexicographic structures supplied by the", "tag": "TOPIC"}, {"qas_id": "A94-1002.16_A94-1002.18", "question_text": "paper [BREAK] analysis", "context": "Practical Issues In Automatic Documentation Generation . PLANDoc, a system under joint development by Columbia and Bellcore, documents the activity of planning engineers as they study telephone routes . It takes as input a trace of the engineer's interaction with a network planning tool and produces 1-2 page summary . In this paper , we describe the user needs analysis we performed and how it influenced the development of PLANDoc. In particular, we show how it pinpointed the need for a sublanguage specification , allowing us to identify input messages and to characterize the different sentence paraphrases for realizing them. We focus on the systematic use of conjunction in combination with paraphrase that we developed for PLANDoc, which allows for the generation of summaries that are both concise-avoiding repetition of similar information , and fluent-avoiding repetition of similar phrasing.", "tag": "TOPIC"}, {"qas_id": "H90-1017.9_H90-1017.13", "question_text": "system [BREAK] tasks", "context": "The Dragon Continuous Speech Recognition System : A Real- Time Implementation . We present a 1000- word continuous speech recognition (CSR) system that operates in real time on a personal computer (PC). The system , designed for large vocabulary natural language tasks , makes use of phonetic Hidden Markov models (HMM) and incorporates acoustic, phonetic, and linguistic sources of knowledge to achieve high recognition performance . We describe the various components of this system . We also present our strategy for achieving real time recognition on the PC. Using a 486- based PC with a 29K- based add-on board, the recognizer has been timed at 1.1 times real time .", "tag": "USAGE"}, {"qas_id": "H90-1028.19_H90-1028.23", "question_text": "evaluation [BREAK] input", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "USAGE"}, {"qas_id": "H90-1028.24_H90-1028.25", "question_text": "sentences [BREAK] context", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "H90-1028.41_H90-1028.42", "question_text": "paper [BREAK] progress", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H90-1028.46_H90-1028.47", "question_text": "evaluation [BREAK] performance", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H90-1028.52_H90-1028.53", "question_text": "description [BREAK] system", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "H90-1028.68_H90-1028.69", "question_text": "section [BREAK] parser", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H90-1028.75_H90-1028.77", "question_text": "section [BREAK] discussion", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H90-1028.78_H90-1028.80", "question_text": "phenomena [BREAK] sentences", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "PART_WHOLE"}, {"qas_id": "H90-1028.82_H90-1028.84", "question_text": "section [BREAK] performance", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H90-1028.90_H90-1028.91", "question_text": "summary [BREAK] results", "context": "Preliminary ATIS Development At MIT . \"DARPA has recently initiated a plan for a common spoken language task , to be developed independently by all members of the DARPA community , with the hope that it will provide a mechanism leading to appropriate formal evaluation procedures at the level of spoken language . The task that was selected for this purpose is the Air Travel Information System (ATIS) task , based on selected tables from the Official Airline Guide (OAG). It was decided that the first evaluation would be limited in scope to deal with text input only, and to cover only sentences that could be understood unambiguously out of context . Data have been recorded over the past several months at Texas Instruments, using an interface that involves a \"\"wizard\"\" who fully interprets the meaning of the subject's sentences , and generates database responses using a menu driven data access system . We have been actively engaged in the last few months in developing the natural language and back end portions of the MIT version of the ATIS domain . This paper describes our progress to date on this effort , including an evaluation of the performance of the system on the recently released designated DARPA test set . The remainder of this paper is organized as follows. First we will give a general description of the system we are developing , emphasizing those aspects that differ from the current general conception of the common task . Next we will describe in greater detail certain aspects of the back end, including knowledge representation , control strategy , the user interface , and our preliminary treatment of discourse history. This is followed by a section describing changes made in the parser , in the areas of semantics , the interface with the back-end, and a preliminary new-word treatment . This section also includes a brief discussion of some interesting phenomena that occurred in the training sentences . An evaluation section follows, discussing our system 's performance on both training and test data, as well as a preliminary assessment of the perplexity of the system . We conclude with a summary of our results and our position on the nature of the common task . \"", "tag": "TOPIC"}, {"qas_id": "H91-1023.2_H91-1023.6", "question_text": "Semantic <entity id=\"H91-1023.3\">analysis [BREAK] ambiguities", "context": "Session 3: Machine Translation . Semantic <entity id=\"H91-1023.3\">analysis </entity> to resolve lexical and syntactic ambiguities during parsing , and thus reduce translation errors very significantly. Unification grammars allowing syntactic and semantic constraints to be checked in a unified manner while parsing , and permitting reversible grammars i.e., the same grammars to be used for generation as well as for analysis . Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation . Natural language generation , focusing on how to structure fluent target-language output , an activity not truly investigated in the pre-ALPAC days. Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation . MRDs =&gt; MTDs, use of electronic machine-readable dictionaries (MRDs) to partially automate the creation of machine-tractable dictionaries (MTDs) in processable internal form for parsers and generators , permitting principled scaling up in MT configurations .", "tag": "USAGE"}, {"qas_id": "H91-1023.20_H91-1023.22", "question_text": "compilation [BREAK] methodologies", "context": "Session 3: Machine Translation . Semantic <entity id=\"H91-1023.3\">analysis </entity> to resolve lexical and syntactic ambiguities during parsing , and thus reduce translation errors very significantly. Unification grammars allowing syntactic and semantic constraints to be checked in a unified manner while parsing , and permitting reversible grammars i.e., the same grammars to be used for generation as well as for analysis . Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation . Natural language generation , focusing on how to structure fluent target-language output , an activity not truly investigated in the pre-ALPAC days. Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation . MRDs =&gt; MTDs, use of electronic machine-readable dictionaries (MRDs) to partially automate the creation of machine-tractable dictionaries (MTDs) in processable internal form for parsers and generators , permitting principled scaling up in MT configurations .", "tag": "PART_WHOLE"}, {"qas_id": "H91-1023.42_H91-1023.44", "question_text": "information [BREAK] corpora", "context": "Session 3: Machine Translation . Semantic <entity id=\"H91-1023.3\">analysis </entity> to resolve lexical and syntactic ambiguities during parsing , and thus reduce translation errors very significantly. Unification grammars allowing syntactic and semantic constraints to be checked in a unified manner while parsing , and permitting reversible grammars i.e., the same grammars to be used for generation as well as for analysis . Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation . Natural language generation , focusing on how to structure fluent target-language output , an activity not truly investigated in the pre-ALPAC days. Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation . MRDs =&gt; MTDs, use of electronic machine-readable dictionaries (MRDs) to partially automate the creation of machine-tractable dictionaries (MTDs) in processable internal form for parsers and generators , permitting principled scaling up in MT configurations .", "tag": "PART_WHOLE"}, {"qas_id": "H91-1023.51_H91-1023.54", "question_text": "dictionaries [BREAK] dictionaries", "context": "Session 3: Machine Translation . Semantic <entity id=\"H91-1023.3\">analysis </entity> to resolve lexical and syntactic ambiguities during parsing , and thus reduce translation errors very significantly. Unification grammars allowing syntactic and semantic constraints to be checked in a unified manner while parsing , and permitting reversible grammars i.e., the same grammars to be used for generation as well as for analysis . Advanced parsing methodologies , including augmented-LR compilation where knowledge sources ( syntactic grammars, lexicons , and semantic ontologies ) can be defined and maintained separately but are jointly compiled to apply simultaneously at run time , both in parsing and in generation . Natural language generation , focusing on how to structure fluent target-language output , an activity not truly investigated in the pre-ALPAC days. Automated corpus analysis tools , statistical and other means of extracting useful information from large bi- or multi-lingual corpora , including collocations , transfers , and contextual cues for disambiguation . MRDs =&gt; MTDs, use of electronic machine-readable dictionaries (MRDs) to partially automate the creation of machine-tractable dictionaries (MTDs) in processable internal form for parsers and generators , permitting principled scaling up in MT configurations .", "tag": "USAGE"}, {"qas_id": "H91-1045.1_H91-1045.4", "question_text": "Probability [BREAK] Sentence", "context": "Calculating The Probability Of A Partial Parse Of A Sentence . A standard problem iu parsiug algorithms is the organization o[ branched searches lo deal with ambiguous sentences . We discuss shift-reduce parsiug of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses . The score we use is the likelihood that the collection of subtrees can be completed into a full parse tree by means of the steps the parser is constrained lo follow.", "tag": "MODEL-FEATURE"}, {"qas_id": "H91-1064.22_H91-1064.23", "question_text": "paper [BREAK] efforts", "context": "Discourse Structure In The TRAINS Project . In a natural dialog , a considerable proportion of the utterances actually relate to the maintenance of the dialog itself rather than to furthering the task or goals motivating the conversation . For example , many utterances serve to acknowledge, clarify, correct a previous utterance rather than pursue some goal in the domain . In addition , natural dialog is full of false starts, ungrammatical sentences and other complexities not found in in written language . This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .", "tag": "TOPIC"}, {"qas_id": "H91-1064.25_H91-1064.26", "question_text": "model [BREAK] discourse", "context": "Discourse Structure In The TRAINS Project . In a natural dialog , a considerable proportion of the utterances actually relate to the maintenance of the dialog itself rather than to furthering the task or goals motivating the conversation . For example , many utterances serve to acknowledge, clarify, correct a previous utterance rather than pursue some goal in the domain . In addition , natural dialog is full of false starts, ungrammatical sentences and other complexities not found in in written language . This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .", "tag": "MODEL-FEATURE"}, {"qas_id": "H91-1064.28_H91-1064.31", "question_text": "phenomena [BREAK] dialogs", "context": "Discourse Structure In The TRAINS Project . In a natural dialog , a considerable proportion of the utterances actually relate to the maintenance of the dialog itself rather than to furthering the task or goals motivating the conversation . For example , many utterances serve to acknowledge, clarify, correct a previous utterance rather than pursue some goal in the domain . In addition , natural dialog is full of false starts, ungrammatical sentences and other complexities not found in in written language . This paper describes our recent efforts to define and construct a model of discourse interaction that handle dialogs that are rich in these natural dialog-related phenomena .", "tag": "PART_WHOLE"}, {"qas_id": "H93-1063.2_H93-1063.4", "question_text": "paper [BREAK] introduction", "context": "Session 11: Prosody . This paper provides a brief introduction to prosody research in the context of human-computer communication and an overview of the contributions of the papers in the session.", "tag": "TOPIC"}, {"qas_id": "H93-1078.13_H93-1078.14", "question_text": "system [BREAK] processing", "context": "Gisting Continuous Speech . \"The objective of this woik is automatic , real-time \"\"gisting\"\" of voice traffic for updating of information in databases , for producing timely reports , and for prompt notification of events of interest. Specifically, the goal is to build a prototype , real-time system capable of processing radio communication between air traffic controllers and pilots , identifying dialogs and extracting their \"\"gist\"\" (e.g., identifying flights, determining whether they are landing or taking off), and producing a continuous output stream with that information . The approach is intended to be general and applicable to other domains . The system is built upon state-of-the-art techniques in speech recognition , speaker identification , natural language analysis , and topic statistical classification . These techniques have been extended where necessary to address specific aspects of the gisting problem . Because various sources of information must be combined, the system design features a high degree of interaction between the natural language and domain-knowledge components and the speech processing components . \"", "tag": "USAGE"}, {"qas_id": "H93-1078.24_H93-1078.25", "question_text": "techniques [BREAK] system", "context": "Gisting Continuous Speech . \"The objective of this woik is automatic , real-time \"\"gisting\"\" of voice traffic for updating of information in databases , for producing timely reports , and for prompt notification of events of interest. Specifically, the goal is to build a prototype , real-time system capable of processing radio communication between air traffic controllers and pilots , identifying dialogs and extracting their \"\"gist\"\" (e.g., identifying flights, determining whether they are landing or taking off), and producing a continuous output stream with that information . The approach is intended to be general and applicable to other domains . The system is built upon state-of-the-art techniques in speech recognition , speaker identification , natural language analysis , and topic statistical classification . These techniques have been extended where necessary to address specific aspects of the gisting problem . Because various sources of information must be combined, the system design features a high degree of interaction between the natural language and domain-knowledge components and the speech processing components . \"", "tag": "USAGE"}, {"qas_id": "H94-1029.2_H94-1029.4", "question_text": "Component [BREAK] Translation <entity id=\"H94-1029.5\">System", "context": "The Automatic Component Of The LINGSTAT Machine- Aided Translation <entity id=\"H94-1029.5\">System </entity> . We present the newest implementation of the LINGSTAT machine-aided translation system . The most significant change from earlier versions is a new set of modules that produce a draft translation of the document for the user to refer to or modify. This paper describes these modules , with special emphasis on an automatically trained lexicalized grammar used in the parsing module . Some preliminary results from the January 1994 ARPA evaluation are reported .", "tag": "PART_WHOLE"}, {"qas_id": "H94-1029.10_H94-1029.11", "question_text": "modules [BREAK] translation", "context": "The Automatic Component Of The LINGSTAT Machine- Aided Translation <entity id=\"H94-1029.5\">System </entity> . We present the newest implementation of the LINGSTAT machine-aided translation system . The most significant change from earlier versions is a new set of modules that produce a draft translation of the document for the user to refer to or modify. This paper describes these modules , with special emphasis on an automatically trained lexicalized grammar used in the parsing module . Some preliminary results from the January 1994 ARPA evaluation are reported .", "tag": "RESULT"}, {"qas_id": "H94-1029.14_H94-1029.15", "question_text": "paper [BREAK] modules", "context": "The Automatic Component Of The LINGSTAT Machine- Aided Translation <entity id=\"H94-1029.5\">System </entity> . We present the newest implementation of the LINGSTAT machine-aided translation system . The most significant change from earlier versions is a new set of modules that produce a draft translation of the document for the user to refer to or modify. This paper describes these modules , with special emphasis on an automatically trained lexicalized grammar used in the parsing module . Some preliminary results from the January 1994 ARPA evaluation are reported .", "tag": "TOPIC"}, {"qas_id": "A97-1025.1_A97-1025.2", "question_text": "Latent <entity id=\"A97-1025.3\">Semantic Analysis [BREAK] Correction", "context": "Contextual Spelling Correction Using Latent <entity id=\"A97-1025.3\">Semantic Analysis </entity> . Contextual spelling errors are denned as the use of an incorrect, though valid, word in a particular sentence or context . Traditional spelling checkers flag misspelled words , but they do not typically attempt to identify words that are used incorrectly in a sentence . We explore the use of", "tag": "USAGE"}, {"qas_id": "A97-1025.6_A97-1025.7", "question_text": "word [BREAK] sentence", "context": "Contextual Spelling Correction Using Latent <entity id=\"A97-1025.3\">Semantic Analysis </entity> . Contextual spelling errors are denned as the use of an incorrect, though valid, word in a particular sentence or context . Traditional spelling checkers flag misspelled words , but they do not typically attempt to identify words that are used incorrectly in a sentence . We explore the use of", "tag": "MODEL-FEATURE"}, {"qas_id": "A97-1025.11_A97-1025.12", "question_text": "words [BREAK] sentence", "context": "Contextual Spelling Correction Using Latent <entity id=\"A97-1025.3\">Semantic Analysis </entity> . Contextual spelling errors are denned as the use of an incorrect, though valid, word in a particular sentence or context . Traditional spelling checkers flag misspelled words , but they do not typically attempt to identify words that are used incorrectly in a sentence . We explore the use of", "tag": "PART_WHOLE"}, {"qas_id": "X96-1046.11_X96-1046.12", "question_text": "research [BREAK] text <entity id=\"X96-1046.13\">retrieval", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "TOPIC"}, {"qas_id": "X96-1046.17_X96-1046.18", "question_text": "approaches [BREAK] text <entity id=\"X96-1046.19\">retrieval", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "USAGE"}, {"qas_id": "X96-1046.28_X96-1046.30", "question_text": "collection [BREAK] detection", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "USAGE"}, {"qas_id": "X96-1046.39_X96-1046.40", "question_text": "documents [BREAK] collection", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "PART_WHOLE"}, {"qas_id": "X96-1046.46_X96-1046.47", "question_text": "topics [BREAK] collection", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "PART_WHOLE"}, {"qas_id": "X96-1046.52_X96-1046.54", "question_text": "size [BREAK] collection", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "X96-1046.56_X96-1046.57", "question_text": "size [BREAK] collection", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "X96-1046.69_X96-1046.74", "question_text": "techniques [BREAK] experiments", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "PART_WHOLE"}, {"qas_id": "X96-1046.86_X96-1046.87", "question_text": "vector <entity id=\"X96-1046.88\">space [BREAK] systems", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "USAGE"}, {"qas_id": "X96-1046.90_X96-1046.91", "question_text": "approaches [BREAK] approaches", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "COMPARE"}, {"qas_id": "X96-1046.95_X96-1046.97", "question_text": "investigations [BREAK] query <entity id=\"X96-1046.98\">expansion", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "TOPIC"}, {"qas_id": "X96-1046.112_X96-1046.113", "question_text": "techniques [BREAK] results", "context": "The Text REtrieval Conferences (TRECs) - Summary Results Of TREC-3 And TREC-4 . \"There have been four Text REtrieval Conferences (TRECs); TREC-1 in November 1992 , TREC-2 in August 1993 , TREC-3 in November 1994 and TREC-4 in November 1995 . The number of participating systems has grown from 25 in TREC-1 to 36 in TREC-4, including most of the major text retrieval software companies and most of the universities doing research in text <entity id=\"X96-1046.13\">retrieval </entity> (see table for some of the participants ). The diversity of the participating groups has ensured that TREC represents many different approaches to text <entity id=\"X96-1046.19\">retrieval </entity> , while the emphasis on individual experiments evaluated in a common setting has proven to be a major strength of TREC. The test design and test collection used for document detection in TIPSTER was also used in TREC. The participants ran the various tasks , sent results into NIST for evaluation , presented the results at the TREC conferences, and submitted papers for a proceedings . The test collection consists of over 1 million documents from diverse full-text sources , 250 topics , and the set of relevant documents or \"\"right answers\"\" to those topics . A Spanish collection has been built and used during TREC-3 and TREC-4, with a total of 50 topics . TREC-1 required significant system rebuilding by most groups due to the huge increase in the size of the document collection (from a traditional test collection of several megabytes in size to the 2 gigabyte TIPSTER collection ). The results from TREC-2 showed significant improvements over the TREC-1 results , and should be viewed as the appropriate baseline representing state-of-the-art retrieval techniques as scaled up to handling a 2 gigabyte collection . TREC-3 therefore provided the first opportunity for more complex experimentation . The major experiments in TREC-3 included the development of automatic query expansion techniques , the use of passages or sub-documents to increase the precision of retrieval results , and the use of the training information to select only the best terms for routing queries . Some groups explored hybrid approaches (such as the use of the Rocchio methodology in systems not using a vector <entity id=\"X96-1046.88\">space model </entity> ), and others tried approaches that were radically different from their original approaches . TREC-4 allowed a continuation of many of these complex experiments . The topics were made much shorter and this change triggered extensive investigations in automatic query <entity id=\"X96-1046.98\">expansion </entity> . There were also five new tasks , called tracks. These were added to help focus research on certain known problem areas , and included such issues as investigating searching as an interactive task by examining the process as well as the outcome , investigating techniques for merging results from the various TREC subcollections, examining the effects of corrupted data, and evaluating routing systems using a specific effectiveness measure. Additionally more groups participated in a track for Spanish retrieval . The TREC conferences have proven to be very successful, allowing broad participation in the overall DARPA TIPSTER effort , and causing widespread use of a very large test collection . All conferences have had very open, honest discussions of technical issues , and there have been large amounts of \"\" cross-fertilization \"\" of ideas. This will be a continuing effort , with a TREC-5 conference scheduled in November of 1996. \"", "tag": "USAGE"}, {"qas_id": "W93-0110.3_W93-0110.4", "question_text": "Information [BREAK] Texts", "context": "Acquiring Predicate- Argument Mapping Information From Multilingual Texts . This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts . The lexicon of our NLP <entity id=\"W93-0110.14\">system </entity> abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e.", "tag": "PART_WHOLE"}, {"qas_id": "W93-0110.5_W93-0110.7", "question_text": "paper [BREAK] acquisition", "context": "Acquiring Predicate- Argument Mapping Information From Multilingual Texts . This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts . The lexicon of our NLP <entity id=\"W93-0110.14\">system </entity> abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e.", "tag": "TOPIC"}, {"qas_id": "W93-0110.10_W93-0110.11", "question_text": "information [BREAK] texts", "context": "Acquiring Predicate- Argument Mapping Information From Multilingual Texts . This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts . The lexicon of our NLP <entity id=\"W93-0110.14\">system </entity> abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e.", "tag": "PART_WHOLE"}, {"qas_id": "W93-0110.12_W93-0110.13", "question_text": "lexicon [BREAK] NLP <entity id=\"W93-0110.14\">system", "context": "Acquiring Predicate- Argument Mapping Information From Multilingual Texts . This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts . The lexicon of our NLP <entity id=\"W93-0110.14\">system </entity> abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e.", "tag": "PART_WHOLE"}, {"qas_id": "W94-0203.10_W94-0203.12", "question_text": "methods [BREAK] defaults", "context": "Constraints, Exceptions And Representations . This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-default theories . In achieving this result , I offer a characterisation of Underspecification Theory and Optimality Theory in terms of their methods for ordering defaults . The result means that machine learning techniques for building declarative analyses may not provide an adequate basis for morphophonological analysis .", "tag": "USAGE"}, {"qas_id": "W97-1106.2_W97-1106.7", "question_text": "paper [BREAK] approach", "context": "A Czech Morphological Lexicon . In this paper , a treatment of Czech phonological rules in two-level morphology approach is described . First the possible phonological alternations in Czech are listed and then their treatment in a practical application of a Czech morphological lexicon .", "tag": "TOPIC"}, {"qas_id": "W98-0208.3_W98-0208.4", "question_text": "paper [BREAK] initiatives", "context": "Semantic Visualization . This paper summarizes several initiatives at MITRE that are investigating the visualization of a range of content . We present results of our work in relevancy visualization , news visualization , world events visualization and sensor/battlefield visualization to enhance user interaction in information access and exploitation tasks . We summarize several initiatives we are currently pursuing and enumerate unsolved problems .", "tag": "TOPIC"}, {"qas_id": "W98-0613.13_W98-0613.14", "question_text": "types [BREAK] inventory", "context": "Nominal Metonymy Processing . Abstract . We argue for the necessity of resolution of metonymies for nominals (and other cases ) in the context of semantics-based machine translation . By using an ontology as a search space , we are able to identify and resolve m tonymie expressions with significant accuracy , both for a pre-deterrnined inventory of metonymie types and for previously unseen cases . The entity replaced by the metonymy is made explicitly available in our meaning representation , to support translation , anaphora, and other mechanisms .", "tag": "PART_WHOLE"}, {"qas_id": "W98-0802.3_W98-0802.6", "question_text": "paper [BREAK] corpora", "context": "Towards Multimodal Spoken Language Corpora : TransTool And SyncTool . This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora . Two of the components , which have already been implemented as prototypes , are described in more detail : TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ). Finally, a brief comparison is made between these tools and other programs developed for similar purposes .", "tag": "TOPIC"}, {"qas_id": "W98-0802.7_W98-0802.8", "question_text": "components [BREAK] platform", "context": "Towards Multimodal Spoken Language Corpora : TransTool And SyncTool . This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora . Two of the components , which have already been implemented as prototypes , are described in more detail : TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ). Finally, a brief comparison is made between these tools and other programs developed for similar purposes .", "tag": "PART_WHOLE"}, {"qas_id": "W98-0802.19_W98-0802.21", "question_text": "tool [BREAK] transcriptions", "context": "Towards Multimodal Spoken Language Corpora : TransTool And SyncTool . This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora . Two of the components , which have already been implemented as prototypes , are described in more detail : TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ). Finally, a brief comparison is made between these tools and other programs developed for similar purposes .", "tag": "USAGE"}, {"qas_id": "W98-0802.31_W98-0802.32", "question_text": "tools [BREAK] programs", "context": "Towards Multimodal Spoken Language Corpora : TransTool And SyncTool . This paper argues for the usefulness of multimodal spoken language corpora and specifies components of a platform for the creation , maintenance and exploitation of such corpora . Two of the components , which have already been implemented as prototypes , are described in more detail : TransTool and SyncTool. TransTool is a transcription editor meant to facilitate and partially automate the task of a human transcriber, while SyncTool is a tool for aligning the resulting transcriptions with a digitized audio and video recording in order to allow synchronized presentation of different representations (e.g., text , audio, video , acoustic analysis ). Finally, a brief comparison is made between these tools and other programs developed for similar purposes .", "tag": "COMPARE"}, {"qas_id": "W98-1001.2_W98-1001.4", "question_text": "Tagging [BREAK] Text", "context": "Discovering Lexical Information By Tagging Arabic Newspaper Text . In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.", "tag": "USAGE"}, {"qas_id": "W98-1001.5_W98-1001.6", "question_text": "paper [BREAK] system", "context": "Discovering Lexical Information By Tagging Arabic Newspaper Text . In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.", "tag": "TOPIC"}, {"qas_id": "W98-1001.8_W98-1001.10", "question_text": "tagging [BREAK] text", "context": "Discovering Lexical Information By Tagging Arabic Newspaper Text . In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.", "tag": "USAGE"}, {"qas_id": "W98-1001.11_W98-1001.12", "question_text": "techniques [BREAK] system", "context": "Discovering Lexical Information By Tagging Arabic Newspaper Text . In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.", "tag": "PART_WHOLE"}, {"qas_id": "W98-1001.13_W98-1001.14", "question_text": "tagging [BREAK] words", "context": "Discovering Lexical Information By Tagging Arabic Newspaper Text . In this paper we describe a system for building an Arabic lexicon automatically by tagging Arabic newspaper text . In this system we are using several techniques for tagging the words in the text and figuring out their types and their features . The major techniques that we are using are: finding phrases , analyzing the affixes of the words , and analyzing their patterns . Proper nouns are particularly difficult to identify in the Arabic language ; we describe techniques for isolating them.", "tag": "USAGE"}, {"qas_id": "W99-0310.5_W99-0310.6", "question_text": "paper [BREAK] framework", "context": "A Recognition- Based Meta- Scheme For Dialogue Acts Annotation . The paper describes a new formal framework for comparison , design and standardization of annotation schemes for dialogue acts . The framework takes a <entity id=\"W99-0310.13\">recognition-based approach </entity> to dialogue tagging and defines four independent taxonomies of tags , one for each orthogonal dimension of linguistic and contextual analysis assumed to have a bearing on identification of illocutionary acts. The advantages and limitations of this proposal over other previous attempts are discussed and concretely exemplified.", "tag": "TOPIC"}, {"qas_id": "W99-0310.12_W99-0310.15", "question_text": "<entity id=\"W99-0310.13\">recognition-based [BREAK] dialogue", "context": "A Recognition- Based Meta- Scheme For Dialogue Acts Annotation . The paper describes a new formal framework for comparison , design and standardization of annotation schemes for dialogue acts . The framework takes a <entity id=\"W99-0310.13\">recognition-based approach </entity> to dialogue tagging and defines four independent taxonomies of tags , one for each orthogonal dimension of linguistic and contextual analysis assumed to have a bearing on identification of illocutionary acts. The advantages and limitations of this proposal over other previous attempts are discussed and concretely exemplified.", "tag": "USAGE"}, {"qas_id": "W00-0409.9_W00-0409.12", "question_text": "Document [BREAK] Automatic", "context": "Multi- Document Summarization By Visualizing Topical Content . Mani , Inderjeet; House, David ; Klein , Gary ; Hirschman , Lynette ; Firmin Hand , Therese ; Sundheim , Beth M. ,The TIPSTER SUMMAC Text Summarization Evaluation ,Conference Of The European Association For Computation al Linguistics ,1999 *** Nagao , Katashi; Hasida, Koiti, Automatic Text Summarization Based on the Global Document Annotation,COLING-ACL,1998***Power, Richard ; Scott , Donia R.,Multilingual Authoring using Feedback Texts ,COLING-ACL,1998***Mani, Inderjeet; Gates , Barbara ; Bloedorn , Eric ,Improving Summaries By Revising Them,Annual Meeting Of The Association For Computation al Linguistics ,1999", "tag": "USAGE"}, {"qas_id": "W00-1009.14_W00-1009.16", "question_text": "paradigm [BREAK] analysis", "context": "A Common Theory Of Information Fusion From Multiple Text Sources Step One: Cross- Document Structure . We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis . CST takes into account the rhetorical structure of clusters of related textual documents . We present a taxonomy of cross-document relationships . We argue that CST can be the basis for multi-document <entity id=\"W00-1009.25\">summarization </entity> guided by user preferences for summary length , information provenance, cross-source agreement , and chronological ordering of facts.", "tag": "USAGE"}, {"qas_id": "W00-1009.18_W00-1009.19", "question_text": "clusters [BREAK] documents", "context": "A Common Theory Of Information Fusion From Multiple Text Sources Step One: Cross- Document Structure . We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis . CST takes into account the rhetorical structure of clusters of related textual documents . We present a taxonomy of cross-document relationships . We argue that CST can be the basis for multi-document <entity id=\"W00-1009.25\">summarization </entity> guided by user preferences for summary length , information provenance, cross-source agreement , and chronological ordering of facts.", "tag": "MODEL-FEATURE"}, {"qas_id": "W00-1009.20_W00-1009.22", "question_text": "taxonomy [BREAK] relationships", "context": "A Common Theory Of Information Fusion From Multiple Text Sources Step One: Cross- Document Structure . We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis . CST takes into account the rhetorical structure of clusters of related textual documents . We present a taxonomy of cross-document relationships . We argue that CST can be the basis for multi-document <entity id=\"W00-1009.25\">summarization </entity> guided by user preferences for summary length , information provenance, cross-source agreement , and chronological ordering of facts.", "tag": "MODEL-FEATURE"}, {"qas_id": "W00-1009.24_W00-1009.27", "question_text": "preferences [BREAK] multi-document <entity id=\"W00-1009.25\">summarization", "context": "A Common Theory Of Information Fusion From Multiple Text Sources Step One: Cross- Document Structure . We introduce CST ( cross-document structure theory ), a paradigm for multi-document analysis . CST takes into account the rhetorical structure of clusters of related textual documents . We present a taxonomy of cross-document relationships . We argue that CST can be the basis for multi-document <entity id=\"W00-1009.25\">summarization </entity> guided by user preferences for summary length , information provenance, cross-source agreement , and chronological ordering of facts.", "tag": "USAGE"}, {"qas_id": "W00-1309.14_W00-1309.16", "question_text": "contextual <entity id=\"W00-1309.15\">information [BREAK] lexical <entity id=\"W00-1309.17\">entry", "context": "Error- Driven HMM-Based Chunk Tagger With Context- Dependent Lexicon . This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon . Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual <entity id=\"W00-1309.15\">information </entity> into a lexical <entity id=\"W00-1309.17\">entry </entity> . Moreover, an error-driven learning <entity id=\"W00-1309.20\">approach </entity> is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries . Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .", "tag": "PART_WHOLE"}, {"qas_id": "W00-1309.19_W00-1309.21", "question_text": "learning <entity id=\"W00-1309.20\">approach [BREAK] memory", "context": "Error- Driven HMM-Based Chunk Tagger With Context- Dependent Lexicon . This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon . Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual <entity id=\"W00-1309.15\">information </entity> into a lexical <entity id=\"W00-1309.17\">entry </entity> . Moreover, an error-driven learning <entity id=\"W00-1309.20\">approach </entity> is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries . Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .", "tag": "RESULT"}, {"qas_id": "W00-1309.27_W00-1309.30", "question_text": "technique [BREAK] rates", "context": "Error- Driven HMM-Based Chunk Tagger With Context- Dependent Lexicon . This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon . Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual <entity id=\"W00-1309.15\">information </entity> into a lexical <entity id=\"W00-1309.17\">entry </entity> . Moreover, an error-driven learning <entity id=\"W00-1309.20\">approach </entity> is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries . Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .", "tag": "RESULT"}, {"qas_id": "W00-1309.41_W00-1309.44", "question_text": "experiments [BREAK] rates", "context": "Error- Driven HMM-Based Chunk Tagger With Context- Dependent Lexicon . This paper proposes a new error-driven HMM-based text chunk tagger with context-dependent lexicon . Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual <entity id=\"W00-1309.15\">information </entity> into a lexical <entity id=\"W00-1309.17\">entry </entity> . Moreover, an error-driven learning <entity id=\"W00-1309.20\">approach </entity> is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more context-dependent lexical entries . Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types , 93.60% and 94.64% for noun phrases , and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types , 96.49% and 96.99% for noun phrases , and 97.13% and 97.36% for verb phrases .", "tag": "RESULT"}, {"qas_id": "W00-1326.5_W00-1326.8", "question_text": "paper [BREAK] hypothesis", "context": "One Sense Per Collocation And Genre / Topic Variations . This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora . We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities ). We also show that one sense per collocation does hold across corpora , but that collocations vary from one corpus to the other, following genre and topic variations . This explains the low results when performing word sense disambiguation across corpora . In fact, we demonstrate that when two independent corpora share a related genre / topic , the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models .", "tag": "TOPIC"}, {"qas_id": "W00-1326.20_W00-1326.22", "question_text": "genre [BREAK] collocations", "context": "One Sense Per Collocation And Genre / Topic Variations . This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora . We show that the hypothesis is weaker for fine-grained sense distinctions (70% vs. 99% reported earlier on 2-way ambiguities ). We also show that one sense per collocation does hold across corpora , but that collocations vary from one corpus to the other, following genre and topic variations . This explains the low results when performing word sense disambiguation across corpora . In fact, we demonstrate that when two independent corpora share a related genre / topic , the word sense disambiguation results would be better. Future work on word sense disambiguation will have to take into account genre and topic as important parameters on their models .", "tag": "RESULT"}, {"qas_id": "W01-0701.3_W01-0701.4", "question_text": "paper [BREAK] method", "context": "Multidimensional Transformation- Based Learning . This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields . The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework , as the signals for the extra tasks usually constitute inductive bias . The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese . The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train / test split.", "tag": "TOPIC"}, {"qas_id": "W01-0701.6_W01-0701.9", "question_text": "paradigm [BREAK] algorithm", "context": "Multidimensional Transformation- Based Learning . This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields . The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework , as the signals for the extra tasks usually constitute inductive bias . The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese . The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train / test split.", "tag": "USAGE"}, {"qas_id": "W01-0701.32_W01-0701.38", "question_text": "system [BREAK] corpus", "context": "Multidimensional Transformation- Based Learning . This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields . The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework , as the signals for the extra tasks usually constitute inductive bias . The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese . The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train / test split.", "tag": "USAGE"}, {"qas_id": "W01-0701.44_W01-0701.45", "question_text": "tagging [BREAK] Chinese", "context": "Multidimensional Transformation- Based Learning . This paper presents a novel method that allows a machine learning algorithm following the transformation-based learning paradigm ( Brill, 1995 ) to be applied to multiple classification tasks by training jointly and simultaneously on all fields . The motivation for constructing such a system stems from the observation that many tasks in natural language processing are naturally composed of multiple subtasks which need to be resolved simultaneously; also tasks usually learned in isolation can possibly benefit from being learned in a joint framework , as the signals for the extra tasks usually constitute inductive bias . The proposed algorithm is evaluated in two experiments : in one, the system is used to jointly predict the part-of-speech and text chunks /baseNP chunks of an English corpus ; and in the second it is used to learn the joint prediction of word segment boundaries and part-of-speech tagging for Chinese . The results show that the simultaneous learning of multiple tasks does achieve an improvement in each task upon training the same tasks sequentially. The part-of-speech tagging result of 96.63% is state-of-the-art for individual systems on the particular train / test split.", "tag": "USAGE"}, {"qas_id": "W02-0303.7_W02-0303.9", "question_text": "names [BREAK] corpus", "context": "Contrast And Variability In Gene Names . We studied contrast and variability in a corpus of gene names to identify potential heuristics for use in performing entity identification in the molecular biology domain . Based on our findings , we developed heuristics for mapping weakly matching gene names to their official gene names . We then tested these heuristics against a large body of Medline abstracts , and found that using these heuristics can increase recall , with varying levels of precision . Our findings also underscored the importance of good information retrieval and of the ability to disambiguate between genes , proteins , RNA, and a variety of other referents for performing entity identification with high precision .", "tag": "PART_WHOLE"}, {"qas_id": "W02-1706.5_W02-1706.6", "question_text": "processing [BREAK] text", "context": "XML-Based NLP Tools For Analysing And Annotating Medical Language . We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain . The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts . In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage ofa hand-crafted grammar that generates logical forms . And second, we investigate how they contribute to automatic lexical semantic acquisition processes .", "tag": "USAGE"}, {"qas_id": "W02-1706.9_W02-1706.11", "question_text": "paper [BREAK] tools", "context": "XML-Based NLP Tools For Analysing And Annotating Medical Language . We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain . The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts . In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage ofa hand-crafted grammar that generates logical forms . And second, we investigate how they contribute to automatic lexical semantic acquisition processes .", "tag": "TOPIC"}, {"qas_id": "W02-1706.14_W02-1706.15", "question_text": "abstracts [BREAK] corpus", "context": "XML-Based NLP Tools For Analysing And Annotating Medical Language . We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain . The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts . In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage ofa hand-crafted grammar that generates logical forms . And second, we investigate how they contribute to automatic lexical semantic acquisition processes .", "tag": "PART_WHOLE"}, {"qas_id": "W02-1706.19_W02-1706.20", "question_text": "tools [BREAK] pipelines", "context": "XML-Based NLP Tools For Analysing And Annotating Medical Language . We describe the use of a suite of highly flexible xml-based nlp tools in a project for processing and interpreting text in the medical domain . The main aim of the paper is to demonstrate the central role that xml mark-up and xml nlp tools have played in the analysis process and to describe the resultant annotated corpus of medline abstracts . In addition to the xml tools , we have succeeded in integrating a variety of non-xml 'off the shelf' nlp tools into our pipelines , so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage ofa hand-crafted grammar that generates logical forms . And second, we investigate how they contribute to automatic lexical semantic acquisition processes .", "tag": "PART_WHOLE"}, {"qas_id": "W03-0305.4_W03-0305.6", "question_text": "paper [BREAK] results", "context": "Reducing Parameter Space For Word Alignment . This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm . We use IBM Model 4 as a baseline. In order to reduce the parameter space , we pre-processed the training <entity id=\"W03-0305.17\">corpus </entity> using a word lemmatizer and a bilingual term extraction algorithm . Using these additional components , we obtained an improvement in the alignment error <entity id=\"W03-0305.26\">rate </entity> .", "tag": "TOPIC"}, {"qas_id": "W03-0305.16_W03-0305.21", "question_text": "algorithm [BREAK] training <entity id=\"W03-0305.17\">corpus", "context": "Reducing Parameter Space For Word Alignment . This paper presents the experimental results of our attemps to reduce the size of the parameter space in word alignment algorithm . We use IBM Model 4 as a baseline. In order to reduce the parameter space , we pre-processed the training <entity id=\"W03-0305.17\">corpus </entity> using a word lemmatizer and a bilingual term extraction algorithm . Using these additional components , we obtained an improvement in the alignment error <entity id=\"W03-0305.26\">rate </entity> .", "tag": "USAGE"}, {"qas_id": "W03-0902.3_W03-0902.5", "question_text": "World <entity id=\"W03-0902.4\">Knowledge [BREAK] Brown <entity id=\"W03-0902.6\">Corpus", "context": "Extracting And Evaluating General World <entity id=\"W03-0902.4\">Knowledge </entity> From The Brown <entity id=\"W03-0902.6\">Corpus </entity> . \"We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus . We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general \"\"possi-bilistic\"\" propositions from the resulting formulas . Two examples are \"\"A person may believe a proposition\"\", and \"\"Children may live with relatives \"\". Our methods currently yield over 117,000 such propositions (of variable quality ) for the Brown <entity id=\"W03-0902.32\">corpus </entity> (more than 2 per sentence ). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many ofthese propositions pass muster as \"\"reasonable general claims \"\" about the world in the opinion of humanjudges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge . The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing \"\" knowledge acquisition bottleneck\"\" in AI. \"", "tag": "PART_WHOLE"}, {"qas_id": "W03-0902.9_W03-0902.11", "question_text": "extracting [BREAK] texts", "context": "Extracting And Evaluating General World <entity id=\"W03-0902.4\">Knowledge </entity> From The Brown <entity id=\"W03-0902.6\">Corpus </entity> . \"We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus . We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general \"\"possi-bilistic\"\" propositions from the resulting formulas . Two examples are \"\"A person may believe a proposition\"\", and \"\"Children may live with relatives \"\". Our methods currently yield over 117,000 such propositions (of variable quality ) for the Brown <entity id=\"W03-0902.32\">corpus </entity> (more than 2 per sentence ). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many ofthese propositions pass muster as \"\"reasonable general claims \"\" about the world in the opinion of humanjudges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge . The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing \"\" knowledge acquisition bottleneck\"\" in AI. \"", "tag": "USAGE"}, {"qas_id": "W03-0902.18_W03-0902.19", "question_text": "rules [BREAK] patterns", "context": "Extracting And Evaluating General World <entity id=\"W03-0902.4\">Knowledge </entity> From The Brown <entity id=\"W03-0902.6\">Corpus </entity> . \"We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus . We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general \"\"possi-bilistic\"\" propositions from the resulting formulas . Two examples are \"\"A person may believe a proposition\"\", and \"\"Children may live with relatives \"\". Our methods currently yield over 117,000 such propositions (of variable quality ) for the Brown <entity id=\"W03-0902.32\">corpus </entity> (more than 2 per sentence ). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many ofthese propositions pass muster as \"\"reasonable general claims \"\" about the world in the opinion of humanjudges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge . The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing \"\" knowledge acquisition bottleneck\"\" in AI. \"", "tag": "USAGE"}, {"qas_id": "W03-0902.27_W03-0902.31", "question_text": "methods [BREAK] Brown <entity id=\"W03-0902.32\">corpus", "context": "Extracting And Evaluating General World <entity id=\"W03-0902.4\">Knowledge </entity> From The Brown <entity id=\"W03-0902.6\">Corpus </entity> . \"We have been developing techniques for extracting general world knowledge from miscellaneous texts by a process of approximate interpretation and abstraction , focusing initially on the Brown corpus . We apply interpretive rules to clausal patterns and patterns of modification , and concurrently abstract general \"\"possi-bilistic\"\" propositions from the resulting formulas . Two examples are \"\"A person may believe a proposition\"\", and \"\"Children may live with relatives \"\". Our methods currently yield over 117,000 such propositions (of variable quality ) for the Brown <entity id=\"W03-0902.32\">corpus </entity> (more than 2 per sentence ). We report here on our efforts to evaluate these results with a judging scheme aimed at determining how many ofthese propositions pass muster as \"\"reasonable general claims \"\" about the world in the opinion of humanjudges. We find that nearly 60% of the extracted propositions are favorably judged according to our scheme by any given judge . The percentage unanimously judged to be reasonable claims by multiple judges is lower, but still sufficiently high to suggest that our techniques may be of some use in tackling the long-standing \"\" knowledge acquisition bottleneck\"\" in AI. \"", "tag": "USAGE"}, {"qas_id": "W03-1026.26_W03-1026.28", "question_text": "paper [BREAK] combination", "context": "How To Get A Chinese Name ( Entity ): Segmentation And Combination Issues . When building a Chinese named entity recognition system , one must deal with certain language-specific issues such as whether the model should be based on characters or words . While there is no unique answer to this question , we discuss in detail advantages and disadvantages of each model , identify problems in segmentation and suggest possible solutions , presenting our observations , analysis , and experimental results . The second topic of this paper is classifier combination . We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs . The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .", "tag": "TOPIC"}, {"qas_id": "W03-1026.29_W03-1026.33", "question_text": "classifiers [BREAK] recognition", "context": "How To Get A Chinese Name ( Entity ): Segmentation And Combination Issues . When building a Chinese named entity recognition system , one must deal with certain language-specific issues such as whether the model should be based on characters or words . While there is no unique answer to this question , we discuss in detail advantages and disadvantages of each model , identify problems in segmentation and suggest possible solutions , presenting our observations , analysis , and experimental results . The second topic of this paper is classifier combination . We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs . The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .", "tag": "USAGE"}, {"qas_id": "W03-1026.38_W03-1026.40", "question_text": "combination [BREAK] improving", "context": "How To Get A Chinese Name ( Entity ): Segmentation And Combination Issues . When building a Chinese named entity recognition system , one must deal with certain language-specific issues such as whether the model should be based on characters or words . While there is no unique answer to this question , we discuss in detail advantages and disadvantages of each model , identify problems in segmentation and suggest possible solutions , presenting our observations , analysis , and experimental results . The second topic of this paper is classifier combination . We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs . The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .", "tag": "USAGE"}, {"qas_id": "W03-1026.43_W03-1026.47", "question_text": "experiments [BREAK] reduction", "context": "How To Get A Chinese Name ( Entity ): Segmentation And Combination Issues . When building a Chinese named entity recognition system , one must deal with certain language-specific issues such as whether the model should be based on characters or words . While there is no unique answer to this question , we discuss in detail advantages and disadvantages of each model , identify problems in segmentation and suggest possible solutions , presenting our observations , analysis , and experimental results . The second topic of this paper is classifier combination . We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs . The results demonstrate that classifier combination is an effective technique of improving system performance : experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error .", "tag": "RESULT"}, {"qas_id": "W03-1101.7_W03-1101.12", "question_text": "paper [BREAK] compression", "context": "Improving Summarization Performance By Sentence Compression - A Pilot Study . In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system . Our results show that pure syntactic-based compression does not improve system performance . Topic signature-based reranking of compressed sentences does not help much either. However reranking using an oracle showed a significant improvement remains possible. Keywords: Text Summarization , Sentence Extraction , Sentence Compression , Evaluation .", "tag": "TOPIC"}, {"qas_id": "W03-1101.18_W03-1101.21", "question_text": "compression [BREAK] performance", "context": "Improving Summarization Performance By Sentence Compression - A Pilot Study . In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system . Our results show that pure syntactic-based compression does not improve system performance . Topic signature-based reranking of compressed sentences does not help much either. However reranking using an oracle showed a significant improvement remains possible. Keywords: Text Summarization , Sentence Extraction , Sentence Compression , Evaluation .", "tag": "RESULT"}, {"qas_id": "W99-0611.3_W99-0611.5", "question_text": "algorithm [BREAK] coreference <entity id=\"W99-0611.6\">resolution", "context": "Noun Phrase Coreference As Clustering . This paper introduces a new, unsupervised algorithm for noun phrase coreference <entity id=\"W99-0611.6\">resolution </entity> . It differs from existing methods in that it views coreference resolution as a clustering task . In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation . More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem . The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .", "tag": "USAGE"}, {"qas_id": "W99-0611.11_W99-0611.13", "question_text": "evaluation [BREAK] corpus", "context": "Noun Phrase Coreference As Clustering . This paper introduces a new, unsupervised algorithm for noun phrase coreference <entity id=\"W99-0611.6\">resolution </entity> . It differs from existing methods in that it views coreference resolution as a clustering task . In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation . More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem . The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .", "tag": "USAGE"}, {"qas_id": "W99-0611.18_W99-0611.19", "question_text": "approach [BREAK] system", "context": "Noun Phrase Coreference As Clustering . This paper introduces a new, unsupervised algorithm for noun phrase coreference <entity id=\"W99-0611.6\">resolution </entity> . It differs from existing methods in that it views coreference resolution as a clustering task . In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation . More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem . The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .", "tag": "COMPARE"}, {"qas_id": "W99-0611.30_W99-0611.31", "question_text": "preferences [BREAK] partitioning", "context": "Noun Phrase Coreference As Clustering . This paper introduces a new, unsupervised algorithm for noun phrase coreference <entity id=\"W99-0611.6\">resolution </entity> . It differs from existing methods in that it views coreference resolution as a clustering task . In an evaluation on the MUC-6 coreference resolution corpus , the algorithm achieves an F-measure of 53.6%, placing it firmly between the worst (40%) and best (65%) systems in the MUC-6 evaluation . More importantly, the clustering approach outperforms the only MUC-6 system to treat coreference resolution as a learning problem . The clustering algorithm appears to provide a flexible mechanism for coordinating the application of context-independent and context-dependent constraints and preferences for accurate partitioning of noun phrases into coreference equivalence classes .", "tag": "USAGE"}, {"qas_id": "W08-0215.4_W08-0215.7", "question_text": "modeling [BREAK] computational <entity id=\"W08-0215.8\">linguistics", "context": "Psychocomputational Linguistics : A Gateway to the Computational Linguistics Curriculum . Computational modeling of human language processes is a small but growing subfield of computational <entity id=\"W08-0215.8\">linguistics </entity> . This paper describes a course that makes use of recent research in psychocomputational modeling as a framework to introduce a number of mainstream computational linguistics concepts to an audience of linguistics , cognitive science and computer science doctoral students. The emphasis on what I take to be the largely interdisciplinary nature of computational linguistics is particularly germane for the computer science students. Since 2002 the course has been taught three times under the auspices of the MA/PhD program in Linguistics at The City University of New York's Graduate Center . A brief description of some of the students' experiences after having taken the course is also provided .", "tag": "PART_WHOLE"}, {"qas_id": "J80-3003.2_J80-3003.3", "question_text": "Analysis [BREAK] Speech Act", "context": "A Plan- Based Analysis Of Indirect Speech Act . We propose an account of indirect forms of speech acts to request and inform based on the hypothesis that language users can recognize actions being performed by others, infer goals being sought, and cooperate in their achievement. This cooperative behaviour is independently motivated and may or may not be intended by speakers. If the hearer believes it is intended, he or she can recognize the speech act as indirect; otherwise it is interpreted directly. Heuristics are suggested to decide among the interpretations .", "tag": "TOPIC"}, {"qas_id": "J92-4002.17_J92-4002.19", "question_text": "Framework [BREAK] Interpretation", "context": "Ambiguous Noun Phrases In Logical Form . \"Schubert , Lenhart K.; Pelletier , Francis Jeffry ,From English To Logic : Context- Free Computation Of \"\"Conventional\"\" Logical Translation ,American Journal Of Computation al Linguistics ,1982 *** Hobbs , Jerry R. ,An Improper Treatment Of Quantification In Ordinary English ,Annual Meeting Of The Association For Computation al Linguistics ,1983 *** Pollack , Martha E. ; Pereira , Fernando ,An Integrated Framework For Semantic And Pragmatic Interpretation ,Annual Meeting Of The Association For Computation al Linguistics ,1988 *** Alshawi , Hiyan ; Van Eijck , Jan,Logical Forms In The Core Language Engine ,Annual Meeting Of The Association For Computation al Linguistics ,1989 \"", "tag": "USAGE"}, {"qas_id": "J00-3001.1_J00-3001.3", "question_text": "Extracting [BREAK] Words", "context": "Extracting The Lowest- Frequency Words : Pitfalls And Possibilities . Church, Kenneth Ward ; Hanks , Patrick , Word Association Norms , Mutual Information , And Lexicography, Computation al Linguistics ,1990 *** Dunning , Ted E. ,Accurate Methods For The Statistics Of Surprise And Coincidence, Computation al Linguistics ,1993 *** Smadja , Frank A. ,Retrieving Collocations From Text : Xtract, Computation al Linguistics ,1993", "tag": "USAGE"}, {"qas_id": "P98-2228.1_P98-2228.3", "question_text": "Knowledge Sources [BREAK] Word <entity id=\"P98-2228.2\">Sense Disambiguation", "context": "Word <entity id=\"P98-2228.2\">Sense Disambiguation </entity> using Optimised Combinations of Knowledge Sources . Word sense disambiguation algorithms , with few exceptions , have made use of only one lexical knowledge <entity id=\"P98-2228.8\">source </entity> . We describe a system which performs word sense disambiguation on all content <entity id=\"P98-2228.13\">words </entity> in free text by combining different knowledge sources : semantic preferences , dictionary definitions and subject/ domain codes along with part-of-speech tags , optimised by means of a learning algorithm . We also describe the creation of a new sense tagged corpus by combining existing resources . Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .", "tag": "USAGE"}, {"qas_id": "P98-2228.5_P98-2228.7", "question_text": "knowledge <entity id=\"P98-2228.8\">source [BREAK] algorithms", "context": "Word <entity id=\"P98-2228.2\">Sense Disambiguation </entity> using Optimised Combinations of Knowledge Sources . Word sense disambiguation algorithms , with few exceptions , have made use of only one lexical knowledge <entity id=\"P98-2228.8\">source </entity> . We describe a system which performs word sense disambiguation on all content <entity id=\"P98-2228.13\">words </entity> in free text by combining different knowledge sources : semantic preferences , dictionary definitions and subject/ domain codes along with part-of-speech tags , optimised by means of a learning algorithm . We also describe the creation of a new sense tagged corpus by combining existing resources . Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .", "tag": "USAGE"}, {"qas_id": "P98-2228.9_P98-2228.12", "question_text": "system [BREAK] content <entity id=\"P98-2228.13\">words", "context": "Word <entity id=\"P98-2228.2\">Sense Disambiguation </entity> using Optimised Combinations of Knowledge Sources . Word sense disambiguation algorithms , with few exceptions , have made use of only one lexical knowledge <entity id=\"P98-2228.8\">source </entity> . We describe a system which performs word sense disambiguation on all content <entity id=\"P98-2228.13\">words </entity> in free text by combining different knowledge sources : semantic preferences , dictionary definitions and subject/ domain codes along with part-of-speech tags , optimised by means of a learning algorithm . We also describe the creation of a new sense tagged corpus by combining existing resources . Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .", "tag": "USAGE"}, {"qas_id": "P98-2228.29_P98-2228.30", "question_text": "approach [BREAK] accuracy", "context": "Word <entity id=\"P98-2228.2\">Sense Disambiguation </entity> using Optimised Combinations of Knowledge Sources . Word sense disambiguation algorithms , with few exceptions , have made use of only one lexical knowledge <entity id=\"P98-2228.8\">source </entity> . We describe a system which performs word sense disambiguation on all content <entity id=\"P98-2228.13\">words </entity> in free text by combining different knowledge sources : semantic preferences , dictionary definitions and subject/ domain codes along with part-of-speech tags , optimised by means of a learning algorithm . We also describe the creation of a new sense tagged corpus by combining existing resources . Tested accuracy of our approach on this corpus exceeds 92% , demonstrating the viability of all-word disambiguation rather than restricting oneself to a small sample .", "tag": "RESULT"}, {"qas_id": "W08-1138.3_W08-1138.7", "question_text": "generation <entity id=\"W08-1138.4\">system [BREAK] task", "context": "GRAPH: The Costs of Redundancy in Referring Expressions . We describe a graph-based generation <entity id=\"W08-1138.4\">system </entity> that participated in the Tuna attribute selection and realisation task of the reg 2008 Challenge . Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 dice and .54 masi scores for attribute selection on the development set. For realisation , it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions .", "tag": "USAGE"}, {"qas_id": "W08-1138.17_W08-1138.19", "question_text": "selection [BREAK] differences", "context": "GRAPH: The Costs of Redundancy in Referring Expressions . We describe a graph-based generation <entity id=\"W08-1138.4\">system </entity> that participated in the Tuna attribute selection and realisation task of the reg 2008 Challenge . Using a stochastic cost function (with certain properties for free), and trying attributes from cheapest to more expensive, the system achieves overall .76 dice and .54 masi scores for attribute selection on the development set. For realisation , it turns out that in some cases higher attribute selection accuracy leads to larger differences between system-generated and human descriptions .", "tag": "RESULT"}, {"qas_id": "W03-2805.3_W03-2805.4", "question_text": "algorithm [BREAK] output", "context": "Colouring Summaries BLEU . In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output . The objective of this experiment is to explore whether a metric , originally developed for the evaluation of machine <entity id=\"W03-2805.15\">translation </entity> output , could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task , we put the feasibility of porting BLEU in different Natural Language Processing research areas under test . Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment .", "tag": "USAGE"}, {"qas_id": "W03-2805.7_W03-2805.8", "question_text": "evaluation [BREAK] output", "context": "Colouring Summaries BLEU . In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output . The objective of this experiment is to explore whether a metric , originally developed for the evaluation of machine <entity id=\"W03-2805.15\">translation </entity> output , could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task , we put the feasibility of porting BLEU in different Natural Language Processing research areas under test . Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment .", "tag": "TOPIC"}, {"qas_id": "W03-2805.13_W03-2805.14", "question_text": "evaluation [BREAK] machine <entity id=\"W03-2805.15\">translation", "context": "Colouring Summaries BLEU . In this paper we attempt to apply the IBM algorithm , BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output . The objective of this experiment is to explore whether a metric , originally developed for the evaluation of machine <entity id=\"W03-2805.15\">translation </entity> output , could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task , we put the feasibility of porting BLEU in different Natural Language Processing research areas under test . Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment .", "tag": "TOPIC"}, {"qas_id": "I08-1029.7_I08-1029.8", "question_text": "models [BREAK] classification", "context": "Automatic Prosodic Labeling with Conditional Random Fields and Rich Acoustic Features . Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models . In this paper we employ linear chain and factorial conditional random <entity id=\"I08-1029.12\">fields </entity> (CRFs) in conjunction with rich, contextually-based prosodic features , to exploit sequential dependencies and to facilitate integration with lexical features . Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF <entity id=\"I08-1029.28\">models </entity> can improve over linear chain based prediction of pitch accent.", "tag": "USAGE"}, {"qas_id": "I08-1029.9_I08-1029.11", "question_text": "paper [BREAK] conditional random <entity id=\"I08-1029.12\">fields", "context": "Automatic Prosodic Labeling with Conditional Random Fields and Rich Acoustic Features . Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models . In this paper we employ linear chain and factorial conditional random <entity id=\"I08-1029.12\">fields </entity> (CRFs) in conjunction with rich, contextually-based prosodic features , to exploit sequential dependencies and to facilitate integration with lexical features . Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF <entity id=\"I08-1029.28\">models </entity> can improve over linear chain based prediction of pitch accent.", "tag": "TOPIC"}, {"qas_id": "I08-1029.18_I08-1029.23", "question_text": "Integration [BREAK] prediction", "context": "Automatic Prosodic Labeling with Conditional Random Fields and Rich Acoustic Features . Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models . In this paper we employ linear chain and factorial conditional random <entity id=\"I08-1029.12\">fields </entity> (CRFs) in conjunction with rich, contextually-based prosodic features , to exploit sequential dependencies and to facilitate integration with lexical features . Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF <entity id=\"I08-1029.28\">models </entity> can improve over linear chain based prediction of pitch accent.", "tag": "RESULT"}, {"qas_id": "I08-1029.27_I08-1029.32", "question_text": "CRF <entity id=\"I08-1029.28\">models [BREAK] prediction", "context": "Automatic Prosodic Labeling with Conditional Random Fields and Rich Acoustic Features . Many acoustic approaches to prosodic labeling in English have employed only local classifiers , although text-based classification has employed some sequential models . In this paper we employ linear chain and factorial conditional random <entity id=\"I08-1029.12\">fields </entity> (CRFs) in conjunction with rich, contextually-based prosodic features , to exploit sequential dependencies and to facilitate integration with lexical features . Integration of lexical and prosodic features improves pitch accent prediction over either feature set alone, and for lower accuracy feature sets , factorial CRF <entity id=\"I08-1029.28\">models </entity> can improve over linear chain based prediction of pitch accent.", "tag": "RESULT"}, {"qas_id": "W04-0829.4_W04-0829.5", "question_text": "paper [BREAK] system", "context": "WSD Based On Mutual Information And Syntactic Patterns . This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks , that relies on two different unsupervised approaches . The first one selects the senses according to mutual information proximity between a context word a variant of the sense . The second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred. This patterns are matched against the disambiguation contexts . We show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. The high precision obtained recommends deeper research of the techniques . Results for the lexical sample task are also provided .", "tag": "TOPIC"}, {"qas_id": "W04-0829.19_W04-0829.22", "question_text": "patterns [BREAK] contexts", "context": "WSD Based On Mutual Information And Syntactic Patterns . This paper describes a hybrid system for WSD, presented to the English all-words and lexical-sample tasks , that relies on two different unsupervised approaches . The first one selects the senses according to mutual information proximity between a context word a variant of the sense . The second heuristic analyzes the examples of use in the glosses of the senses so that simple syntactic patterns are inferred. This patterns are matched against the disambiguation contexts . We show that the first heuristic obtains a precision and recall of .58 and .35 respectively in the all words task while the second obtains .80 and .25. The high precision obtained recommends deeper research of the techniques . Results for the lexical sample task are also provided .", "tag": "COMPARE"}, {"qas_id": "W04-0863.3_W04-0863.5", "question_text": "paper [BREAK] models", "context": "Joining Forces To Resolve Lexical Ambiguity : East Meets West In Barcelona . \"This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model , this paper focuses solely on the joint contributions to the \"\"Swat-HK\"\" effort . \"", "tag": "TOPIC"}, {"qas_id": "W04-0863.13_W04-0863.15", "question_text": "paper [BREAK] contributions", "context": "Joining Forces To Resolve Lexical Ambiguity : East Meets West In Barcelona . \"This paper describes the component models and combination model built as a joint effort between Swarthmore College, Hong Kong PolyU, and HKUST. Though other models described elsewhere contributed to the final combination model , this paper focuses solely on the joint contributions to the \"\"Swat-HK\"\" effort . \"", "tag": "TOPIC"}, {"qas_id": "W04-0914.6_W04-0914.10", "question_text": "paper [BREAK] semantics", "context": "Semantic Forensics : An Application Of Ontological Semantics To Information Assurance . \"The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in \"\"colonizing\"\" IAS. The main part of the paper deals with the following issues : human deception detection abilities and NLP modeling of it; manipulation of fact repositories for this purpose beyond the current state of the art; acquisition of scripts for complex ontological concepts ; degrees of lying complexity and feasibility of their automatic detection . This is not a report on a system implementation but rather an application-establishing proof-of-concept effort based on the algorithmic and machine-tractable recombination and extension of the previously implemented ONSE modules . The strength of the approach is that it emphasizes the use of the existing NLP applications , with very few domain- and goal-specific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 1\"", "tag": "TOPIC"}, {"qas_id": "W04-0914.14_W04-0914.17", "question_text": "resources [BREAK] text", "context": "Semantic Forensics : An Application Of Ontological Semantics To Information Assurance . \"The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in \"\"colonizing\"\" IAS. The main part of the paper deals with the following issues : human deception detection abilities and NLP modeling of it; manipulation of fact repositories for this purpose beyond the current state of the art; acquisition of scripts for complex ontological concepts ; degrees of lying complexity and feasibility of their automatic detection . This is not a report on a system implementation but rather an application-establishing proof-of-concept effort based on the algorithmic and machine-tractable recombination and extension of the previously implemented ONSE modules . The strength of the approach is that it emphasizes the use of the existing NLP applications , with very few domain- and goal-specific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 1\"", "tag": "USAGE"}, {"qas_id": "W04-0914.20_W04-0914.22", "question_text": "paper [BREAK] introduction", "context": "Semantic Forensics : An Application Of Ontological Semantics To Information Assurance . \"The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in \"\"colonizing\"\" IAS. The main part of the paper deals with the following issues : human deception detection abilities and NLP modeling of it; manipulation of fact repositories for this purpose beyond the current state of the art; acquisition of scripts for complex ontological concepts ; degrees of lying complexity and feasibility of their automatic detection . This is not a report on a system implementation but rather an application-establishing proof-of-concept effort based on the algorithmic and machine-tractable recombination and extension of the previously implemented ONSE modules . The strength of the approach is that it emphasizes the use of the existing NLP applications , with very few domain- and goal-specific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 1\"", "tag": "TOPIC"}, {"qas_id": "W04-0914.65_W04-0914.66", "question_text": "paper [BREAK] extensions", "context": "Semantic Forensics : An Application Of Ontological Semantics To Information Assurance . \"The paper deals with the latest application of natural language processing (NLP), specifically of ontological semantics (ONSE) to natural language information assurance and security (NL IAS). It demonstrates how the existing ideas, methods , and resources of ontological semantics can be applied to detect deception in NL text (and, eventually, in data and other media as well). After stating the problem , the paper proceeds to a brief introduction to ONSE, followed by an equally brief survey of our 5-year-old effort in \"\"colonizing\"\" IAS. The main part of the paper deals with the following issues : human deception detection abilities and NLP modeling of it; manipulation of fact repositories for this purpose beyond the current state of the art; acquisition of scripts for complex ontological concepts ; degrees of lying complexity and feasibility of their automatic detection . This is not a report on a system implementation but rather an application-establishing proof-of-concept effort based on the algorithmic and machine-tractable recombination and extension of the previously implemented ONSE modules . The strength of the approach is that it emphasizes the use of the existing NLP applications , with very few domain- and goal-specific adjustments, in a most promising and growing new area of IAS. So, while clearly dealing with a new application , the paper addresses theoretical and methodological extensions of ONSE, as defined currently, that will be useful for other applications as well. 1\"", "tag": "TOPIC"}, {"qas_id": "W04-1217.1_W04-1217.3", "question_text": "Context [BREAK] Recognition", "context": "Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web . We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic <entity id=\"W04-1217.12\">features </entity> within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .", "tag": "USAGE"}, {"qas_id": "W04-1217.6_W04-1217.7", "question_text": "system [BREAK] recognition", "context": "Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web . We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic <entity id=\"W04-1217.12\">features </entity> within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .", "tag": "USAGE"}, {"qas_id": "W04-1217.8_W04-1217.9", "question_text": "names [BREAK] texts", "context": "Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web . We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic <entity id=\"W04-1217.12\">features </entity> within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .", "tag": "PART_WHOLE"}, {"qas_id": "W04-1217.10_W04-1217.11", "question_text": "syntactic <entity id=\"W04-1217.12\">features [BREAK] system", "context": "Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web . We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic <entity id=\"W04-1217.12\">features </entity> within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .", "tag": "USAGE"}, {"qas_id": "W04-1217.19_W04-1217.20", "question_text": "entities [BREAK] corpus", "context": "Exploiting Context For Biomedical Entity Recognition : From Syntax To The Web . We describe a machine learning system for the recognition of names in biomedical texts . The system makes extensive use of local and syntactic <entity id=\"W04-1217.12\">features </entity> within the text , as well as external resources including the web and gazetteers . It achieves an F-score of 70% on the Coling 2004 NLPBA/BioNLP shared task of identifying five biomedical named entities in the GENIA corpus .", "tag": "PART_WHOLE"}, {"qas_id": "W04-1221.3_W04-1221.4", "question_text": "Feature [BREAK] Recognition", "context": "Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets . As the wealth of biomedical knowledge in the form of literature increases , there is a rising need for effective natural language processing tools to assist in organizing, curating, and retrieving this information . To that end, named entity recognition (the task of identifying words and phrases in free text that belong to certain classes of interest) is an important first step for many of these larger information management goals . In recent years, much attention has been focused on the problem of recognizing gene and protein mentions in biomedical abstracts . This paper presents a framework for simultaneously recognizing occurrences of PROTEIN , DNA, RNA, CELL-LINE, CELL-TYPE F1", "tag": "USAGE"}, {"qas_id": "W04-1221.30_W04-1221.31", "question_text": "paper [BREAK] framework", "context": "Biomedical Named Entity Recognition Using Conditional Random Fields And Rich Feature Sets . As the wealth of biomedical knowledge in the form of literature increases , there is a rising need for effective natural language processing tools to assist in organizing, curating, and retrieving this information . To that end, named entity recognition (the task of identifying words and phrases in free text that belong to certain classes of interest) is an important first step for many of these larger information management goals . In recent years, much attention has been focused on the problem of recognizing gene and protein mentions in biomedical abstracts . This paper presents a framework for simultaneously recognizing occurrences of PROTEIN , DNA, RNA, CELL-LINE, CELL-TYPE F1", "tag": "TOPIC"}, {"qas_id": "W04-2903.3_W04-2903.5", "question_text": "paper [BREAK] efforts", "context": "Audio Hot Spotting And Retrieval Using Multiple Features . This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query <entity id=\"W04-2903.44\">expansion </entity> to improve audio retrieval performance and to access cross-lingual data .", "tag": "TOPIC"}, {"qas_id": "W04-2903.6_W04-2903.7", "question_text": "features [BREAK] stream", "context": "Audio Hot Spotting And Retrieval Using Multiple Features . This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query <entity id=\"W04-2903.44\">expansion </entity> to improve audio retrieval performance and to access cross-lingual data .", "tag": "MODEL-FEATURE"}, {"qas_id": "W04-2903.11_W04-2903.12", "question_text": "algorithms [BREAK] features", "context": "Audio Hot Spotting And Retrieval Using Multiple Features . This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query <entity id=\"W04-2903.44\">expansion </entity> to improve audio retrieval performance and to access cross-lingual data .", "tag": "RESULT"}, {"qas_id": "W04-2903.26_W04-2903.27", "question_text": "data [BREAK] sources", "context": "Audio Hot Spotting And Retrieval Using Multiple Features . This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query <entity id=\"W04-2903.44\">expansion </entity> to improve audio retrieval performance and to access cross-lingual data .", "tag": "PART_WHOLE"}, {"qas_id": "W04-2903.43_W04-2903.47", "question_text": "query <entity id=\"W04-2903.44\">expansion [BREAK] performance", "context": "Audio Hot Spotting And Retrieval Using Multiple Features . This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news , teleconferences, and meetings . These features are derived from algorithms including automatic speech recognition , automatic speech indexing , speaker identification , prosodic and audio feature extraction . We describe our research prototype - the Audio Hot Spotting System -that allows users to query and retrieve data from multimedia sources utilizing these multiple features . The system aims to accurately find segments of user interest, i.e., audio hot spots within seconds of the actual event . In addition to spoken keywords , the system also retrieves audio hot spots by speaker identity, word spoken by a specific speaker, a change of speech rate , and other non-lexical features , including applause and laughter. Finally, we discuss our approach to semantic , morphological, phonetic query <entity id=\"W04-2903.44\">expansion </entity> to improve audio retrieval performance and to access cross-lingual data .", "tag": "RESULT"}, {"qas_id": "W04-3006.7_W04-3006.11", "question_text": "research [BREAK] errors", "context": "Error Detection And Recovery In Spoken Dialogue Systems . \"This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems . The paper consists of two major components . The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation . An important observation is that, upon a request for keypad entry , users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone . The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a \"\" speak-and-spell \"\" entry . A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy . We have found that the speak-and-spell strategy is quite effective in simulation mode , but it remains to be tested in real user dialogues . \"", "tag": "TOPIC"}, {"qas_id": "W04-3006.13_W04-3006.14", "question_text": "paper [BREAK] components", "context": "Error Detection And Recovery In Spoken Dialogue Systems . \"This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems . The paper consists of two major components . The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation . An important observation is that, upon a request for keypad entry , users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone . The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a \"\" speak-and-spell \"\" entry . A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy . We have found that the speak-and-spell strategy is quite effective in simulation mode , but it remains to be tested in real user dialogues . \"", "tag": "TOPIC"}, {"qas_id": "W04-3006.22_W04-3006.23", "question_text": "investigation [BREAK] patterns", "context": "Error Detection And Recovery In Spoken Dialogue Systems . \"This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems . The paper consists of two major components . The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation . An important observation is that, upon a request for keypad entry , users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone . The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a \"\" speak-and-spell \"\" entry . A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy . We have found that the speak-and-spell strategy is quite effective in simulation mode , but it remains to be tested in real user dialogues . \"", "tag": "TOPIC"}, {"qas_id": "W04-3006.44_W04-3006.46", "question_text": "evaluation [BREAK] strategy", "context": "Error Detection And Recovery In Spoken Dialogue Systems . \"This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems . The paper consists of two major components . The first half concerns the design of the error detection mechanism for resolving city names in our mercury flight reservation system , and an investigation of the behavioral patterns of users in subsequent subdialogues involving keypad entry for disambiguation . An important observation is that, upon a request for keypad entry , users are frequently unresponsive to the extent of waiting for a time-out or hanging up the phone . The second half concerns a pilot experiment investigating the feasibility of replacing the solicitation of a keypad entry with that of a \"\" speak-and-spell \"\" entry . A novelty of our work is the introduction of a speech synthesizer to simulate the user , which facilitates development and evaluation of our proposed strategy . We have found that the speak-and-spell strategy is quite effective in simulation mode , but it remains to be tested in real user dialogues . \"", "tag": "TOPIC"}, {"qas_id": "W05-0707.6_W05-0707.7", "question_text": "words [BREAK] corpus", "context": "Part Of Speech Tagging For Amharic Using Conditional Random Fields . We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words . Given the size of the data and the large number of unknown <entity id=\"W05-0707.12\">words </entity> in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic.", "tag": "PART_WHOLE"}, {"qas_id": "W05-0707.8_W05-0707.9", "question_text": "size [BREAK] data", "context": "Part Of Speech Tagging For Amharic Using Conditional Random Fields . We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words . Given the size of the data and the large number of unknown <entity id=\"W05-0707.12\">words </entity> in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic.", "tag": "MODEL-FEATURE"}, {"qas_id": "W05-0707.11_W05-0707.14", "question_text": "unknown <entity id=\"W05-0707.12\">words [BREAK] corpus", "context": "Part Of Speech Tagging For Amharic Using Conditional Random Fields . We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words . Given the size of the data and the large number of unknown <entity id=\"W05-0707.12\">words </entity> in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic.", "tag": "PART_WHOLE"}, {"qas_id": "W05-0707.15_W05-0707.16", "question_text": "word segmentation [BREAK] accuracy", "context": "Part Of Speech Tagging For Amharic Using Conditional Random Fields . We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words . Given the size of the data and the large number of unknown <entity id=\"W05-0707.12\">words </entity> in the test corpus (80%), an accuracy of 84% for Amharic word segmentation and 74% for POS tagging is encouraging, indicating the applicability of CRFs for a morphologically complex language like Amharic.", "tag": "RESULT"}, {"qas_id": "W05-0801.10_W05-0801.11", "question_text": "generative <entity id=\"W05-0801.12\">models [BREAK] methods", "context": "Association- Based Bilingual Word Alignment . Bilingual word alignment forms the foundation of current work on statistical machine translation . Standard word-alignment methods involve the use of probabilistic generative <entity id=\"W05-0801.12\">models </entity> that are complex to implement and slow to train . In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics .", "tag": "USAGE"}, {"qas_id": "W05-0801.22_W05-0801.26", "question_text": "statistics [BREAK] algorithms", "context": "Association- Based Bilingual Word Alignment . Bilingual word alignment forms the foundation of current work on statistical machine translation . Standard word-alignment methods involve the use of probabilistic generative <entity id=\"W05-0801.12\">models </entity> that are complex to implement and slow to train . In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics .", "tag": "USAGE"}, {"qas_id": "W06-0103.3_W06-0103.6", "question_text": "Probabilistic <entity id=\"W06-0103.4\">Model [BREAK] Recovery", "context": "Mining Atomic Chinese Abbreviation Pairs: A Probabilistic <entity id=\"W06-0103.4\">Model </entity> For Single Character Word Recovery . \"An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of \"\"atomic abbreviation pairs \"\" from a large text corpus . By an \"\"atomic abbreviation pair ,\"\" it refers to an abbreviated word and its root word (i.e., unabbreviated form ) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be \"\"compositional\"\"; in other words , one can often decode an abbreviated word , such as \"\"nX\"\" (Taiwan University ), character-by-character back to its root form . With a large atomic abbreviation dictionary , one may be able to recover multiple-character abbreviations more easily. With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . \"", "tag": "USAGE"}, {"qas_id": "W06-0103.8_W06-0103.10", "question_text": "paper [BREAK] Model", "context": "Mining Atomic Chinese Abbreviation Pairs: A Probabilistic <entity id=\"W06-0103.4\">Model </entity> For Single Character Word Recovery . \"An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of \"\"atomic abbreviation pairs \"\" from a large text corpus . By an \"\"atomic abbreviation pair ,\"\" it refers to an abbreviated word and its root word (i.e., unabbreviated form ) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be \"\"compositional\"\"; in other words , one can often decode an abbreviated word , such as \"\"nX\"\" (Taiwan University ), character-by-character back to its root form . With a large atomic abbreviation dictionary , one may be able to recover multiple-character abbreviations more easily. With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . \"", "tag": "TOPIC"}, {"qas_id": "W06-0103.13_W06-0103.15", "question_text": "pairs [BREAK] corpus", "context": "Mining Atomic Chinese Abbreviation Pairs: A Probabilistic <entity id=\"W06-0103.4\">Model </entity> For Single Character Word Recovery . \"An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of \"\"atomic abbreviation pairs \"\" from a large text corpus . By an \"\"atomic abbreviation pair ,\"\" it refers to an abbreviated word and its root word (i.e., unabbreviated form ) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be \"\"compositional\"\"; in other words , one can often decode an abbreviated word , such as \"\"nX\"\" (Taiwan University ), character-by-character back to its root form . With a large atomic abbreviation dictionary , one may be able to recover multiple-character abbreviations more easily. With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . \"", "tag": "PART_WHOLE"}, {"qas_id": "W06-0103.40_W06-0103.41", "question_text": "model [BREAK] precision", "context": "Mining Atomic Chinese Abbreviation Pairs: A Probabilistic <entity id=\"W06-0103.4\">Model </entity> For Single Character Word Recovery . \"An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of \"\"atomic abbreviation pairs \"\" from a large text corpus . By an \"\"atomic abbreviation pair ,\"\" it refers to an abbreviated word and its root word (i.e., unabbreviated form ) in which the abbreviation is a single Chinese character. This task is interesting since the abbreviation process for Chinese compound words seems to be \"\"compositional\"\"; in other words , one can often decode an abbreviated word , such as \"\"nX\"\" (Taiwan University ), character-by-character back to its root form . With a large atomic abbreviation dictionary , one may be able to recover multiple-character abbreviations more easily. With only a few training iterations , the acquisition accuracy of the proposed SCR model achieves 62% and 50 % precision for training set and test set , respectively, from the ASWSC-2001 corpus . \"", "tag": "RESULT"}, {"qas_id": "W06-0308.4_W06-0308.5", "question_text": "paper [BREAK] results", "context": "Towards A Validated Model For Affective Classification Of Texts . In this paper , we present the results of experiments aiming to validate a two-dimensional typology of affective states as a suitable basis for affective classification of texts . Using a corpus of English weblog posts, annotated for mood by their authors, we trained support <entity id=\"W06-0308.16\">vector machine </entity> binary classifiers to distinguish texts on the basis of their affiliation with one region of the space . We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology . Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more fine-grained classification along these two axes.", "tag": "TOPIC"}, {"qas_id": "W06-0308.11_W06-0308.13", "question_text": "corpus [BREAK] mood", "context": "Towards A Validated Model For Affective Classification Of Texts . In this paper , we present the results of experiments aiming to validate a two-dimensional typology of affective states as a suitable basis for affective classification of texts . Using a corpus of English weblog posts, annotated for mood by their authors, we trained support <entity id=\"W06-0308.16\">vector machine </entity> binary classifiers to distinguish texts on the basis of their affiliation with one region of the space . We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology . Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more fine-grained classification along these two axes.", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-0308.15_W06-0308.19", "question_text": "support <entity id=\"W06-0308.16\">vector [BREAK] texts", "context": "Towards A Validated Model For Affective Classification Of Texts . In this paper , we present the results of experiments aiming to validate a two-dimensional typology of affective states as a suitable basis for affective classification of texts . Using a corpus of English weblog posts, annotated for mood by their authors, we trained support <entity id=\"W06-0308.16\">vector machine </entity> binary classifiers to distinguish texts on the basis of their affiliation with one region of the space . We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology . Our results indicate that it is possible to extend the standard binary sentiment analysis (positive/negative) approach to a two dimensional model (positive/negative; active/passive), and provide some evidence to support a more fine-grained classification along these two axes.", "tag": "USAGE"}, {"qas_id": "W06-0903.5_W06-0903.6", "question_text": "occurrence [BREAK] words", "context": "Automatic Dating Of Documents And Temporal Text Classification . The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . The algorithm performs in O(n", "tag": "MODEL-FEATURE"}, {"qas_id": "W06-0903.11_W06-0903.12", "question_text": "method [BREAK] extracting", "context": "Automatic Dating Of Documents And Temporal Text Classification . The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . The algorithm performs in O(n", "tag": "USAGE"}, {"qas_id": "W06-0903.13_W06-0903.14", "question_text": "information [BREAK] text", "context": "Automatic Dating Of Documents And Temporal Text Classification . The frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . This work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . The algorithm performs in O(n", "tag": "PART_WHOLE"}, {"qas_id": "W06-1107.1_W06-1107.3", "question_text": "Evaluation [BREAK] Algorithms", "context": "Evaluation Of Several Phonetic Similarity Algorithms On The Task Of Cognate Identification . We investigate the problem of measuring phonetic similarity , focusing on the identification of cognates, words of the same origin in different languages . We compare representatives of two principal approaches to computing phonetic similarity : manually-designed metrics , and learning algorithms . In particular, we consider a stochastic transducer, a Pair HMM, several DBN models , and two constructed schemes . We test those approaches on the task of identifying cognates among Indoeuropean languages , both in the supervised and unsupervised context . Our results suggest that the averaged context DBN model and the Pair HMM achieve the highest accuracy given a large training set of positive examples .", "tag": "TOPIC"}, {"qas_id": "W06-1620.4_W06-1620.9", "question_text": "conditional random <entity id=\"W06-1620.5\">field-based [BREAK] lexical <entity id=\"W06-1620.10\">items", "context": "Multilingual Deep Lexical Acquisition For HPSGs Via Supertagging . We propose a conditional random <entity id=\"W06-1620.5\">field-based </entity> method for supertagging, and apply it to the task of learning new lexical <entity id=\"W06-1620.10\">items </entity> for HPSG-based precision grammars of English and Japanese . Using a pseudo-likelihood approximation we are able to scale our model to hundreds of supertags and tens-of-thousands of training sentences . We show that it is possible to achieve start-of-the-art results for both languages using maximally language-independent lexical features . Further, we explore the performance of the models at the type- and token-level , demonstrating their superior performance when compared to a unigram-based baseline and a transformation-based learning approach .", "tag": "USAGE"}, {"qas_id": "W04-0402.15_W04-0402.16", "question_text": "paper [BREAK] regularity", "context": "Paraphrasing Of Japanese Light- Verb Constructions Based On Lexical Conceptual Structure . Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge . In this paper , we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary . Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases .", "tag": "TOPIC"}, {"qas_id": "W04-0402.23_W04-0402.26", "question_text": "Structures [BREAK] model", "context": "Paraphrasing Of Japanese Light- Verb Constructions Based On Lexical Conceptual Structure . Some particular classes of lexical paraphrases such as verb alteration and compound noun decomposition can be handled by a handful of general rules and lexical semantic knowledge . In this paper , we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs). We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements. We also propose a refinement of an existing LCS dictionary . Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases .", "tag": "USAGE"}, {"qas_id": "W06-2718.2_W06-2718.3", "question_text": "framework [BREAK] integration", "context": "A Standoff Annotation Interface Between DELPH-In Components . We present a standoff annotation framework for the integration of NLP components , currently implemented in the context of the DELPH-IN tools", "tag": "USAGE"}, {"qas_id": "C00-2134.17_C00-2134.18", "question_text": "module [BREAK] texts", "context": "Lexicalized Tree Automata- Based Grammars For Translating Conversational Texts . We propose a new lexicalized grammar formalism called Lexicalized Tree Automata-based Grammar, which lexicalizes tree acceptors instead of trees themselves. We discuss the properties of the grammar and present a chart parsing algorithm . We have implemented a translation module for conversational texts using this formalism , and applied it to an experimental automatic interpretation system (speech translation system ).", "tag": "USAGE"}, {"qas_id": "W04-3203.3_W04-3203.5", "question_text": "approach [BREAK] parsers", "context": "Induction Of Greedy Controllers For Deterministic Treebank Parsers . Most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank. An alternative approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers . We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust , considering their speed and simplicity . They are almost as fast as current part-of-speech taggers, and considerably more accurate than a basic unlexicalized PCFG parser . We also describe Markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .", "tag": "USAGE"}, {"qas_id": "W04-3203.14_W04-3203.15", "question_text": "robust [BREAK] parsers", "context": "Induction Of Greedy Controllers For Deterministic Treebank Parsers . Most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank. An alternative approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers . We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust , considering their speed and simplicity . They are almost as fast as current part-of-speech taggers, and considerably more accurate than a basic unlexicalized PCFG parser . We also describe Markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .", "tag": "MODEL-FEATURE"}, {"qas_id": "W04-3203.24_W04-3203.26", "question_text": "framework [BREAK] modeling", "context": "Induction Of Greedy Controllers For Deterministic Treebank Parsers . Most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank. An alternative approach is to induce a controller for a given parsing automaton. Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers . We use decision trees to learn the controllers. The resulting parsers are surprisingly accurate and robust , considering their speed and simplicity . They are almost as fast as current part-of-speech taggers, and considerably more accurate than a basic unlexicalized PCFG parser . We also describe Markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .", "tag": "USAGE"}, {"qas_id": "W01-0511.8_W01-0511.9", "question_text": "techniques [BREAK] semantic <entity id=\"W01-0511.10\">relations", "context": "Classifying The Semantic Relations In Noun Compounds Via A Domain- Specific Lexical Hierarchy . We are developing corpus-based techniques for identifying semantic <entity id=\"W01-0511.10\">relations </entity> at an intermediate level of description (more specific than those used in case frames , but more general than those used in traditional knowledge representation systems ). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves.", "tag": "USAGE"}, {"qas_id": "W01-0511.17_W01-0511.19", "question_text": "paper [BREAK] algorithm", "context": "Classifying The Semantic Relations In Noun Compounds Via A Domain- Specific Lexical Hierarchy . We are developing corpus-based techniques for identifying semantic <entity id=\"W01-0511.10\">relations </entity> at an intermediate level of description (more specific than those used in case frames , but more general than those used in traditional knowledge representation systems ). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves.", "tag": "TOPIC"}, {"qas_id": "W01-0511.24_W01-0511.26", "question_text": "algorithm [BREAK] approach", "context": "Classifying The Semantic Relations In Noun Compounds Via A Domain- Specific Lexical Hierarchy . We are developing corpus-based techniques for identifying semantic <entity id=\"W01-0511.10\">relations </entity> at an intermediate level of description (more specific than those used in case frames , but more general than those used in traditional knowledge representation systems ). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances , performing better on previously unseen words than a baseline consisting of training on the words themselves.", "tag": "USAGE"}, {"qas_id": "W07-0401.8_W07-0401.11", "question_text": "chunks [BREAK] method", "context": "Chunk- Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation . In this paper , we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation . First, we shallow parse the source language sentences . Then, reordering rules are automatically learned from source-side chunks and word alignments . During translation , the rules are used to generate a reordering lattice for each sentence . Experimental results are reported for a Chinese-to- English task , showing an improvement of 0.5%-1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level .", "tag": "USAGE"}, {"qas_id": "W07-0401.17_W07-0401.19", "question_text": "rules [BREAK] chunks", "context": "Chunk- Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation . In this paper , we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation . First, we shallow parse the source language sentences . Then, reordering rules are automatically learned from source-side chunks and word alignments . During translation , the rules are used to generate a reordering lattice for each sentence . Experimental results are reported for a Chinese-to- English task , showing an improvement of 0.5%-1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level .", "tag": "PART_WHOLE"}, {"qas_id": "W07-0401.24_W07-0401.25", "question_text": "lattice [BREAK] sentence", "context": "Chunk- Level Reordering of Source Language Sentences with Automatically Learned Rules for Statistical Machine Translation . In this paper , we describe a source-side reordering method based on syntactic chunks for phrase-based statistical machine translation . First, we shallow parse the source language sentences . Then, reordering rules are automatically learned from source-side chunks and word alignments . During translation , the rules are used to generate a reordering lattice for each sentence . Experimental results are reported for a Chinese-to- English task , showing an improvement of 0.5%-1.8% BLEU score absolute on various test sets and better computational efficiency than reordering during decoding. The experiments also show that the reordering at the chunk-level performs better than at the POS-level .", "tag": "MODEL-FEATURE"}, {"qas_id": "D07-1059.11_D07-1059.12", "question_text": "analogies [BREAK] text", "context": "Generating Lexical Analogies Using Dependency Relations . A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency <entity id=\"D07-1059.25\">relations </entity> , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .", "tag": "PART_WHOLE"}, {"qas_id": "D07-1059.14_D07-1059.15", "question_text": "study [BREAK] system", "context": "Generating Lexical Analogies Using Dependency Relations . A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency <entity id=\"D07-1059.25\">relations </entity> , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .", "tag": "TOPIC"}, {"qas_id": "D07-1059.18_D07-1059.20", "question_text": "analogies [BREAK] data", "context": "Generating Lexical Analogies Using Dependency Relations . A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency <entity id=\"D07-1059.25\">relations </entity> , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .", "tag": "PART_WHOLE"}, {"qas_id": "D07-1059.21_D07-1059.24", "question_text": "dependency <entity id=\"D07-1059.25\">relations [BREAK] system", "context": "Generating Lexical Analogies Using Dependency Relations . A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency <entity id=\"D07-1059.25\">relations </entity> , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .", "tag": "USAGE"}, {"qas_id": "D07-1059.35_D07-1059.39", "question_text": "system [BREAK] precision", "context": "Generating Lexical Analogies Using Dependency Relations . A lexical analogy is a pair of word-pairs that share a similar semantic relation . Lexical analogies occur frequently in text and are useful in various natural language processing tasks . In this study , we present a system that generates lexical analogies automatically from text data . Our system discovers semantically related pairs of words by using dependency <entity id=\"D07-1059.25\">relations </entity> , and applies novel machine learning algorithms to match these word-pairs to form lexical analogies . Empirical evaluation shows that our system generates valid lexical analogies with a precision of 70% , and produces quality output although not at the level of the best human-generated lexical analogies .", "tag": "RESULT"}, {"qas_id": "I08-3014.11_I08-3014.13", "question_text": "sentences [BREAK] discourse", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "PART_WHOLE"}, {"qas_id": "I08-3014.15_I08-3014.17", "question_text": "words [BREAK] sentences", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "PART_WHOLE"}, {"qas_id": "I08-3014.18_I08-3014.21", "question_text": "Anaphora <entity id=\"I08-3014.19\">resolution [BREAK] discourse <entity id=\"I08-3014.22\">analysis", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "PART_WHOLE"}, {"qas_id": "I08-3014.31_I08-3014.34", "question_text": "paper [BREAK] factors", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "TOPIC"}, {"qas_id": "I08-3014.36_I08-3014.38", "question_text": "order [BREAK] anaphora <entity id=\"I08-3014.39\">resolution", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "RESULT"}, {"qas_id": "I08-3014.40_I08-3014.44", "question_text": "Algorithms [BREAK] rate", "context": "An Optimal Order of Factors for the Computational Treatment of Personal Anaphoric Devices in Urdu Discourse . Handling of human language by computer is a very intricate and complex task . In natural languages , sentences are usually part of discourse units just as words are part of sentences . Anaphora <entity id=\"I08-3014.19\">resolution </entity> plays a significant role in discourse <entity id=\"I08-3014.22\">analysis </entity> for chopping larger discourse units into smaller ones. This process is done for the purpose of better understanding and making easier the further processing of text by computer . This paper is focused on the discussion of various factors and their optimal order that play an important role in personal anaphora <entity id=\"I08-3014.39\">resolution </entity> in Urdu. Algorithms are developed that resolves pronominal anaphoric devices with 77-80% success rate .", "tag": "RESULT"}, {"qas_id": "I08-4004.1_I08-4004.3", "question_text": "Machine <entity id=\"I08-4004.2\">Learning Approach [BREAK] Coreference <entity id=\"I08-4004.4\">Resolution", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "USAGE"}, {"qas_id": "I08-4004.6_I08-4004.7", "question_text": "approach [BREAK] coreference <entity id=\"I08-4004.8\">resolution", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "USAGE"}, {"qas_id": "I08-4004.13_I08-4004.16", "question_text": "learning <entity id=\"I08-4004.14\">method [BREAK] features", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "RESULT"}, {"qas_id": "I08-4004.20_I08-4004.22", "question_text": "knowledge [BREAK] algorithm", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "USAGE"}, {"qas_id": "I08-4004.25_I08-4004.30", "question_text": "method [BREAK] coreference <entity id=\"I08-4004.26\">resolution", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "USAGE"}, {"qas_id": "I08-4004.45_I08-4004.47", "question_text": "method [BREAK] gain", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "RESULT"}, {"qas_id": "I08-4004.49_I08-4004.52", "question_text": "algorithm [BREAK] performance", "context": "An Effective Hybrid Machine <entity id=\"I08-4004.2\">Learning Approach </entity> for Coreference <entity id=\"I08-4004.4\">Resolution </entity> . We present a hybrid machine learning approach for coreference <entity id=\"I08-4004.8\">resolution </entity> . In our method , we use CRFs as basic training model , use active learning <entity id=\"I08-4004.14\">method </entity> to generate combined features so as to make existed features used more effectively; at last, we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . We built a coreference <entity id=\"I08-4004.26\">resolution system </entity> based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of NPs. Experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolution 's performance and our clustering algorithm is very effective; and the key of coreference resolution is to improve the performance of the normal noun 's resolution , especially the pronoun's resolution .", "tag": "RESULT"}, {"qas_id": "P07-1122.4_P07-1122.7", "question_text": "studies [BREAK] parsing", "context": "Generalizing Tree Transformations for Inductive Dependency Parsing . Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages /treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .", "tag": "TOPIC"}, {"qas_id": "P07-1122.9_P07-1122.12", "question_text": "transformations [BREAK] accuracy", "context": "Generalizing Tree Transformations for Inductive Dependency Parsing . Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages /treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .", "tag": "RESULT"}, {"qas_id": "P07-1122.28_P07-1122.31", "question_text": "language [BREAK] parsing", "context": "Generalizing Tree Transformations for Inductive Dependency Parsing . Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages /treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .", "tag": "RESULT"}, {"qas_id": "P07-1122.35_P07-1122.37", "question_text": "strategy [BREAK] transformations", "context": "Generalizing Tree Transformations for Inductive Dependency Parsing . Previous studies in data-driven dependency parsing have shown that tree transformations can improve parsing accuracy for specific parsers and data sets. We investigate to what extent this can be generalized across languages /treebanks and parsers , focusing on pseudo-projective parsing , as a way of capturing non-projective dependencies , and transformations used to facilitate parsing of coordinate structures and verb groups. The results indicate that the beneficial effect of pseudo-projective parsing is independent of parsing strategy but sensitive to language or treebank specific properties . By contrast , the construction specific transformations appear to be more sensitive to parsing strategy but have a constant positive effect over several languages .", "tag": "RESULT"}, {"qas_id": "P07-2035.3_P07-2035.4", "question_text": "Vocabulary [BREAK] Dictionary", "context": "Construction of Domain Dictionary for Fundamental Vocabulary . Guthrie , Joe A. ; Guthrie , Louise ; Aidinejad, Homa; Wilks , Yorick,Subject-Dependent Co- Occurrence And Word Sense Disambiguation ,Annual Meeting Of The Association For Computation al Linguistics ,1991", "tag": "PART_WHOLE"}, {"qas_id": "P08-2061.2_P08-2061.3", "question_text": "Representation [BREAK] Text", "context": "Extracting a Representation from Text for Semantic Analysis . We present a novel fine-grained semantic <entity id=\"P08-2061.6\">representation </entity> of text and an approach to constructing it. This representation is largely extractable by today's technologies and facilitates more detailed semantic analysis . We discuss the requirements driving the representation , suggest how it might be of value in the automated tutoring domain , and provide evidence of its validity .", "tag": "MODEL-FEATURE"}, {"qas_id": "P08-2061.5_P08-2061.7", "question_text": "semantic <entity id=\"P08-2061.6\">representation [BREAK] text", "context": "Extracting a Representation from Text for Semantic Analysis . We present a novel fine-grained semantic <entity id=\"P08-2061.6\">representation </entity> of text and an approach to constructing it. This representation is largely extractable by today's technologies and facilitates more detailed semantic analysis . We discuss the requirements driving the representation , suggest how it might be of value in the automated tutoring domain , and provide evidence of its validity .", "tag": "MODEL-FEATURE"}, {"qas_id": "P08-2061.10_P08-2061.11", "question_text": "representation [BREAK] technologies", "context": "Extracting a Representation from Text for Semantic Analysis . We present a novel fine-grained semantic <entity id=\"P08-2061.6\">representation </entity> of text and an approach to constructing it. This representation is largely extractable by today's technologies and facilitates more detailed semantic analysis . We discuss the requirements driving the representation , suggest how it might be of value in the automated tutoring domain , and provide evidence of its validity .", "tag": "RESULT"}, {"qas_id": "C08-1021.2_C08-1021.4", "question_text": "paper [BREAK] method", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "TOPIC"}, {"qas_id": "C08-1021.6_C08-1021.9", "question_text": "knowledge <entity id=\"C08-1021.7\">bases [BREAK] resources", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "PART_WHOLE"}, {"qas_id": "C08-1021.10_C08-1021.14", "question_text": "algorithm [BREAK] method", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "USAGE"}, {"qas_id": "C08-1021.17_C08-1021.19", "question_text": "concepts [BREAK] knowledge-base", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "PART_WHOLE"}, {"qas_id": "C08-1021.22_C08-1021.23", "question_text": "knowledge [BREAK] corpora", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "PART_WHOLE"}, {"qas_id": "C08-1021.28_C08-1021.29", "question_text": "knowledge [BREAK] resource", "context": "KnowNet: Building a Large Net of Knowledge from the Web . This paper presents a new fully automatic method for building highly dense and accurate knowledge <entity id=\"C08-1021.7\">bases </entity> from existing semantic resources . Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge KnowNet contains outperform any other resource when is empirically evaluated in a common framework .", "tag": "COMPARE"}, {"qas_id": "D07-1122.7_D07-1122.10", "question_text": "method [BREAK] parser", "context": "A Two-Stage Parser for Multilingual Dependency Parsing . We present a two-stage multilingual dependency parsing system submitted to the Multilingual Track of CoNLL-2007. The parser first identifies dependencies using a deterministic parsing method and then labels those dependencies as a sequence labeling problem . We describe the features used in each stage. For four languages with different values of ROOT, we design some special features for the ROOT labeler. Then we present evaluation results and error analyses focusing on Chinese .", "tag": "USAGE"}, {"qas_id": "P02-1035.1_P02-1035.2", "question_text": "Parsing [BREAK] Wall Street <entity id=\"P02-1035.3\">Journal", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "USAGE"}, {"qas_id": "P02-1035.8_P02-1035.11", "question_text": "parser [BREAK] system", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "PART_WHOLE"}, {"qas_id": "P02-1035.18_P02-1035.19", "question_text": "parsing [BREAK] Wall Street <entity id=\"P02-1035.20\">Journal", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "USAGE"}, {"qas_id": "P02-1035.21_P02-1035.24", "question_text": "techniques [BREAK] model", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "PART_WHOLE"}, {"qas_id": "P02-1035.31_P02-1035.32", "question_text": "models [BREAK] estimation", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "USAGE"}, {"qas_id": "P02-1035.44_P02-1035.45", "question_text": "evaluation [BREAK] gold <entity id=\"P02-1035.46\">standard", "context": "Parsing The Wall Street <entity id=\"P02-1035.3\">Journal </entity> Using A Lexical- Functional Grammar And Discriminative Estimation Techniques . We present a stochastic parsing system consisting of a Lexical- Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model . We report on the results of applying this system to parsing the UPenn Wall Street <entity id=\"P02-1035.20\">Journal </entity> (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data . The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models . Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets . On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold <entity id=\"P02-1035.46\">standard </entity> of dependency relations for Brown corpus data achieves 76% F-score.", "tag": "USAGE"}, {"qas_id": "P02-1053.2_P02-1053.4", "question_text": "Orientation [BREAK] Classification", "context": "Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews . This paper presents a simple unsupervised learning <entity id=\"P02-1053.8\">algorithm </entity> for classifying reviews as recommended not recommended semantic orientation", "tag": "USAGE"}, {"qas_id": "P02-1053.5_P02-1053.7", "question_text": "paper [BREAK] learning <entity id=\"P02-1053.8\">algorithm", "context": "Thumbs Up Or Thumbs Down? Semantic Orientation Applied To Unsupervised Classification Of Reviews . This paper presents a simple unsupervised learning <entity id=\"P02-1053.8\">algorithm </entity> for classifying reviews as recommended not recommended semantic orientation", "tag": "TOPIC"}, {"qas_id": "P03-1038.1_P03-1038.5", "question_text": "Models [BREAK] Tagging", "context": "Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging . This paper presents a method to develop a class of variable memory Markov <entity id=\"P03-1038.13\">models </entity> that have higher memory capacity than traditional (uniform memory ) Markov <entity id=\"P03-1038.18\">models </entity> . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov <entity id=\"P03-1038.32\">models </entity> in a part-of-speech tagging task .", "tag": "USAGE"}, {"qas_id": "P03-1038.6_P03-1038.7", "question_text": "paper [BREAK] method", "context": "Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging . This paper presents a method to develop a class of variable memory Markov <entity id=\"P03-1038.13\">models </entity> that have higher memory capacity than traditional (uniform memory ) Markov <entity id=\"P03-1038.18\">models </entity> . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov <entity id=\"P03-1038.32\">models </entity> in a part-of-speech tagging task .", "tag": "TOPIC"}, {"qas_id": "P03-1038.12_P03-1038.17", "question_text": "Markov <entity id=\"P03-1038.13\">models [BREAK] Markov <entity id=\"P03-1038.18\">models", "context": "Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging . This paper presents a method to develop a class of variable memory Markov <entity id=\"P03-1038.13\">models </entity> that have higher memory capacity than traditional (uniform memory ) Markov <entity id=\"P03-1038.18\">models </entity> . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov <entity id=\"P03-1038.32\">models </entity> in a part-of-speech tagging task .", "tag": "COMPARE"}, {"qas_id": "P03-1038.19_P03-1038.23", "question_text": "structure [BREAK] corpus", "context": "Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging . This paper presents a method to develop a class of variable memory Markov <entity id=\"P03-1038.13\">models </entity> that have higher memory capacity than traditional (uniform memory ) Markov <entity id=\"P03-1038.18\">models </entity> . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov <entity id=\"P03-1038.32\">models </entity> in a part-of-speech tagging task .", "tag": "PART_WHOLE"}, {"qas_id": "P03-1038.29_P03-1038.31", "question_text": "models [BREAK] Markov <entity id=\"P03-1038.32\">models", "context": "Self-Organizing Markov Models And Their Application To Part- Of- Speech Tagging . This paper presents a method to develop a class of variable memory Markov <entity id=\"P03-1038.13\">models </entity> that have higher memory capacity than traditional (uniform memory ) Markov <entity id=\"P03-1038.18\">models </entity> . The structure of the variable memory models is induced from a manually annotated corpus through a decision tree learning algorithm . A series of comparative experiments show the resulting models outperform uniform memory Markov <entity id=\"P03-1038.32\">models </entity> in a part-of-speech tagging task .", "tag": "COMPARE"}, {"qas_id": "P04-1037.1_P04-1037.3", "question_text": "Models [BREAK] Sense <entity id=\"P04-1037.2\">Disambiguation", "context": "Unsupervised Sense <entity id=\"P04-1037.2\">Disambiguation </entity> Using Bilingual Probabilistic Models . We describe two probabilistic <entity id=\"P04-1037.5\">models </entity> for unsupervised <entity id=\"P04-1037.7\">word-sense disambiguation </entity> using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .", "tag": "USAGE"}, {"qas_id": "P04-1037.4_P04-1037.6", "question_text": "probabilistic <entity id=\"P04-1037.5\">models [BREAK] <entity id=\"P04-1037.7\">word-sense", "context": "Unsupervised Sense <entity id=\"P04-1037.2\">Disambiguation </entity> Using Bilingual Probabilistic Models . We describe two probabilistic <entity id=\"P04-1037.5\">models </entity> for unsupervised <entity id=\"P04-1037.7\">word-sense disambiguation </entity> using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .", "tag": "USAGE"}, {"qas_id": "P04-1037.24_P04-1037.26", "question_text": "variable [BREAK] model", "context": "Unsupervised Sense <entity id=\"P04-1037.2\">Disambiguation </entity> Using Bilingual Probabilistic Models . We describe two probabilistic <entity id=\"P04-1037.5\">models </entity> for unsupervised <entity id=\"P04-1037.7\">word-sense disambiguation </entity> using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .", "tag": "USAGE"}, {"qas_id": "P04-1037.29_P04-1037.31", "question_text": "models [BREAK] performance", "context": "Unsupervised Sense <entity id=\"P04-1037.2\">Disambiguation </entity> Using Bilingual Probabilistic Models . We describe two probabilistic <entity id=\"P04-1037.5\">models </entity> for unsupervised <entity id=\"P04-1037.7\">word-sense disambiguation </entity> using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .", "tag": "RESULT"}, {"qas_id": "P04-1037.42_P04-1037.43", "question_text": "inventory [BREAK] language", "context": "Unsupervised Sense <entity id=\"P04-1037.2\">Disambiguation </entity> Using Bilingual Probabilistic Models . We describe two probabilistic <entity id=\"P04-1037.5\">models </entity> for unsupervised <entity id=\"P04-1037.7\">word-sense disambiguation </entity> using parallel corpora . The first model , which we call the Sense model , builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language , and recasts their approach in a probabilistic framework . The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsu-pervised approaches , with the Concept model showing the largest improvement . Furthermore, in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language .", "tag": "MODEL-FEATURE"}, {"qas_id": "P04-1059.2_P04-1059.4", "question_text": "paper [BREAK] system", "context": "Adaptive Chinese Word Segmentation . This paper presents a Chinese word segmentation system which can adapt to different domains and standards . We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear <entity id=\"P04-1059.16\">models </entity> . We explore several features and describe how to create training data by sampling . We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.", "tag": "TOPIC"}, {"qas_id": "P04-1059.12_P04-1059.15", "question_text": "linear <entity id=\"P04-1059.16\">models [BREAK] approach", "context": "Adaptive Chinese Word Segmentation . This paper presents a Chinese word segmentation system which can adapt to different domains and standards . We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear <entity id=\"P04-1059.16\">models </entity> . We explore several features and describe how to create training data by sampling . We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.", "tag": "USAGE"}, {"qas_id": "P04-1059.27_P04-1059.29", "question_text": "Evaluation [BREAK] system", "context": "Adaptive Chinese Word Segmentation . This paper presents a Chinese word segmentation system which can adapt to different domains and standards . We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear <entity id=\"P04-1059.16\">models </entity> . We explore several features and describe how to create training data by sampling . We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.", "tag": "TOPIC"}, {"qas_id": "P04-1059.32_P04-1059.33", "question_text": "system [BREAK] performance", "context": "Adaptive Chinese Word Segmentation . This paper presents a Chinese word segmentation system which can adapt to different domains and standards . We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear <entity id=\"P04-1059.16\">models </entity> . We explore several features and describe how to create training data by sampling . We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . Evaluation of the proposed system on five test sets with different standards shows that the system achieves stateof-the-art performance on all of them.", "tag": "RESULT"}, {"qas_id": "P04-3029.4_P04-3029.8", "question_text": "techniques [BREAK] dialogue <entity id=\"P04-3029.5\">system", "context": "Multimodal Database Access On Handheld Devices . We present the final MIAMM system , a multimodal dialogue <entity id=\"P04-3029.5\">system </entity> that employs speech , haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices .", "tag": "USAGE"}, {"qas_id": "P05-2024.4_P05-2024.10", "question_text": "paper [BREAK] parser", "context": "Corpus- Oriented Development Of Japanese HPSG Parsers . This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser . We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules , and then extracted lexical <entity id=\"P05-2024.16\">entries </entity> from the tree-bank . The grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development . We also trained a statistical parser for the grammar on the tree-bank , and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis .", "tag": "TOPIC"}, {"qas_id": "P05-2024.15_P05-2024.17", "question_text": "lexical <entity id=\"P05-2024.16\">entries [BREAK] tree-bank", "context": "Corpus- Oriented Development Of Japanese HPSG Parsers . This paper reports the corpus-oriented development of a wide-coverage Japanese HPSG parser . We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules , and then extracted lexical <entity id=\"P05-2024.16\">entries </entity> from the tree-bank . The grammar developed using this method attained wide coverage that could hardly be obtained by conventional manual development . We also trained a statistical parser for the grammar on the tree-bank , and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis .", "tag": "PART_WHOLE"}, {"qas_id": "C82-1067.4_C82-1067.6", "question_text": "Dictionary [BREAK] Processing", "context": "Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing . This is a report on the semantic dictionary for natural language processing we are constructing now. This paper explains how to obtain the semantic <entity id=\"C82-1067.14\">information </entity> for the dictionary from an ordinary Japanese language dictionary with about 60,000 items (which had already been put into machine readable form ) and also explains what should be the frame for the representation of meaning of each item ( word ). Then a man-assisted machine procedure that embeds the semantic graph with respect to the head word of the ordinary dictionary into the frame of a head word is discussed.", "tag": "USAGE"}, {"qas_id": "C82-1067.7_C82-1067.9", "question_text": "report [BREAK] dictionary", "context": "Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing . This is a report on the semantic dictionary for natural language processing we are constructing now. This paper explains how to obtain the semantic <entity id=\"C82-1067.14\">information </entity> for the dictionary from an ordinary Japanese language dictionary with about 60,000 items (which had already been put into machine readable form ) and also explains what should be the frame for the representation of meaning of each item ( word ). Then a man-assisted machine procedure that embeds the semantic graph with respect to the head word of the ordinary dictionary into the frame of a head word is discussed.", "tag": "TOPIC"}, {"qas_id": "C82-1067.13_C82-1067.18", "question_text": "semantic <entity id=\"C82-1067.14\">information [BREAK] dictionary", "context": "Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing . This is a report on the semantic dictionary for natural language processing we are constructing now. This paper explains how to obtain the semantic <entity id=\"C82-1067.14\">information </entity> for the dictionary from an ordinary Japanese language dictionary with about 60,000 items (which had already been put into machine readable form ) and also explains what should be the frame for the representation of meaning of each item ( word ). Then a man-assisted machine procedure that embeds the semantic graph with respect to the head word of the ordinary dictionary into the frame of a head word is discussed.", "tag": "PART_WHOLE"}, {"qas_id": "C82-1067.23_C82-1067.24", "question_text": "representation [BREAK] item", "context": "Man-Assisted Machine Construction Of A Semantic Dictionary For Natural Language Processing . This is a report on the semantic dictionary for natural language processing we are constructing now. This paper explains how to obtain the semantic <entity id=\"C82-1067.14\">information </entity> for the dictionary from an ordinary Japanese language dictionary with about 60,000 items (which had already been put into machine readable form ) and also explains what should be the frame for the representation of meaning of each item ( word ). Then a man-assisted machine procedure that embeds the semantic graph with respect to the head word of the ordinary dictionary into the frame of a head word is discussed.", "tag": "MODEL-FEATURE"}, {"qas_id": "C86-1077.27_C86-1077.29", "question_text": "corpus [BREAK] documentation", "context": "Strategies For Interactive Machine Translation : The Experience And Implications Of The UMIST Japanese Project . At the Contre for Computational Linguistics , we are designing and implementing an Eng.lish-to- Japanese interactive machine translation system . The project is funded jointly by the Alvey Directorate and International Computers Limited (ICL). The prototype system runs on the ICL PERQ, though much of the development work has been done on a VAX 11/750. It is implemented in Prolog, in the interests of rapid prototyping , but intended for optimization . The informing principles are those of modern comp1 ex-feature-based linguistic <entity id=\"C86-1077.22\">theories </entity> , in particular Lexical- Functional Grammar ( Bresnan (ed.) 1982, Kaplan and Bresnan 1982 ), and Generalized Phrase Structure Grammar ( Gazdar et al. 1985 ). For development purposes we are using an existing corpus of 10,000 words of continuous prose from the PERQ's graphics documentation ; in the long term , the system will be extended for use by technical writers in fields other than software . At the time of writing, we have well-developed system development software , user interface , and grammar and dictionary handling utilities . The English analysis grammar handles most of the syntactic structures of the corpus , and we have a range of formats for output of linguistic representations and Japanese text . A transfer grammar for English- Japanese has been prototyped, but is not not yet fully adequate to handle all constructions in the corpus ; a facility for dictionary entry in kanji is incorporated. The aspect of the system we will focus on in the present paper is its interactive nature , discussing the range of different types of interaction which are provided or permitted for different types of user .", "tag": "PART_WHOLE"}, {"qas_id": "C88-1065.4_C88-1065.7", "question_text": "paper [BREAK] Question <entity id=\"C88-1065.8\">Answering system", "context": "Exploiting Lexical Regularities In Designing Natural Language Systems . This paper presents the lexical component of the START Question <entity id=\"C88-1065.8\">Answering system </entity> developed at the MIT Artificial Intelligence Laboratory . START is able to interpret correctly a wide range of semantic relationships associated with alternate expressions of the arguments of verbs . The design of the system takes advantage of the results of recent linguistic research into the structure of the lexicon , allowing START to attain a broader range of coverage than many existing systems while maintaining modular organization .", "tag": "TOPIC"}, {"qas_id": "C88-1065.17_C88-1065.21", "question_text": "research [BREAK] design", "context": "Exploiting Lexical Regularities In Designing Natural Language Systems . This paper presents the lexical component of the START Question <entity id=\"C88-1065.8\">Answering system </entity> developed at the MIT Artificial Intelligence Laboratory . START is able to interpret correctly a wide range of semantic relationships associated with alternate expressions of the arguments of verbs . The design of the system takes advantage of the results of recent linguistic research into the structure of the lexicon , allowing START to attain a broader range of coverage than many existing systems while maintaining modular organization .", "tag": "USAGE"}, {"qas_id": "C88-2095.5_C88-2095.7", "question_text": "results [BREAK] parser", "context": "Reasons Why I Do Not Care Grammar Formalism . has borrowed a lot of ideas from We could not have developed even a simple parser without the research results in It is obviously nonsense to claim that we, computational linguists , do not care research results in However, the researchers in it seems to me, are very fond of especially, those who are called They always fight with each other by asserting that their are superior to the others'. They are oversensitive and tend to distinguish people into two groups, and A computational linguist using LFG (or LFG) as a small part in his total system is taken as the ally of LFG, and is certainly accused by the other groups. They promptly demonstrate that LFG is wrong, by showing a lot of peculiar sentences which rarely appear in real texts .Formalisms are prepared for accomplishing specific purposes . The formalisms in have been proposed , roughly speaking, for describing the of distinguishing from arbitrary /ramraa/ica/ sequences , and of relating the grammatical sequences with the other representational levels .On the other hand , a formalism we need in CL is for different purposes . That is, we need a formalism for describing the rules of distinguishing the most feasible grammatical structures from other less feasible but still ones of the same sentences We also need a formalism in which we can manage systematically a large amount of knowledge of various sorts necessary for NLP.Formalisms for different purposes , of course, should be evaluated based on different standards . The current discussions of different formalisms in TL are irrelevant to our standards , though they may be important for their The following is a list of the reasons why I think so.", "tag": "USAGE"}, {"qas_id": "C88-2095.18_C88-2095.19", "question_text": "sentences [BREAK] texts", "context": "Reasons Why I Do Not Care Grammar Formalism . has borrowed a lot of ideas from We could not have developed even a simple parser without the research results in It is obviously nonsense to claim that we, computational linguists , do not care research results in However, the researchers in it seems to me, are very fond of especially, those who are called They always fight with each other by asserting that their are superior to the others'. They are oversensitive and tend to distinguish people into two groups, and A computational linguist using LFG (or LFG) as a small part in his total system is taken as the ally of LFG, and is certainly accused by the other groups. They promptly demonstrate that LFG is wrong, by showing a lot of peculiar sentences which rarely appear in real texts .Formalisms are prepared for accomplishing specific purposes . The formalisms in have been proposed , roughly speaking, for describing the of distinguishing from arbitrary /ramraa/ica/ sequences , and of relating the grammatical sequences with the other representational levels .On the other hand , a formalism we need in CL is for different purposes . That is, we need a formalism for describing the rules of distinguishing the most feasible grammatical structures from other less feasible but still ones of the same sentences We also need a formalism in which we can manage systematically a large amount of knowledge of various sorts necessary for NLP.Formalisms for different purposes , of course, should be evaluated based on different standards . The current discussions of different formalisms in TL are irrelevant to our standards , though they may be important for their The following is a list of the reasons why I think so.", "tag": "PART_WHOLE"}, {"qas_id": "C88-2130.15_C88-2130.16", "question_text": "analysis [BREAK] corpus", "context": "Directing The Generation Of Living Space Descriptions . We have developed a Computational model of the process of describing the layout of an apartment or house, a much-studied discourse task first characterized linguistically by Linde (1974). The model is embodied in a program , APT, that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .", "tag": "TOPIC"}, {"qas_id": "C88-2148.4_C88-2148.6", "question_text": "semantic <entity id=\"C88-2148.5\">analysis [BREAK] topic", "context": "Topic / Focus Articulation And Intensional Logic . A semantic <entity id=\"C88-2148.5\">analysis </entity> of topic and focus as two parts of tectograamatical representation by means of transparent intensional logic (TIL) ia presented. It is pointed out that two sentences (more precisely, their tectograamatical representations ) differing just in the topic / focus articulation (TFA) denote different propositions, i.e. that TFA has an effect upon the semantic content of the sentence . An inforaal short description of an algorithm handling the TFA in the translation of tectogrammatical representations into the constructions of TIL is added. The TFA algorithm divides a representation into two parts corresponding to the topic and focus ; every part is analyzed ( translated ) in isolation and then the resulting construction is put together. The TIL construction discussed here", "tag": "TOPIC"}, {"qas_id": "C88-2148.17_C88-2148.18", "question_text": "content [BREAK] sentence", "context": "Topic / Focus Articulation And Intensional Logic . A semantic <entity id=\"C88-2148.5\">analysis </entity> of topic and focus as two parts of tectograamatical representation by means of transparent intensional logic (TIL) ia presented. It is pointed out that two sentences (more precisely, their tectograamatical representations ) differing just in the topic / focus articulation (TFA) denote different propositions, i.e. that TFA has an effect upon the semantic content of the sentence . An inforaal short description of an algorithm handling the TFA in the translation of tectogrammatical representations into the constructions of TIL is added. The TFA algorithm divides a representation into two parts corresponding to the topic and focus ; every part is analyzed ( translated ) in isolation and then the resulting construction is put together. The TIL construction discussed here", "tag": "PART_WHOLE"}, {"qas_id": "C88-2148.19_C88-2148.20", "question_text": "description [BREAK] algorithm", "context": "Topic / Focus Articulation And Intensional Logic . A semantic <entity id=\"C88-2148.5\">analysis </entity> of topic and focus as two parts of tectograamatical representation by means of transparent intensional logic (TIL) ia presented. It is pointed out that two sentences (more precisely, their tectograamatical representations ) differing just in the topic / focus articulation (TFA) denote different propositions, i.e. that TFA has an effect upon the semantic content of the sentence . An inforaal short description of an algorithm handling the TFA in the translation of tectogrammatical representations into the constructions of TIL is added. The TFA algorithm divides a representation into two parts corresponding to the topic and focus ; every part is analyzed ( translated ) in isolation and then the resulting construction is put together. The TIL construction discussed here", "tag": "TOPIC"}, {"qas_id": "C90-1021.3_C90-1021.8", "question_text": "experiments [BREAK] system", "context": "Bilingual Generation Of Weather Forecasts In An Operations Environment . In 1986 the first experiments in text generation applied to weather forecasts resulted in a prototype system (RAREAS[6,3]) for producing English marine bulletins from forecast data . Subsequent work in 1987 added French output to make the initial system bilingual (RAREAS-2[llj). During 1988 -1989 a full-scale operational system was created to meet the needs of daily marine forecast production for three regional centres in the Canadian Atmospheric Environment Service", "tag": "RESULT"}, {"qas_id": "C90-3009.3_C90-3009.4", "question_text": "paper [BREAK] language", "context": "Syllable- Based Morphology . This paper presents a language for the description of morphological alternations which is based on syllable structure . The justification for such an approach is discussed with reference to examples from a variety of languages and the approach is compared to Koskenniemi 's two-level account of morphonoiogy. Keywords: morphology , phonology , syllables.", "tag": "TOPIC"}, {"qas_id": "C92-4211.4_C92-4211.6", "question_text": "Corpus [BREAK] Parsing", "context": "Knowledge Acquisition And Chinese Parsing Based On Corpus . In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system . In this paper , we will introduce a corpus based Chinese parsing system . Our efforts arc concctrated on: (1) knowledge acquisition and representation ; and (2) the parsing scheme . The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e. the four axioms of the Dependency Grammar (DG). In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .", "tag": "USAGE"}, {"qas_id": "C92-4211.13_C92-4211.18", "question_text": "paper [BREAK] system", "context": "Knowledge Acquisition And Chinese Parsing Based On Corpus . In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system . In this paper , we will introduce a corpus based Chinese parsing system . Our efforts arc concctrated on: (1) knowledge acquisition and representation ; and (2) the parsing scheme . The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e. the four axioms of the Dependency Grammar (DG). In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .", "tag": "TOPIC"}, {"qas_id": "C92-4211.26_C92-4211.29", "question_text": "knowledge [BREAK] corpus", "context": "Knowledge Acquisition And Chinese Parsing Based On Corpus . In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system . In this paper , we will introduce a corpus based Chinese parsing system . Our efforts arc concctrated on: (1) knowledge acquisition and representation ; and (2) the parsing scheme . The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e. the four axioms of the Dependency Grammar (DG). In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .", "tag": "PART_WHOLE"}, {"qas_id": "C92-4211.37_C92-4211.39", "question_text": "parsing [BREAK] sentences", "context": "Knowledge Acquisition And Chinese Parsing Based On Corpus . In Natural Language Processing (NLP), one key problem is how to design a robust and effective parsing system . In this paper , we will introduce a corpus based Chinese parsing system . Our efforts arc concctrated on: (1) knowledge acquisition and representation ; and (2) the parsing scheme . The knowledge of this system is principally extracted from analyzed corpus , others are a few grammatical principles , i.e. the four axioms of the Dependency Grammar (DG). In addition , we also propose the fifth axiom of DG to support the parsing of Chinese sentences .", "tag": "USAGE"}, {"qas_id": "C94-1023.13_C94-1023.14", "question_text": "probabilistic <entity id=\"C94-1023.15\">model [BREAK] error", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "RESULT"}, {"qas_id": "C94-1023.19_C94-1023.20", "question_text": "paper [BREAK] method", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "TOPIC"}, {"qas_id": "C94-1023.21_C94-1023.24", "question_text": "features [BREAK] refinement", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "USAGE"}, {"qas_id": "C94-1023.42_C94-1023.45", "question_text": "features [BREAK] classification <entity id=\"C94-1023.43\">model", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "USAGE"}, {"qas_id": "C94-1023.49_C94-1023.50", "question_text": "tagging [BREAK] Brown <entity id=\"C94-1023.51\">Corpus", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "USAGE"}, {"qas_id": "C94-1023.52_C94-1023.54", "question_text": "classification <entity id=\"C94-1023.53\">model [BREAK] error <entity id=\"C94-1023.55\">rate", "context": "Automatic Model Refinement - With An Application To Tagging . Statistical NLP models usually only consider coarse information and very restricted context to make the estimation of parameters feasible. To reduce the modeling error introduced by a simplified probabilistic <entity id=\"C94-1023.15\">model </entity> , the Classification and Regression Tree (CART) method was adopted in this paper to select more discriminative features for automatic model refinement . Because the features are adopted dependently during splitting the classification tree in CART, the number of training data in each terminal node is small, which makes the labeling process of terminal nodes not robust . This over-tuning phenomenon cannot be completely removed by cross-validation process (i.e., pruning process ). A probabilistic classification <entity id=\"C94-1023.43\">model </entity> based on the selected discriminative features is thus proposed to use the training data more efficiently. In tagging the Brown <entity id=\"C94-1023.51\">Corpus </entity> , our probabilistic classification <entity id=\"C94-1023.53\">model </entity> reduces the error <entity id=\"C94-1023.55\">rate </entity> of the top 10 error dominant words from 5.71% to 4.35%, which shows 23.82% improvement over the unrefined model .", "tag": "RESULT"}, {"qas_id": "C94-1055.2_C94-1055.3", "question_text": "Documents [BREAK] Knowledge <entity id=\"C94-1055.4\">Base", "context": "Generating Multilingual Documents From A Knowledge <entity id=\"C94-1055.4\">Base </entity> The TECHDOC Project . TECHDOC is an implemented system demonstrating the feasibility of generating multilingu.il technical documents on the basis of a language-independent knowledge <entity id=\"C94-1055.13\">base </entity> . Its application domain is use]- and maintenance instructions , which are produced from underlying plan structures representing the activities, the participating objects with their properties , relations , and so on. This paper gives a brief outline of the system <entity id=\"C94-1055.25\">architecture </entity> and discusses some recent developments in the project : the addition of actual event simulation in the KM, steps towards a document authoring tool , and a multimodal user interface .", "tag": "PART_WHOLE"}, {"qas_id": "C94-1055.9_C94-1055.12", "question_text": "documents [BREAK] knowledge <entity id=\"C94-1055.13\">base", "context": "Generating Multilingual Documents From A Knowledge <entity id=\"C94-1055.4\">Base </entity> The TECHDOC Project . TECHDOC is an implemented system demonstrating the feasibility of generating multilingu.il technical documents on the basis of a language-independent knowledge <entity id=\"C94-1055.13\">base </entity> . Its application domain is use]- and maintenance instructions , which are produced from underlying plan structures representing the activities, the participating objects with their properties , relations , and so on. This paper gives a brief outline of the system <entity id=\"C94-1055.25\">architecture </entity> and discusses some recent developments in the project : the addition of actual event simulation in the KM, steps towards a document authoring tool , and a multimodal user interface .", "tag": "PART_WHOLE"}, {"qas_id": "C94-1055.22_C94-1055.24", "question_text": "paper [BREAK] system <entity id=\"C94-1055.25\">architecture", "context": "Generating Multilingual Documents From A Knowledge <entity id=\"C94-1055.4\">Base </entity> The TECHDOC Project . TECHDOC is an implemented system demonstrating the feasibility of generating multilingu.il technical documents on the basis of a language-independent knowledge <entity id=\"C94-1055.13\">base </entity> . Its application domain is use]- and maintenance instructions , which are produced from underlying plan structures representing the activities, the participating objects with their properties , relations , and so on. This paper gives a brief outline of the system <entity id=\"C94-1055.25\">architecture </entity> and discusses some recent developments in the project : the addition of actual event simulation in the KM, steps towards a document authoring tool , and a multimodal user interface .", "tag": "TOPIC"}, {"qas_id": "C94-2138.1_C94-2138.3", "question_text": "Algorithm [BREAK] Network", "context": "A Reestimation Algorithm For Probabilistic Ttecursive Transition Network . Probabilistic. Recursive Transition Network (I'KTN) is an elevated version of RTN to model and process languages in stochastic, parameters . The representation is a direct derivation from the KTN and keeps much the spirit of Hidden Markov Model at the same time . We present a reestimation algorithm for PHTN that is a variation of Inside Outside algorithm that computes the values of the probabilistic parameters from sample sentences (parsed or uuparsed).", "tag": "USAGE"}, {"qas_id": "C94-2138.5_C94-2138.9", "question_text": "Network [BREAK] languages", "context": "A Reestimation Algorithm For Probabilistic Ttecursive Transition Network . Probabilistic. Recursive Transition Network (I'KTN) is an elevated version of RTN to model and process languages in stochastic, parameters . The representation is a direct derivation from the KTN and keeps much the spirit of Hidden Markov Model at the same time . We present a reestimation algorithm for PHTN that is a variation of Inside Outside algorithm that computes the values of the probabilistic parameters from sample sentences (parsed or uuparsed).", "tag": "USAGE"}, {"qas_id": "C86-1052.8_C86-1052.9", "question_text": "descriptions [BREAK] lexical <entity id=\"C86-1052.10\">items", "context": "DCKR - Knowledge Representation In Prolog And Its Application To Natural Language Processing . \"important tasks for natural language processing . Basin to semantic processing is descriptions of lexical <entity id=\"C86-1052.10\">items </entity> . The most frequently used form of description of lexical <entity id=\"C86-1052.14\">items </entity> is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing . A method of the Object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in DCKR, part of context-processing and the greater part of semantic processing can be left to the functions built in Prolog. 09) se ( animal ,age : X,_) :- bottomof(S,B), sem(B,birthYear:Y,_), X is 1986 - Y. 10) seni(face,P,S) :- hasa(eye,P,[face I S]) ; hasa(nose,P,[face IS] ) ; has a(mou th,P,[face !S]). Now the meanings of the gem, isa and hasa predicates, which are important to descriptions in DCKR, are explained later using the DCKR examples given above. 1. I n troduct i on Relationships between knowledge represented predicate logic formulas and knowledge represented Frames or Siruciured objects are clarified by [Hayes 80], [ Nilsson 80], CGoebel 85],[ Bowen 85], al, but their methods requires separately interpreter for their representation . et an The authors have developed a knowledge representation form called DCKR (Definite Clause Knowledge Representation ) [ Koyama 85]. In DCKR, each of the sip_ts composing of a Structured Object (hereinafter simply called an object ) is represented by a Horn clause (a Prolog statement) with the \"\" sen \"\" predicate (to be explained in Section 2) as its head. Therefore, an Object can be regarded as a set of Horn clauses (slots) headed by the sen predicate with the same first argument . From the foregoing it follows that almost all of a program for performing semantic intepretations relative to lexical items described in DCKR can be replaced by functions built in Prolog. That is, most of programming efforts of semantic processing can be left to the functions built in Pro 1og. DCKR will be described in detail in Section 2. Section 3 will discuss applications of DCKR to semantic processing of natural languages . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "C86-1052.12_C86-1052.13", "question_text": "description [BREAK] lexical <entity id=\"C86-1052.14\">items", "context": "DCKR - Knowledge Representation In Prolog And Its Application To Natural Language Processing . \"important tasks for natural language processing . Basin to semantic processing is descriptions of lexical <entity id=\"C86-1052.10\">items </entity> . The most frequently used form of description of lexical <entity id=\"C86-1052.14\">items </entity> is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing . A method of the Object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in DCKR, part of context-processing and the greater part of semantic processing can be left to the functions built in Prolog. 09) se ( animal ,age : X,_) :- bottomof(S,B), sem(B,birthYear:Y,_), X is 1986 - Y. 10) seni(face,P,S) :- hasa(eye,P,[face I S]) ; hasa(nose,P,[face IS] ) ; has a(mou th,P,[face !S]). Now the meanings of the gem, isa and hasa predicates, which are important to descriptions in DCKR, are explained later using the DCKR examples given above. 1. I n troduct i on Relationships between knowledge represented predicate logic formulas and knowledge represented Frames or Siruciured objects are clarified by [Hayes 80], [ Nilsson 80], CGoebel 85],[ Bowen 85], al, but their methods requires separately interpreter for their representation . et an The authors have developed a knowledge representation form called DCKR (Definite Clause Knowledge Representation ) [ Koyama 85]. In DCKR, each of the sip_ts composing of a Structured Object (hereinafter simply called an object ) is represented by a Horn clause (a Prolog statement) with the \"\" sen \"\" predicate (to be explained in Section 2) as its head. Therefore, an Object can be regarded as a set of Horn clauses (slots) headed by the sen predicate with the same first argument . From the foregoing it follows that almost all of a program for performing semantic intepretations relative to lexical items described in DCKR can be replaced by functions built in Prolog. That is, most of programming efforts of semantic processing can be left to the functions built in Pro 1og. DCKR will be described in detail in Section 2. Section 3 will discuss applications of DCKR to semantic processing of natural languages . \"", "tag": "MODEL-FEATURE"}, {"qas_id": "C86-1052.72_C86-1052.73", "question_text": "Section [BREAK] applications", "context": "DCKR - Knowledge Representation In Prolog And Its Application To Natural Language Processing . \"important tasks for natural language processing . Basin to semantic processing is descriptions of lexical <entity id=\"C86-1052.10\">items </entity> . The most frequently used form of description of lexical <entity id=\"C86-1052.14\">items </entity> is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing . A method of the Object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in DCKR, part of context-processing and the greater part of semantic processing can be left to the functions built in Prolog. 09) se ( animal ,age : X,_) :- bottomof(S,B), sem(B,birthYear:Y,_), X is 1986 - Y. 10) seni(face,P,S) :- hasa(eye,P,[face I S]) ; hasa(nose,P,[face IS] ) ; has a(mou th,P,[face !S]). Now the meanings of the gem, isa and hasa predicates, which are important to descriptions in DCKR, are explained later using the DCKR examples given above. 1. I n troduct i on Relationships between knowledge represented predicate logic formulas and knowledge represented Frames or Siruciured objects are clarified by [Hayes 80], [ Nilsson 80], CGoebel 85],[ Bowen 85], al, but their methods requires separately interpreter for their representation . et an The authors have developed a knowledge representation form called DCKR (Definite Clause Knowledge Representation ) [ Koyama 85]. In DCKR, each of the sip_ts composing of a Structured Object (hereinafter simply called an object ) is represented by a Horn clause (a Prolog statement) with the \"\" sen \"\" predicate (to be explained in Section 2) as its head. Therefore, an Object can be regarded as a set of Horn clauses (slots) headed by the sen predicate with the same first argument . From the foregoing it follows that almost all of a program for performing semantic intepretations relative to lexical items described in DCKR can be replaced by functions built in Prolog. That is, most of programming efforts of semantic processing can be left to the functions built in Pro 1og. DCKR will be described in detail in Section 2. Section 3 will discuss applications of DCKR to semantic processing of natural languages . \"", "tag": "TOPIC"}, {"qas_id": "C88-1048.10_C88-1048.12", "question_text": "strategy [BREAK] performance", "context": "Improving Search Strategies An Experiment In Best-First Parsing . \"Viewing the syntactic analysis of natural language as a search problem , the right choice of parsing strategy plays an important role in the performance of natural language parsers . After a motivation of the use of various heuristic criteria , a framework for defining and testing ' parsing strategies is presented. On this basis systematic tests on different parsing strategies have been performed , the results of which are dicussed. Generally ;hese tests show that a \"\"guided\"\" depth-oriented strategy gives a considerable reduction of search effort compared to the classical depth first strategy . \"", "tag": "RESULT"}, {"qas_id": "C88-1048.22_C88-1048.24", "question_text": "tests [BREAK] strategies", "context": "Improving Search Strategies An Experiment In Best-First Parsing . \"Viewing the syntactic analysis of natural language as a search problem , the right choice of parsing strategy plays an important role in the performance of natural language parsers . After a motivation of the use of various heuristic criteria , a framework for defining and testing ' parsing strategies is presented. On this basis systematic tests on different parsing strategies have been performed , the results of which are dicussed. Generally ;hese tests show that a \"\"guided\"\" depth-oriented strategy gives a considerable reduction of search effort compared to the classical depth first strategy . \"", "tag": "TOPIC"}, {"qas_id": "C88-2122.23_C88-2122.24", "question_text": "generation [BREAK] output", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "USAGE"}, {"qas_id": "C88-2122.26_C88-2122.27", "question_text": "analysis [BREAK] input", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "TOPIC"}, {"qas_id": "C88-2122.28_C88-2122.29", "question_text": "part [BREAK] strategy", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "TOPIC"}, {"qas_id": "C88-2122.30_C88-2122.31", "question_text": "generating [BREAK] output", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "USAGE"}, {"qas_id": "C88-2122.47_C88-2122.49", "question_text": "component [BREAK] expressions", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "USAGE"}, {"qas_id": "C88-2122.51_C88-2122.52", "question_text": "factors [BREAK] gestures", "context": "Generating Multimodal Output- Conditions, Advantages And Problems . \"In natural communication situations , multimodal referent specification is frequent and efficient. The linguistic component are deictic expressions , e.g. 'this' and 'here'. Extralinguistic devices in dialogs are different body movements, mainly pointing gestures . Their functional equivalent in texts are means like arrows and indices . This paper has two intentions. First, it discusses the advantages of multimodal reference in interhuman communication which motivate the integration of extralinguistic \"\"pointing\"\" devices into NL dialog systems . The generation of multimodal output poses specific problems , which have no counterpart in the analysis of multimodal input . The second part presents the strategy for generating multimodal output which has been developed within the framework of the XTRA system (a NL access system to expert systems ). XTRA allows the combination of verbal descriptions and pointing gestures in order to specify elements of the given visual context , i.e. a form displayed on the screen . The component POPEL generates referential expressions which may be accompanied by a pointing gesture . The appearance of these gestures depends on several factors , e.g. the type of referent (whether it is a region or an entry of the form ) and its complexity . \"", "tag": "RESULT"}, {"qas_id": "D08-1060.18_D08-1060.19", "question_text": "rules [BREAK] word-reordering", "context": "Generalizing Local and Non-Local Word- Reordering Patterns for Syntax- Based Machine Translation . Syntactic word reordering is essential for translations across different grammar structures between syntactically distant language-pairs . In this paper , we propose to embed local and non-local word reordering decisions in a synchronous context free grammar , and leverages the grammar in a chart-based decoder . Local word-reordering is effectively encoded in Hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to source-side syntactic constituents . We demonstrate how these rules are learned from parallel <entity id=\"D08-1060.31\">corpora </entity> . Our proposed shallow Tree-to- String rules show significant improvements in translation <entity id=\"D08-1060.38\">quality </entity> across different test sets .", "tag": "MODEL-FEATURE"}, {"qas_id": "D08-1060.29_D08-1060.30", "question_text": "rules [BREAK] parallel <entity id=\"D08-1060.31\">corpora", "context": "Generalizing Local and Non-Local Word- Reordering Patterns for Syntax- Based Machine Translation . Syntactic word reordering is essential for translations across different grammar structures between syntactically distant language-pairs . In this paper , we propose to embed local and non-local word reordering decisions in a synchronous context free grammar , and leverages the grammar in a chart-based decoder . Local word-reordering is effectively encoded in Hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to source-side syntactic constituents . We demonstrate how these rules are learned from parallel <entity id=\"D08-1060.31\">corpora </entity> . Our proposed shallow Tree-to- String rules show significant improvements in translation <entity id=\"D08-1060.38\">quality </entity> across different test sets .", "tag": "PART_WHOLE"}, {"qas_id": "I05-1060.4_I05-1060.5", "question_text": "Lexicon [BREAK] Corpus", "context": "Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus . Abstract .", "tag": "PART_WHOLE"}, {"qas_id": "I05-1063.2_I05-1063.3", "question_text": "Model [BREAK] Coreference <entity id=\"I05-1063.4\">Resolution", "context": "A Twin- Candidate Model of Coreference <entity id=\"I05-1063.4\">Resolution </entity> with Non-Anaphor Identification Capability . Abstract .", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-2039.3_I05-2039.5", "question_text": "Homogeneity [BREAK] Performance", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "RESULT"}, {"qas_id": "I05-2039.9_I05-2039.12", "question_text": "homogeneity [BREAK] performance", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "RESULT"}, {"qas_id": "I05-2039.13_I05-2039.15", "question_text": "Experiments [BREAK] language <entity id=\"I05-2039.16\">models", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "USAGE"}, {"qas_id": "I05-2039.18_I05-2039.19", "question_text": "translating [BREAK] Japanese", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "USAGE"}, {"qas_id": "I05-2039.34_I05-2039.36", "question_text": "sizes [BREAK] data", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-2039.49_I05-2039.51", "question_text": "Domain [BREAK] performance", "context": "The Influence of Data Homogeneity on NLP System Performance . In this work we study the influence of corpus homogeneity on corpus-based NLP system performance . Experiments are performed on both stochastic language <entity id=\"I05-2039.16\">models </entity> and an EBMT system translating from Japanese to English with a large bicorpus, in order to reassess the assumption that using only homogeneous data tends to make system performance go up. We describe a method to represent corpus homogeneity as a distribution of similarity coefficients based on a cross-entropic measure investigated in previous works. We show that beyond minimal sizes of training data the excessive elimination of heterogeneous data proves prejudicial in terms of both perplexity and translation quality : excessively restricting the training data to a particular domain may be prejudicial in terms of In- Domain system performance , and that heterogeneous, Out-of- Domain data may in fact contribute to better sytem performance .", "tag": "RESULT"}, {"qas_id": "I05-2046.10_I05-2046.11", "question_text": "dictionaries [BREAK] systems", "context": "Using Maximum Entropy to Extract Biomedical Named Entities without Dictionaries . Current NER approaches include : dictionary-based , rule-based , or machine learning . Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily. In this paper , we apply Maximum <entity id=\"I05-2046.17\">Entropy </entity> (ME) to construct our NER framework . We represent shallow linguistic information as linguistic features in our ME model . On the GENIA 3.02 corpus , our system achieves satisfactory F-scores of 74.3% in protein and 70.0% overall without using any dictionary . Our system performs significantly better than <entity id=\"I05-2046.30\">dictionary-based systems </entity> . Using partial match criteria , our system achieves an F-score of 81.3%. Using appropriate domain knowledge to modify the boundaries , our system has the potential to achieve an F-score of over 80%.", "tag": "USAGE"}, {"qas_id": "I05-2046.16_I05-2046.19", "question_text": "Maximum <entity id=\"I05-2046.17\">Entropy [BREAK] framework", "context": "Using Maximum Entropy to Extract Biomedical Named Entities without Dictionaries . Current NER approaches include : dictionary-based , rule-based , or machine learning . Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily. In this paper , we apply Maximum <entity id=\"I05-2046.17\">Entropy </entity> (ME) to construct our NER framework . We represent shallow linguistic information as linguistic features in our ME model . On the GENIA 3.02 corpus , our system achieves satisfactory F-scores of 74.3% in protein and 70.0% overall without using any dictionary . Our system performs significantly better than <entity id=\"I05-2046.30\">dictionary-based systems </entity> . Using partial match criteria , our system achieves an F-score of 81.3%. Using appropriate domain knowledge to modify the boundaries , our system has the potential to achieve an F-score of over 80%.", "tag": "USAGE"}, {"qas_id": "I05-2046.27_I05-2046.29", "question_text": "system [BREAK] <entity id=\"I05-2046.30\">dictionary-based", "context": "Using Maximum Entropy to Extract Biomedical Named Entities without Dictionaries . Current NER approaches include : dictionary-based , rule-based , or machine learning . Since there is no consolidated nomenclature for most biomedical NEs, most NER systems relying on limited dictionaries or rules do not perform satisfactorily. In this paper , we apply Maximum <entity id=\"I05-2046.17\">Entropy </entity> (ME) to construct our NER framework . We represent shallow linguistic information as linguistic features in our ME model . On the GENIA 3.02 corpus , our system achieves satisfactory F-scores of 74.3% in protein and 70.0% overall without using any dictionary . Our system performs significantly better than <entity id=\"I05-2046.30\">dictionary-based systems </entity> . Using partial match criteria , our system achieves an F-score of 81.3%. Using appropriate domain knowledge to modify the boundaries , our system has the potential to achieve an F-score of over 80%.", "tag": "COMPARE"}, {"qas_id": "I05-3016.3_I05-3016.4", "question_text": "study [BREAK] anaphora <entity id=\"I05-3016.5\">resolution", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "TOPIC"}, {"qas_id": "I05-3016.12_I05-3016.13", "question_text": "algorithm [BREAK] gold <entity id=\"I05-3016.14\">standard", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "USAGE"}, {"qas_id": "I05-3016.20_I05-3016.22", "question_text": "characteristics [BREAK] algorithm", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "USAGE"}, {"qas_id": "I05-3016.26_I05-3016.28", "question_text": "order [BREAK] languages", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "I05-3016.33_I05-3016.34", "question_text": "algorithm [BREAK] accuracy", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "RESULT"}, {"qas_id": "I05-3016.40_I05-3016.41", "question_text": "algorithm [BREAK] accuracy", "context": "Resolving Pronominal References in Chinese with the Hobbs Algorithm . \"This study addresses pronominal anaphora <entity id=\"I05-3016.5\">resolution </entity> , including zero pronouns, in Chinese . A syntactic , rule-based pronoun resolution algorithm , the \"\"Hobbs algorithm \"\" was run on \"\" gold <entity id=\"I05-3016.14\">standard </entity> \"\" hand parses from the Penn Chinese Treebank. While first proposed for English , the algorithm counts for its success on two characteristics that Chinese and English have in common . Both languages are SVO, and both are fixed word order languages . No changes were made to adapt the algorithm to Chinese . The accuracy of the algorithm on overt, third-person pronouns at the matrix level was 77.6%, and the accuracy for resolving matrix-level zero pronouns was 73.3%. In contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3%, providing support for Miltsakaki 's two-mechanism proposal for resolving inter- vs. \"", "tag": "RESULT"}, {"qas_id": "I05-3034.1_I05-3034.4", "question_text": "Maximum <entity id=\"I05-3034.5\">Entropy [BREAK] Chinese <entity id=\"I05-3034.2\">Word Segmentation", "context": "Chinese <entity id=\"I05-3034.2\">Word Segmentation </entity> Based On Direct Maximum <entity id=\"I05-3034.5\">Entropy Model </entity> . Och, Franz Josef ; Ney , Hermann ,Discriminative Training And Maximum <entity id=\"I05-3034.9\">Entropy </entity> Models For Statistical <entity id=\"I05-3034.12\">Machine Translation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2002 *** Gao , Jianfeng ; Li , Mu; Huang , Changning,Improved Source- Channel Models For Chinese <entity id=\"I05-3034.20\">Word Segmentation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2003", "tag": "USAGE"}, {"qas_id": "I05-3034.8_I05-3034.11", "question_text": "Maximum <entity id=\"I05-3034.9\">Entropy [BREAK] Statistical <entity id=\"I05-3034.12\">Machine Translation", "context": "Chinese <entity id=\"I05-3034.2\">Word Segmentation </entity> Based On Direct Maximum <entity id=\"I05-3034.5\">Entropy Model </entity> . Och, Franz Josef ; Ney , Hermann ,Discriminative Training And Maximum <entity id=\"I05-3034.9\">Entropy </entity> Models For Statistical <entity id=\"I05-3034.12\">Machine Translation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2002 *** Gao , Jianfeng ; Li , Mu; Huang , Changning,Improved Source- Channel Models For Chinese <entity id=\"I05-3034.20\">Word Segmentation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2003", "tag": "USAGE"}, {"qas_id": "I05-3034.18_I05-3034.19", "question_text": "Models [BREAK] Chinese <entity id=\"I05-3034.20\">Word Segmentation", "context": "Chinese <entity id=\"I05-3034.2\">Word Segmentation </entity> Based On Direct Maximum <entity id=\"I05-3034.5\">Entropy Model </entity> . Och, Franz Josef ; Ney , Hermann ,Discriminative Training And Maximum <entity id=\"I05-3034.9\">Entropy </entity> Models For Statistical <entity id=\"I05-3034.12\">Machine Translation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2002 *** Gao , Jianfeng ; Li , Mu; Huang , Changning,Improved Source- Channel Models For Chinese <entity id=\"I05-3034.20\">Word Segmentation </entity> ,Annual Meeting Of The Association For Computation al Linguistics ,2003", "tag": "USAGE"}, {"qas_id": "E03-1088.10_E03-1088.12", "question_text": "paper [BREAK] variation", "context": "Linguistic Variation And Computation (Invited Talk) . Language variationists study how languages vary along geographical or social lines or along lines of age and gender . Variationist data is available and challenging , in particular for dialectology, the study of geographical variation , which will be the focus of this paper , although we present approaches we expect to transfer smoothly to the study of variation correlating with other extralinguistic variables . Techniques from computational linguistics on the one hand , and standard statistical data reduction techniques on the other, not only shed light on this classic linguistic problem , but they also suggest avenues for exploring the question at more abstract levels , and perhaps for seeking the determinants of variation .", "tag": "TOPIC"}, {"qas_id": "C96-2164.4_C96-2164.5", "question_text": "paper [BREAK] system", "context": "A Method For Abstracting Newspaper Articles By Using Surface Clues . This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text . To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .", "tag": "TOPIC"}, {"qas_id": "C96-2164.6_C96-2164.7", "question_text": "abstract [BREAK] newspaper", "context": "A Method For Abstracting Newspaper Articles By Using Surface Clues . This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text . To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .", "tag": "MODEL-FEATURE"}, {"qas_id": "C96-2164.8_C96-2164.9", "question_text": "sentences [BREAK] text", "context": "A Method For Abstracting Newspaper Articles By Using Surface Clues . This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text . To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .", "tag": "PART_WHOLE"}, {"qas_id": "C96-2164.16_C96-2164.19", "question_text": "analysis [BREAK] corpus", "context": "A Method For Abstracting Newspaper Articles By Using Surface Clues . This paper describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text . To determine the importance of a sentence , several superficial features are considered, and weights for features are determined by multiple-regression analysis of a hand processed corpus .", "tag": "TOPIC"}, {"qas_id": "C00-1036.11_C00-1036.13", "question_text": "mechanisms [BREAK] surface", "context": "XML And Multilingual Document Authoring: Convergent Trends . Typical approaches to XML authoring view a XML document as a mixture of structure (the tags ) and surface ( text between the tags ). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms . This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions , that the structure can then be viewed as interlingual content , and that the textual output should be derived from this content by language-specific realization mechanisms , thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose : (1) they are weak at propagating semantic dependencies between different parts of the structure , and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units . We present two related proposals for overcoming these limitations : one (GF) originating in the tradition of mathematical proof editors and constructive type theory , the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF.", "tag": "USAGE"}, {"qas_id": "C00-1036.22_C00-1036.23", "question_text": "output [BREAK] content", "context": "XML And Multilingual Document Authoring: Convergent Trends . Typical approaches to XML authoring view a XML document as a mixture of structure (the tags ) and surface ( text between the tags ). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms . This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions , that the structure can then be viewed as interlingual content , and that the textual output should be derived from this content by language-specific realization mechanisms , thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose : (1) they are weak at propagating semantic dependencies between different parts of the structure , and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units . We present two related proposals for overcoming these limitations : one (GF) originating in the tradition of mathematical proof editors and constructive type theory , the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF.", "tag": "PART_WHOLE"}, {"qas_id": "C00-1036.29_C00-1036.30", "question_text": "limitations [BREAK] tools", "context": "XML And Multilingual Document Authoring: Convergent Trends . Typical approaches to XML authoring view a XML document as a mixture of structure (the tags ) and surface ( text between the tags ). We advocate a radical approach where the surface disappears from the XML document altogether to be handled exclusively by rendering mechanisms . This move is based on the view that the author's choices when authoring XML documents are best seen as language-neutral semantic decisions , that the structure can then be viewed as interlingual content , and that the textual output should be derived from this content by language-specific realization mechanisms , thus assimilating XML authoring to Multilingual Document Authoring. However, standard XML tools have important limitations when used for such a purpose : (1) they are weak at propagating semantic dependencies between different parts of the structure , and, (2) current XML rendering tools are ill-suited for handling the grammatical combination of textual units . We present two related proposals for overcoming these limitations : one (GF) originating in the tradition of mathematical proof editors and constructive type theory , the other (IG), a specialization of Definite Clause Grammars strongly inspired by GF.", "tag": "MODEL-FEATURE"}, {"qas_id": "C02-1041.13_C02-1041.14", "question_text": "semantic <entity id=\"C02-1041.15\">representations [BREAK] sentences", "context": "Automatic Semantic Grouping In A Spoken Language User Interface Toolkit . With the rapid growth of real application domains for NLP systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems . Such a toolkit should provide an interface to accept sample sentences and convert them into semantic <entity id=\"C02-1041.15\">representations </entity> so as to allow programmers to map them to domain actions . In order to reduce the workload of managing a large number of semantic forms individually, the toolkit will perform what we call semantic grouping to organize the forms into meaningful groups. In this paper , we present three semantic grouping methods : similarity-based , verb-based and category-based grouping, and their implementation in the SLUI toolkit. We also discuss the pros and cons of each method and how they can be utilized according to the different domain needs.", "tag": "MODEL-FEATURE"}, {"qas_id": "C02-1041.27_C02-1041.29", "question_text": "paper [BREAK] methods", "context": "Automatic Semantic Grouping In A Spoken Language User Interface Toolkit . With the rapid growth of real application domains for NLP systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems . Such a toolkit should provide an interface to accept sample sentences and convert them into semantic <entity id=\"C02-1041.15\">representations </entity> so as to allow programmers to map them to domain actions . In order to reduce the workload of managing a large number of semantic forms individually, the toolkit will perform what we call semantic grouping to organize the forms into meaningful groups. In this paper , we present three semantic grouping methods : similarity-based , verb-based and category-based grouping, and their implementation in the SLUI toolkit. We also discuss the pros and cons of each method and how they can be utilized according to the different domain needs.", "tag": "TOPIC"}, {"qas_id": "E89-1008.2_E89-1008.3", "question_text": "notation [BREAK] relationships", "context": "Paradigmatic Morphology . We present a notation for the declarative statement of morphological relationships and lexical rules , based on the traditional notion of Word and Paradigm Elsewhere Condition, string equations", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1022.3_E99-1022.8", "question_text": "control [BREAK] parser", "context": "Selective Magic HPSG Parsing . We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control . The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion . State of the art top-down processing techniques are used to deal with the remaining constraints . We discuss various aspects concerning the implementation of the parser as part of a grammar development system .", "tag": "USAGE"}, {"qas_id": "E99-1022.12_E99-1022.13", "question_text": "compilation [BREAK] constraints", "context": "Selective Magic HPSG Parsing . We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control . The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion . State of the art top-down processing techniques are used to deal with the remaining constraints . We discuss various aspects concerning the implementation of the parser as part of a grammar development system .", "tag": "USAGE"}, {"qas_id": "E99-1022.18_E99-1022.21", "question_text": "processing [BREAK] constraints", "context": "Selective Magic HPSG Parsing . We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control . The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion . State of the art top-down processing techniques are used to deal with the remaining constraints . We discuss various aspects concerning the implementation of the parser as part of a grammar development system .", "tag": "USAGE"}, {"qas_id": "E99-1022.25_E99-1022.28", "question_text": "parser [BREAK] system", "context": "Selective Magic HPSG Parsing . We propose a parser for constraint-logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top-down control . The parser allows the user to apply magic compilation to specific constraints in a grammar which as a result can be processed dynamically in a bottom-up and goal-directed fashion . State of the art top-down processing techniques are used to deal with the remaining constraints . We discuss various aspects concerning the implementation of the parser as part of a grammar development system .", "tag": "PART_WHOLE"}, {"qas_id": "E99-1023.2_E99-1023.4", "question_text": "words [BREAK] sentences", "context": "Representing Text Chunks . \"Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and Marcus, 1995 ) have introduced a \"\"convenient\"\" data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the the data representation choice has a minor influence on chunking performance . However, equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set.\"", "tag": "PART_WHOLE"}, {"qas_id": "E99-1023.9_E99-1023.10", "question_text": "data [BREAK] representation", "context": "Representing Text Chunks . \"Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and Marcus, 1995 ) have introduced a \"\"convenient\"\" data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the the data representation choice has a minor influence on chunking performance . However, equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set.\"", "tag": "TOPIC"}, {"qas_id": "E99-1023.14_E99-1023.16", "question_text": "paper [BREAK] representations", "context": "Representing Text Chunks . \"Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and Marcus, 1995 ) have introduced a \"\"convenient\"\" data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the the data representation choice has a minor influence on chunking performance . However, equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set.\"", "tag": "TOPIC"}, {"qas_id": "E99-1023.22_E99-1023.25", "question_text": "choice [BREAK] performance", "context": "Representing Text Chunks . \"Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and Marcus, 1995 ) have introduced a \"\"convenient\"\" data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the the data representation choice has a minor influence on chunking performance . However, equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set.\"", "tag": "RESULT"}, {"qas_id": "E99-1024.4_E99-1024.5", "question_text": "Word [BREAK] List", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "PART_WHOLE"}, {"qas_id": "E99-1024.10_E99-1024.12", "question_text": "method [BREAK] errors", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "USAGE"}, {"qas_id": "E99-1024.15_E99-1024.17", "question_text": "errors [BREAK] revision", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "PART_WHOLE"}, {"qas_id": "E99-1024.20_E99-1024.21", "question_text": "errors [BREAK] texts", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1024.36_E99-1024.38", "question_text": "word [BREAK] list", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "PART_WHOLE"}, {"qas_id": "E99-1024.39_E99-1024.40", "question_text": "strength [BREAK] word", "context": "Detection Of Japanese Homophone Errors By A Decision List Including A Written Word As A Default Evidence . In this paper , we propose a practical method to detect Japanese homophone errors in Japanese texts . It is very important to detect homophone errors in Japanese revision systems because Japanese texts suffer from homophone errors frequently. In order to detect homophone errors , we have only to solve the homophone problem . We can use the decision list to do it because the homophone problem is equivalent to the word sense disambiguation problem . However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper , we incorporate the written word into the original decision list by obtaining the identifying strength of the written word . The improved decision list can raise the F-measure of error detection .", "tag": "MODEL-FEATURE"}, {"qas_id": "E99-1025.1_E99-1025.2", "question_text": "Models [BREAK] Disambiguation", "context": "New Models For Improving Supertag Disambiguation . In previous work, supertag disambiguation has been presented as a robust partial parsing technique . In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .", "tag": "USAGE"}, {"qas_id": "E99-1025.8_E99-1025.10", "question_text": "models [BREAK] paper", "context": "New Models For Improving Supertag Disambiguation . In previous work, supertag disambiguation has been presented as a robust partial parsing technique . In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .", "tag": "TOPIC"}, {"qas_id": "E99-1025.12_E99-1025.15", "question_text": "features [BREAK] performance", "context": "New Models For Improving Supertag Disambiguation . In previous work, supertag disambiguation has been presented as a robust partial parsing technique . In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .", "tag": "RESULT"}, {"qas_id": "E99-1025.16_E99-1025.17", "question_text": "class-based models [BREAK] words", "context": "New Models For Improving Supertag Disambiguation . In previous work, supertag disambiguation has been presented as a robust partial parsing technique . In this paper we present two approaches : contextual models , which exploit a variety of features in order to improve supertag performance , and class-based models , which assign sets of supertags to words in order to substantially improve accuracy with only a slight increase in ambiguity .", "tag": "USAGE"}, {"qas_id": "E99-1026.3_E99-1026.6", "question_text": "Models [BREAK] Analysis", "context": "Japanese Dependency Structure Analysis Based On Maximum Entropy Models . This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models . Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units . The dependency accuracy of our system is 87.2% using the Kyoto University corpus . We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy .", "tag": "USAGE"}, {"qas_id": "E99-1026.7_E99-1026.9", "question_text": "paper [BREAK] analysis", "context": "Japanese Dependency Structure Analysis Based On Maximum Entropy Models . This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models . Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units . The dependency accuracy of our system is 87.2% using the Kyoto University corpus . We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy .", "tag": "TOPIC"}, {"qas_id": "E99-1026.14_E99-1026.15", "question_text": "weights [BREAK] model", "context": "Japanese Dependency Structure Analysis Based On Maximum Entropy Models . This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models . Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units . The dependency accuracy of our system is 87.2% using the Kyoto University corpus . We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy .", "tag": "USAGE"}, {"qas_id": "E99-1026.21_E99-1026.22", "question_text": "accuracy [BREAK] system", "context": "Japanese Dependency Structure Analysis Based On Maximum Entropy Models . This paper describes a dependency structure analysis of Japanese sentences based on the maximum entropy models . Our model is created by learning the weights of some features from a training corpus to predict the dependency between bunsetsus or phrasal units . The dependency accuracy of our system is 87.2% using the Kyoto University corpus . We discuss the contribution of each feature set and the relationship between the number of training data and the accuracy .", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-1043.5_P06-1043.8", "question_text": "Wall Street Journal [BREAK] parsers", "context": "Reranking And Self- Training For Parser Adaptation . \" Statistical \" parsers trained and tested on the Penn Wall Street Journal (wsj) treebank have shown vast improvements over the last 10 years. Much of this improvement , however, is based upon an ever-increasing number of features to be trained on (typically) the wsj treebank data . This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . Such worries have merit. The standard \"\"Charniak parser \"\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set , but only 82.9% on the test set from the Brown treebank corpus . This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in ( McClosky et al., 2006 ) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data . This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%. \"", "tag": "USAGE"}, {"qas_id": "P06-1043.10_P06-1043.13", "question_text": "features [BREAK] improvement", "context": "Reranking And Self- Training For Parser Adaptation . \" Statistical \" parsers trained and tested on the Penn Wall Street Journal (wsj) treebank have shown vast improvements over the last 10 years. Much of this improvement , however, is based upon an ever-increasing number of features to be trained on (typically) the wsj treebank data . This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . Such worries have merit. The standard \"\"Charniak parser \"\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set , but only 82.9% on the test set from the Brown treebank corpus . This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in ( McClosky et al., 2006 ) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data . This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%. \"", "tag": "USAGE"}, {"qas_id": "P06-1043.17_P06-1043.18", "question_text": "corpus [BREAK] parsers", "context": "Reranking And Self- Training For Parser Adaptation . \" Statistical \" parsers trained and tested on the Penn Wall Street Journal (wsj) treebank have shown vast improvements over the last 10 years. Much of this improvement , however, is based upon an ever-increasing number of features to be trained on (typically) the wsj treebank data . This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . Such worries have merit. The standard \"\"Charniak parser \"\" checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set , but only 82.9% on the test set from the Brown treebank corpus . This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in ( McClosky et al., 2006 ) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data . This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%. \"", "tag": "USAGE"}, {"qas_id": "P06-1044.2_P06-1044.3", "question_text": "Classification [BREAK] Texts", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "USAGE"}, {"qas_id": "P06-1044.15_P06-1044.16", "question_text": "construction [BREAK] classes", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "USAGE"}, {"qas_id": "P06-1044.18_P06-1044.21", "question_text": "verb <entity id=\"P06-1044.19\">classes [BREAK] corpora", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "PART_WHOLE"}, {"qas_id": "P06-1044.25_P06-1044.28", "question_text": "technology [BREAK] domain", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "USAGE"}, {"qas_id": "P06-1044.30_P06-1044.31", "question_text": "corpus [BREAK] classification", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "USAGE"}, {"qas_id": "P06-1044.36_P06-1044.37", "question_text": "verbs [BREAK] texts", "context": "Automatic Classification Of Verbs In Biomedical Texts . Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing (nlp) tasks . While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb <entity id=\"P06-1044.19\">classes </entity> from cross-domain corpora with promising accuracy . We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification , acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific . It can be used to aid bio-nlp directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts .", "tag": "PART_WHOLE"}, {"qas_id": "P06-1082.26_P06-1082.28", "question_text": "word <entity id=\"P06-1082.27\">alignment [BREAK] English", "context": "Word Alignment In English- Hindi Parallel Corpus Using Recency- Vector Approach : Some Studies . Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown, 1994 ) and ( Somers, 1998 ), respectively, for word <entity id=\"P06-1082.27\">alignment </entity> in English - Hindi parallel corpus . But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus . The present paper discusses the new version of the algorithm and its performance in detail .", "tag": "USAGE"}, {"qas_id": "P06-1082.41_P06-1082.43", "question_text": "paper [BREAK] algorithm", "context": "Word Alignment In English- Hindi Parallel Corpus Using Recency- Vector Approach : Some Studies . Word alignment using recency-vector based approach has recently become popular. One major advantage of these techniques is that unlike other approaches they perform well even if the size of the parallel corpora is small. This makes these algorithms worth-studying for languages where resources are scarce. In this work we studied the performance of two very popular recency-vector based approaches , proposed in ( Fung and McKeown, 1994 ) and ( Somers, 1998 ), respectively, for word <entity id=\"P06-1082.27\">alignment </entity> in English - Hindi parallel corpus . But performance of the above algorithms was not found to be satisfactory. However, subsequent addition of some new constraints improved the performance of the recency-vector based alignment technique significantly for the said corpus . The present paper discusses the new version of the algorithm and its performance in detail .", "tag": "TOPIC"}, {"qas_id": "P06-1097.4_P06-1097.6", "question_text": "approach [BREAK] statistical <entity id=\"P06-1097.7\">machine translation", "context": "Semi-Supervised Training For Statistical Word Alignment . We introduce a semi-supervised approach to training for statistical <entity id=\"P06-1097.7\">machine translation </entity> that alternates the traditional Expectation Maximization step that is applied on a large training <entity id=\"P06-1097.13\">corpus </entity> with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus . We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .", "tag": "USAGE"}, {"qas_id": "P06-1097.10_P06-1097.12", "question_text": "step [BREAK] training <entity id=\"P06-1097.13\">corpus", "context": "Semi-Supervised Training For Statistical Word Alignment . We introduce a semi-supervised approach to training for statistical <entity id=\"P06-1097.7\">machine translation </entity> that alternates the traditional Expectation Maximization step that is applied on a large training <entity id=\"P06-1097.13\">corpus </entity> with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus . We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .", "tag": "USAGE"}, {"qas_id": "P06-1097.14_P06-1097.17", "question_text": "step [BREAK] quality", "context": "Semi-Supervised Training For Statistical Word Alignment . We introduce a semi-supervised approach to training for statistical <entity id=\"P06-1097.7\">machine translation </entity> that alternates the traditional Expectation Maximization step that is applied on a large training <entity id=\"P06-1097.13\">corpus </entity> with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus . We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality .", "tag": "RESULT"}, {"qas_id": "P06-1099.3_P06-1099.7", "question_text": "Evaluation [BREAK] Extraction", "context": "You Can't Beat Frequency (Unless You Use Linguistic Knowledge ) - A Qualitative Evaluation Of Association Measures For Collocation And Term Extraction . In the past years, a number of lexical association measures have been studied to help extract new scientific terminology or general-language collocations . The implicit assumption of this research was that newly designed term measures involving more sophisticated statistical criteria would outperform simple counts of cooccurrence frequencies . We here explicitly test this assumption . By way of four qualitative criteria , we show that purely statistics-based measures reveal virtually no difference compared with frequency of occurrence counts, while linguistically more informed metrics do reveal such a marked difference .", "tag": "TOPIC"}, {"qas_id": "P06-2002.7_P06-2002.8", "question_text": "paper [BREAK] extractor", "context": "A Rote Extractor With Edit Distance- Based Generalisation And Multi- Corpora Precision Calculation . In this paper , we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring. These include the use of part-of-speech tags to guide the generalization , Named Entity categories inside the patterns , an edit-distance-based pattern generalization algorithm , and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora . In an evaluation with 14 entities , the system attains a precision higher than 50% for half of the relationships considered.", "tag": "TOPIC"}, {"qas_id": "P06-2002.13_P06-2002.15", "question_text": "procedures [BREAK] generalization", "context": "A Rote Extractor With Edit Distance- Based Generalisation And Multi- Corpora Precision Calculation . In this paper , we describe a rote extractor that learns patterns for finding semantic relationships in unrestricted text , with new procedures for pattern generalization and scoring. These include the use of part-of-speech tags to guide the generalization , Named Entity categories inside the patterns , an edit-distance-based pattern generalization algorithm , and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora . In an evaluation with 14 entities , the system attains a precision higher than 50% for half of the relationships considered.", "tag": "USAGE"}, {"qas_id": "P06-2013.1_P06-2013.2", "question_text": "Study [BREAK] Chinese", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "TOPIC"}, {"qas_id": "P06-2013.6_P06-2013.7", "question_text": "chunking [BREAK] corpus", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "USAGE"}, {"qas_id": "P06-2013.32_P06-2013.35", "question_text": "chunking [BREAK] methods", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "USAGE"}, {"qas_id": "P06-2013.37_P06-2013.39", "question_text": "methods [BREAK] methods", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "COMPARE"}, {"qas_id": "P06-2013.44_P06-2013.45", "question_text": "model [BREAK] models", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "COMPARE"}, {"qas_id": "P06-2013.47_P06-2013.49", "question_text": "approaches [BREAK] performance", "context": "An Empirical Study Of Chinese Chunking . In this paper , we describe an empirical study of Chinese chunking on a corpus , which is extracted from UPENN Chinese Treebank-4 (CTB4). First, we compare the performance of the state-of-the-art machine learning models . Then we propose two approaches in order to improve the performance of Chinese chunking . 1) We propose an approach to resolve the special problems of Chinese chunking . This approach extends the chunk tags for every problem by a tag-extension function . 2) We propose two novel voting methods based on the characteristics of chunking task . Compared with traditional voting methods , the proposed voting methods consider long distance information . The experimental results show that the SVMs model outperforms the other models and that our proposed approaches can improve performance significantly.", "tag": "RESULT"}, {"qas_id": "P06-2014.1_P06-2014.2", "question_text": "Syntactic [BREAK] Word <entity id=\"P06-2014.3\">Alignment", "context": "Soft Syntactic Constraints For Word <entity id=\"P06-2014.3\">Alignment </entity> Through Discriminative Training . Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree . However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment <entity id=\"P06-2014.18\">models </entity> become more complex . We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint . The resulting aligner is the first, to our knowledge , to use a discriminative learning <entity id=\"P06-2014.28\">method </entity> to train an ITG bitext parser .", "tag": "USAGE"}, {"qas_id": "P06-2014.13_P06-2014.15", "question_text": "constraint [BREAK] alignments", "context": "Soft Syntactic Constraints For Word <entity id=\"P06-2014.3\">Alignment </entity> Through Discriminative Training . Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree . However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment <entity id=\"P06-2014.18\">models </entity> become more complex . We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint . The resulting aligner is the first, to our knowledge , to use a discriminative learning <entity id=\"P06-2014.28\">method </entity> to train an ITG bitext parser .", "tag": "RESULT"}, {"qas_id": "P06-2014.17_P06-2014.19", "question_text": "complex [BREAK] alignment <entity id=\"P06-2014.18\">models", "context": "Soft Syntactic Constraints For Word <entity id=\"P06-2014.3\">Alignment </entity> Through Discriminative Training . Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree . However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment <entity id=\"P06-2014.18\">models </entity> become more complex . We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint . The resulting aligner is the first, to our knowledge , to use a discriminative learning <entity id=\"P06-2014.28\">method </entity> to train an ITG bitext parser .", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-2014.27_P06-2014.30", "question_text": "learning <entity id=\"P06-2014.28\">method [BREAK] parser", "context": "Soft Syntactic Constraints For Word <entity id=\"P06-2014.3\">Alignment </entity> Through Discriminative Training . Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree . However, this hard constraint can also rule out correct alignments , and its utility decreases as alignment <entity id=\"P06-2014.18\">models </entity> become more complex . We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint . The resulting aligner is the first, to our knowledge , to use a discriminative learning <entity id=\"P06-2014.28\">method </entity> to train an ITG bitext parser .", "tag": "USAGE"}, {"qas_id": "P06-2023.1_P06-2023.4", "question_text": "Approach [BREAK] Extraction", "context": "A Bio-Inspired Approach For Multi- Word Expression Extraction . This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based <entity id=\"P06-2023.42\">approach </entity> achieves better results than n-gram .", "tag": "USAGE"}, {"qas_id": "P06-2023.5_P06-2023.7", "question_text": "paper [BREAK] approach", "context": "A Bio-Inspired Approach For Multi- Word Expression Extraction . This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based <entity id=\"P06-2023.42\">approach </entity> achieves better results than n-gram .", "tag": "TOPIC"}, {"qas_id": "P06-2023.9_P06-2023.10", "question_text": "extraction [BREAK] Expression", "context": "A Bio-Inspired Approach For Multi- Word Expression Extraction . This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based <entity id=\"P06-2023.42\">approach </entity> achieves better results than n-gram .", "tag": "USAGE"}, {"qas_id": "P06-2023.27_P06-2023.29", "question_text": "technique [BREAK] extraction", "context": "A Bio-Inspired Approach For Multi- Word Expression Extraction . This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based <entity id=\"P06-2023.42\">approach </entity> achieves better results than n-gram .", "tag": "USAGE"}, {"qas_id": "P06-2023.41_P06-2023.43", "question_text": "LCS-based <entity id=\"P06-2023.42\">approach [BREAK] results", "context": "A Bio-Inspired Approach For Multi- Word Expression Extraction . This paper proposes a new approach for Multi-word Expression (MWE) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics. We perform this developed LCS technique combined with linguistic criteria in MWE extraction . In comparison with traditional n-gram method , which is the major technique for MWE extraction , LCS approach is applied with great efficiency and performance guarantee. Experimental results show that LCS-based <entity id=\"P06-2023.42\">approach </entity> achieves better results than n-gram .", "tag": "RESULT"}, {"qas_id": "P06-2047.26_P06-2047.27", "question_text": "Algorithm [BREAK] computing", "context": "Graph Branch Algorithm : An Optimum Tree Search Method For Scored Dependency Graph With Arc Co- Occurrence Constraints . \"Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms . This paper classifies the scored dependency graphs and discusses the specific features of the \"\" Dependency Forest \"\" (DF) which is the packed shared data structure adopted in the \"\" Preference Dependency Grammar \"\" (PDG), and proposes the \"\"Graph Branch Algorithm \"\" for computing the optimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm . \"", "tag": "USAGE"}, {"qas_id": "P06-2047.34_P06-2047.35", "question_text": "behavior [BREAK] algorithm", "context": "Graph Branch Algorithm : An Optimum Tree Search Method For Scored Dependency Graph With Arc Co- Occurrence Constraints . \"Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms . This paper classifies the scored dependency graphs and discusses the specific features of the \"\" Dependency Forest \"\" (DF) which is the packed shared data structure adopted in the \"\" Preference Dependency Grammar \"\" (PDG), and proposes the \"\"Graph Branch Algorithm \"\" for computing the optimum dependency tree from a DF. This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm . \"", "tag": "PART_WHOLE"}, {"qas_id": "P06-2091.4_P06-2091.6", "question_text": "paper [BREAK] method", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "TOPIC"}, {"qas_id": "P06-2091.8_P06-2091.11", "question_text": "semantic <entity id=\"P06-2091.12\">representations [BREAK] outputs", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-2091.18_P06-2091.19", "question_text": "representations [BREAK] contexts", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-2091.26_P06-2091.27", "question_text": "robustness [BREAK] method", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-2091.30_P06-2091.31", "question_text": "implementation [BREAK] coverage", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "RESULT"}, {"qas_id": "P06-2091.32_P06-2091.34", "question_text": "text [BREAK] corpus", "context": "Translating HPSG-Style Outputs Of A Robust Parser Into Typed Dynamic Logic . The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic <entity id=\"P06-2091.12\">representations </entity> of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus . With its higher-order representations of contexts , TDL analyzes and describes the inherently inter-sentential nature of quantification and anaphora in a strictly lexicalized and compositional manner . The present study shows that the proposed translation method successfully combines robustness and descriptive adequacy of contemporary semantics . The present implementation achieves high coverage , approximately 90%, for the real text of the Penn Treebank corpus .", "tag": "PART_WHOLE"}, {"qas_id": "P06-2096.3_P06-2096.4", "question_text": "Texts [BREAK] Generation", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.21_P06-2096.22", "question_text": "translations [BREAK] text", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.25_P06-2096.27", "question_text": "parallel <entity id=\"P06-2096.28\">texts [BREAK] techniques", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.29_P06-2096.31", "question_text": "syntactic <entity id=\"P06-2096.30\">information [BREAK] alignments", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.32_P06-2096.33", "question_text": "paper [BREAK] algorithm", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "TOPIC"}, {"qas_id": "P06-2096.41_P06-2096.42", "question_text": "dynamic <entity id=\"P06-2096.43\">programming [BREAK] method", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.45_P06-2096.48", "question_text": "similarity [BREAK] decision", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "USAGE"}, {"qas_id": "P06-2096.52_P06-2096.54", "question_text": "alignment [BREAK] methods", "context": "Adding Syntax To Dynamic Programming For Aligning Comparable Texts For The Generation Of Paraphrases . Multiple sequence alignment techniques have recently gained popularity in the Natural Language community , especially for tasks such as machine <entity id=\"P06-2096.14\">translation </entity> , text generation , and paraphrase identification . Prior work falls into two categories , depending on the type of input used: (a) parallel corpora (e.g., multiple translations of the same text ) or (b) comparable texts (non-parallel but on the same topic ). So far, only techniques based on parallel <entity id=\"P06-2096.28\">texts </entity> have successfully used syntactic <entity id=\"P06-2096.30\">information </entity> to guide alignments . In this paper , we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts . Our method uses dynamic <entity id=\"P06-2096.43\">programming </entity> with alignment decision based on the local syntactic similarity between two sentences . Our results show that syntactic alignment outrivals syntax-free methods by 20% in both grammatically and fidelity when computed over the novel sentences generated by alignment-induced finite state automata.", "tag": "COMPARE"}, {"qas_id": "P06-3015.1_P06-3015.2", "question_text": "Parsing [BREAK] Interaction", "context": "Clavius: Bi-Directional Parsing For Generic Multimodal Interaction . We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.", "tag": "USAGE"}, {"qas_id": "P06-3015.3_P06-3015.7", "question_text": "parsing [BREAK] interaction", "context": "Clavius: Bi-Directional Parsing For Generic Multimodal Interaction . We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.", "tag": "USAGE"}, {"qas_id": "P06-3015.9_P06-3015.11", "question_text": "constraints [BREAK] constituents", "context": "Clavius: Bi-Directional Parsing For Generic Multimodal Interaction . We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-3015.12_P06-3015.13", "question_text": "difficulties [BREAK] systems", "context": "Clavius: Bi-Directional Parsing For Generic Multimodal Interaction . We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.", "tag": "MODEL-FEATURE"}, {"qas_id": "P06-3015.15_P06-3015.18", "question_text": "criteria [BREAK] process", "context": "Clavius: Bi-Directional Parsing For Generic Multimodal Interaction . We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments . By lifting some traditional constraints , namely those related to the ordering of constituents , we overcome several difficulties of other systems in this domain . We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions . Some early analyses of our implementation are discussed.", "tag": "RESULT"}, {"qas_id": "P06-4002.2_P06-4002.6", "question_text": "Evaluation [BREAK] Generation", "context": "Is It Correct? - Towards Web- Based Evaluation Of Automatic Natural Language Phrase Generation . This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases", "tag": "TOPIC"}, {"qas_id": "P06-4002.7_P06-4002.8", "question_text": "paper [BREAK] approach", "context": "Is It Correct? - Towards Web- Based Evaluation Of Automatic Natural Language Phrase Generation . This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases", "tag": "TOPIC"}, {"qas_id": "P06-4002.10_P06-4002.13", "question_text": "generation [BREAK] phrases", "context": "Is It Correct? - Towards Web- Based Evaluation Of Automatic Natural Language Phrase Generation . This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases", "tag": "USAGE"}, {"qas_id": "C80-1015.1_C80-1015.3", "question_text": "Model [BREAK] Time", "context": "A Model Of Natural Language Processing Of Time - Related Expressions .", "tag": "MODEL-FEATURE"}, {"qas_id": "C80-1063.1_C80-1063.3", "question_text": "Machine <entity id=\"C80-1063.2\">Translation System [BREAK] Japanese", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "USAGE"}, {"qas_id": "C80-1063.7_C80-1063.9", "question_text": "machine <entity id=\"C80-1063.8\">translation system [BREAK] Japanese", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "USAGE"}, {"qas_id": "C80-1063.12_C80-1063.14", "question_text": "translation [BREAK] manuals", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "USAGE"}, {"qas_id": "C80-1063.18_C80-1063.19", "question_text": "principles [BREAK] system", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "PART_WHOLE"}, {"qas_id": "C80-1063.23_C80-1063.25", "question_text": "effectiveness [BREAK] procedures", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C80-1063.49_C80-1063.50", "question_text": "abilities [BREAK] system", "context": "A Machine <entity id=\"C80-1063.2\">Translation System </entity> From Japanese Into English - Another Perspective Of MT Systems . A machine <entity id=\"C80-1063.8\">translation system </entity> from Japanese into English is described. The system aims at translation of computer manuals , and basically follows to the transfer approach . The design principles of the system are discussed in detail , together with the overall constructions of the system . Especially, the effectiveness of lexicon-based procedures , i.e. lexicon-based analysis , transfer , and synthesis , is emphasized. Most of the linguistic phenomena are treated by using lexical descriptions and lexical rules , instead of by general syntactic rules . Because Japanese and English belong to quite different language families, much more structural transfers are necessary than in other MT systems among European languages . Special cares have been paid for designing the transfer component . Some translation results are also given to illustrate the current abilities of the system .", "tag": "PART_WHOLE"}, {"qas_id": "C82-1016.7_C82-1016.8", "question_text": "paper [BREAK] model", "context": "Referential Nets With Attributes . One of the essential problems in natural language production and understanding is the problem of processing referential relations . In this paper I describe a model for representing and processing referential relations : referential nets with attributes. Both processes (analyzing and generating referential expressions ) are controlled by attributes. There are two types of attributes, on one hand , the ones to the internal substitutes of the objects spoken about, on the other hand , the ones to the descriptions of these objects .", "tag": "TOPIC"}, {"qas_id": "C82-1016.9_C82-1016.10", "question_text": "processing [BREAK] relations", "context": "Referential Nets With Attributes . One of the essential problems in natural language production and understanding is the problem of processing referential relations . In this paper I describe a model for representing and processing referential relations : referential nets with attributes. Both processes (analyzing and generating referential expressions ) are controlled by attributes. There are two types of attributes, on one hand , the ones to the internal substitutes of the objects spoken about, on the other hand , the ones to the descriptions of these objects .", "tag": "USAGE"}, {"qas_id": "C82-1021.10_C82-1021.12", "question_text": "paper [BREAK] approach", "context": "A Multilayered Approach To The Handling Of Word Formation . The treatment of word formations has until recently been a neglected topic in natural language AI research . This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural <entity id=\"C82-1021.18\">language </entity> dialogue <entity id=\"C82-1021.20\">system </entity> . Analysis and generation strategies being developed for the dialogue <entity id=\"C82-1021.26\">system </entity> HAM-ANS are described. Identification of word format ions , semantic interpretation , and evaluation in the context of a dialogue are the main levels of analysis on which the system successively attempts to infer the implicit relations between word formation components . Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .", "tag": "TOPIC"}, {"qas_id": "C82-1021.17_C82-1021.19", "question_text": "natural <entity id=\"C82-1021.18\">language [BREAK] dialogue <entity id=\"C82-1021.20\">system", "context": "A Multilayered Approach To The Handling Of Word Formation . The treatment of word formations has until recently been a neglected topic in natural language AI research . This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural <entity id=\"C82-1021.18\">language </entity> dialogue <entity id=\"C82-1021.20\">system </entity> . Analysis and generation strategies being developed for the dialogue <entity id=\"C82-1021.26\">system </entity> HAM-ANS are described. Identification of word format ions , semantic interpretation , and evaluation in the context of a dialogue are the main levels of analysis on which the system successively attempts to infer the implicit relations between word formation components . Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .", "tag": "USAGE"}, {"qas_id": "C82-1021.23_C82-1021.25", "question_text": "strategies [BREAK] dialogue <entity id=\"C82-1021.26\">system", "context": "A Multilayered Approach To The Handling Of Word Formation . The treatment of word formations has until recently been a neglected topic in natural language AI research . This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural <entity id=\"C82-1021.18\">language </entity> dialogue <entity id=\"C82-1021.20\">system </entity> . Analysis and generation strategies being developed for the dialogue <entity id=\"C82-1021.26\">system </entity> HAM-ANS are described. Identification of word format ions , semantic interpretation , and evaluation in the context of a dialogue are the main levels of analysis on which the system successively attempts to infer the implicit relations between word formation components . Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .", "tag": "USAGE"}, {"qas_id": "C82-1021.44_C82-1021.48", "question_text": "word [BREAK] utterances", "context": "A Multilayered Approach To The Handling Of Word Formation . The treatment of word formations has until recently been a neglected topic in natural language AI research . This paper proposes a multilayered approach to word formation which treats derivatives and compounds on several different levels of processing within a natural <entity id=\"C82-1021.18\">language </entity> dialogue <entity id=\"C82-1021.20\">system </entity> . Analysis and generation strategies being developed for the dialogue <entity id=\"C82-1021.26\">system </entity> HAM-ANS are described. Identification of word format ions , semantic interpretation , and evaluation in the context of a dialogue are the main levels of analysis on which the system successively attempts to infer the implicit relations between word formation components . Generation of word formations is viewed as a process comparable to the generation of elliptical utterances .", "tag": "COMPARE"}, {"qas_id": "C82-1031.11_C82-1031.12", "question_text": "paper [BREAK] problems", "context": "The Anatomy Of A Systemic Choice . Choice is one of the most prominent organizing concepts in systemic linguistics . Languages are described in terms of the choices available to the speaker and the relationships of those choices to each other and to the language produced. This paper addresses the problems of", "tag": "TOPIC"}, {"qas_id": "C82-1059.3_C82-1059.5", "question_text": "paper [BREAK] parser", "context": "Parsing German . The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production <entity id=\"C82-1059.10\">system </entity> which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .", "tag": "TOPIC"}, {"qas_id": "C82-1059.11_C82-1059.12", "question_text": "syntax [BREAK] method", "context": "Parsing German . The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production <entity id=\"C82-1059.10\">system </entity> which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .", "tag": "USAGE"}, {"qas_id": "C82-1059.15_C82-1059.16", "question_text": "representation [BREAK] system", "context": "Parsing German . The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production <entity id=\"C82-1059.10\">system </entity> which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .", "tag": "MODEL-FEATURE"}, {"qas_id": "C82-1059.20_C82-1059.21", "question_text": "features [BREAK] language", "context": "Parsing German . The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production <entity id=\"C82-1059.10\">system </entity> which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .", "tag": "MODEL-FEATURE"}, {"qas_id": "C82-1059.23_C82-1059.24", "question_text": "language [BREAK] parser", "context": "Parsing German . The first part of this paper is dedicated to an overview of the parser of the system VIE-LANG (Viennese Language Understanding System ). The parser is a production <entity id=\"C82-1059.10\">system </entity> which uses an interleaved method that combines syntax and semantics . It parses directly into the internal representation of the system , without producing an. intermediate syntactic structure . The last part discusses the relationship between some special features of the German language , and properties of the parser that originate in the language .", "tag": "USAGE"}, {"qas_id": "C86-1088.2_C86-1088.3", "question_text": "Semantics [BREAK] Discourse", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "TOPIC"}, {"qas_id": "C86-1088.12_C86-1088.13", "question_text": "semantics [BREAK] natural <entity id=\"C86-1088.14\">language", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "TOPIC"}, {"qas_id": "C86-1088.21_C86-1088.24", "question_text": "context [BREAK] interpretation", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "USAGE"}, {"qas_id": "C86-1088.26_C86-1088.27", "question_text": "information [BREAK] utterance", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "PART_WHOLE"}, {"qas_id": "C86-1088.33_C86-1088.36", "question_text": "investigation [BREAK] phenomena", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "TOPIC"}, {"qas_id": "C86-1088.37_C86-1088.38", "question_text": "description [BREAK] processes", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "MODEL-FEATURE"}, {"qas_id": "C86-1088.41_C86-1088.42", "question_text": "treatment [BREAK] noun <entity id=\"C86-1088.43\">phrases", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "TOPIC"}, {"qas_id": "C86-1088.71_C86-1088.73", "question_text": "context [BREAK] interpretation", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "USAGE"}, {"qas_id": "C86-1088.87_C86-1088.89", "question_text": "extensions [BREAK] system", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "PART_WHOLE"}, {"qas_id": "C86-1088.100_C86-1088.101", "question_text": "treatment [BREAK] noun <entity id=\"C86-1088.102\">phrases", "context": "Definite Noun Phrases And The Semantics Of Discourse . Discourse Representation Theory (DRT), developed by Hans Kamp several years ago ( Kamp 1981 ), belongs, together with Irene Heims narrowly related File Change Semantics ( Heim 1982 ) and Situation Semantics (Barwise/ Perry 1983 ), to a group of theoretical approaches which in the early Eighties introduced a dynamic, context-oriented perspective into the semantics of natural <entity id=\"C86-1088.14\">language </entity> . This recent development in theoretical semantics indicates a shift of interest towards topics that have been familiar in natural language processing research for the last decade: among others, the interpretation of new utterances with respect to a given context , and integration of the utterance information into that context ; the step-by-step construction of representations for larger pieces of discourse ; the investigation of text coherence phenomena ; and the description of referential processes . The core of DRT (and File Change Semantics ) is the treatment of indefinite noun <entity id=\"C86-1088.43\">phrases </entity> as reference establishing terms (as opposed to their standard truth-conditional quantifier analysis , but in accordance with the treatmant of indefinites in NLP research ) and definite noun phrases (pronouns as well as full NPs) as anaphoric expressions . It is one of the theoretically most appealing features of these theories that they provide simple unified accounts for all indefinites, and for all definites, respectively. This theoretical simplicity stands however in sharp contrast to the complexity of the process of etablishing reference observed in NLP research , and the variety of phenomena and linguistic levels involved. On the one hand , this contrast is quite natural : As a semantically motivated theory , DRT should not be expected to incorporate every detail of inferencing necessary to come up with an interpretation for a specific utterance in a given context ; it can better be thought of as an interface relating theoretical, truth-conditional semantics and the genuinely pragmatic work of text understanding . On the other hand , if DRT is seriously intended to bridge the gap between theoretical linguistics and the NLP approach , it should take into consideration as many factual restrictions on NP reference , and distinctions among subtypes of referential expressions , as is possible in a systematic and descriptive way. Several extensions of the standard system are at work, e.g. for the treatment of plural and temporal anaphora. Little, however, has yet been done to arrive at a closer view of the analysis of (singular) definite noun phrases , once the basic concepts had been established. The only attempt I know about is by Kamp himself, described in Kamp (1983), an unpublished fragment . In this talk I will first give a short overview of the basic DRT system , and sketch Kamp 's proposal for the treatment of definite noun <entity id=\"C86-1088.102\">phrases </entity> . Then I will indicate how the basic reference establishing function and the ' side-effects 'of different types of definite NPs can be described in more detail . In doing this, I will refer to the work about anaphora done in the NLP area (esp. by Barbara Grosz , Candy Sidner , and Bonnie Webber ), integrating some of their assumptions into the DRT framework , and critically commenting on some others.", "tag": "TOPIC"}, {"qas_id": "C86-1091.2_C86-1091.4", "question_text": "Acquisition [BREAK] Data", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "USAGE"}, {"qas_id": "C86-1091.5_C86-1091.8", "question_text": "knowledge <entity id=\"C86-1091.6\">base [BREAK] systems", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "PART_WHOLE"}, {"qas_id": "C86-1091.26_C86-1091.27", "question_text": "classification [BREAK] words", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "USAGE"}, {"qas_id": "C86-1091.28_C86-1091.32", "question_text": "Lexical knowledge [BREAK] Classification", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "USAGE"}, {"qas_id": "C86-1091.41_C86-1091.42", "question_text": "paper [BREAK] system", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "TOPIC"}, {"qas_id": "C86-1091.58_C86-1091.60", "question_text": "morphological <entity id=\"C86-1091.59\">analysis [BREAK] words", "context": "Towards The Automatic Acquisition Of Lexical Data . \"Creating a knowledge <entity id=\"C86-1091.6\">base </entity> has always been a bottleneck in the implementation of AI systems . This is also true for Natural Language Understanding (NEU) systems , particularly for data-driven ones. While a perfect system for automatic acquisition of all sorts of knowledge is still feir from being realized, partial solutions are possible. This holds especially for lexical data . Nevertheless, the task is not trivial, in particular when dealing with languages rich in inflectional Horms like German. Our: system is to be used by persons with no specific linguistic knowledge , thus linguistic expertise has been put into the system to ascertain correct classification of words . Classification is done by means of a small rule based system with Lexical knowledge and language-specific heuristics. The key idea is the identification of three sorts of knowledge which are processed distinctly and the optimal use of knowledge already contained in the existing Lexicon . 1 Introduction in this paper we introduce a system for the semi-automatic enlargement of a morphological 1 ex icon . If forms part of VIE-BANG, a German Language dialogue system ( Buchberger et al. 1982 ) . VI K-l. ANG serves not only as an ob ject but as a meta system as we 11 : i ts knowledge base is to be enlarged, and its facilities are used L:o support that process : the parsor serves to analyze the i nput to the acquisj ti on system , the generator i. s used to provide exampl es. Ln contrast to English the morphological <entity id=\"C86-1091.59\">analysis </entity> of German words is no trivial task , due to two causes: - First, there is a rich inflectional system , consisting of about 60 different endings (where most endings have various different interpretations ), some prefixes ('go-PPP, \"", "tag": "TOPIC"}, {"qas_id": "C88-1041.1_C88-1041.2", "question_text": "Architecture [BREAK] Parsing", "context": "The PSI/PHI Architecture For Prosodic Parsing . In is in as a a", "tag": "USAGE"}, {"qas_id": "C88-2134.19_C88-2134.20", "question_text": "description [BREAK] algorithms", "context": "Optimization Algorithms Of Deciphering As The Elements Of A Linguistic Theory . This paper presents an outline of the linguistic theory which may be identified with the partially ordered set of optimization algorithms of deciphering. An algorithm of deciphering is the operational definition of a given linguistic phenomenon which han the following three components : a set of admissible solutions , an objective function and a procedure which finds out the ministurn or the maximum of the objective function . The paper contains the description of the four algorithms of the proposed type : 1. The algorithm which classifies the letters into vowels and consonants . 2. The algorithm which identifies the morphemes in the text without the boundaries between words . 3. The algorithm whioh finds out the dependency <entity id=\"C88-2134.32\">tree </entity> of a sentence .", "tag": "TOPIC"}, {"qas_id": "C88-2134.28_C88-2134.29", "question_text": "boundaries [BREAK] words", "context": "Optimization Algorithms Of Deciphering As The Elements Of A Linguistic Theory . This paper presents an outline of the linguistic theory which may be identified with the partially ordered set of optimization algorithms of deciphering. An algorithm of deciphering is the operational definition of a given linguistic phenomenon which han the following three components : a set of admissible solutions , an objective function and a procedure which finds out the ministurn or the maximum of the objective function . The paper contains the description of the four algorithms of the proposed type : 1. The algorithm which classifies the letters into vowels and consonants . 2. The algorithm which identifies the morphemes in the text without the boundaries between words . 3. The algorithm whioh finds out the dependency <entity id=\"C88-2134.32\">tree </entity> of a sentence .", "tag": "PART_WHOLE"}, {"qas_id": "C88-2134.31_C88-2134.33", "question_text": "dependency <entity id=\"C88-2134.32\">tree [BREAK] sentence", "context": "Optimization Algorithms Of Deciphering As The Elements Of A Linguistic Theory . This paper presents an outline of the linguistic theory which may be identified with the partially ordered set of optimization algorithms of deciphering. An algorithm of deciphering is the operational definition of a given linguistic phenomenon which han the following three components : a set of admissible solutions , an objective function and a procedure which finds out the ministurn or the maximum of the objective function . The paper contains the description of the four algorithms of the proposed type : 1. The algorithm which classifies the letters into vowels and consonants . 2. The algorithm which identifies the morphemes in the text without the boundaries between words . 3. The algorithm whioh finds out the dependency <entity id=\"C88-2134.32\">tree </entity> of a sentence .", "tag": "MODEL-FEATURE"}, {"qas_id": "C90-2046.32_C90-2046.33", "question_text": "paper [BREAK] typology", "context": "Tenets For An Interlingual Representation Of Definite NPs . The main goal of this paper (as in Keenan and Stavi 1986 ) is to characterize the possible determiner denotations in order to develop a computational approach that makes explicit use of this information . To cope with the constraints that languages impose when generating determiners , a computational model has to follow the laws that map d finiteness to structures and strings and viceversa. In the following proposal I distantiate from K. B hlers Deixis Theory and Weinrichs (76) proposal where indefinites suggest subsequent information , while definite point out facts from the previous information . This very general position is insufficient if we want to formalize NP-definiteness. The semantics of NP defmiteness must be captured adequately in computational frameworks for such tasks as answering quantified NL-- questions , or in a MT system to convert NPs from one language into another. In the first part of this paper I draw a typology of defmiteness ; later I reflect on the defmiteness of NPs in an IL-representation . The major result is given by the determiner generators . Defmiteness should be evaluated in a Q-A system and in MT. The extensive functionality of defmiteness is first elaborated in the parsing and results in an IL-representation ; finally the determiner generators create correct morphological determiners and right determiner structures .", "tag": "TOPIC"}, {"qas_id": "C90-2057.3_C90-2057.11", "question_text": "paper [BREAK] machine <entity id=\"C90-2057.12\">translation system", "context": "Lexical Gaps And Idioms In Machine Translation . This paper describes the treatment of lexical gaps , collocation information and idioms in the English to Portuguese machine <entity id=\"C90-2057.12\">translation system </entity> PORTUGA. The perspective is strictly bilingual, in the sense that all problems referenced above are considered to belong to the transfer phase , and not, as in other systems , to analysis or generation . The solution presented invokes a parser for the target language (Portuguese) that analyses , producing the corresponding graph structure , the multiword <entity id=\"C90-2057.27\">expression </entity> selected as the result of lexical transfer . This process seems to bring considerable advantage in what readability and ease of bilingual dictionary development is concerned , and to furnish maximal flexibility together with minimal storage requirements . Finally, it also provides complete independence between dictionary and grammar formalisms .", "tag": "TOPIC"}, {"qas_id": "C90-2057.22_C90-2057.26", "question_text": "parser [BREAK] multiword <entity id=\"C90-2057.27\">expression", "context": "Lexical Gaps And Idioms In Machine Translation . This paper describes the treatment of lexical gaps , collocation information and idioms in the English to Portuguese machine <entity id=\"C90-2057.12\">translation system </entity> PORTUGA. The perspective is strictly bilingual, in the sense that all problems referenced above are considered to belong to the transfer phase , and not, as in other systems , to analysis or generation . The solution presented invokes a parser for the target language (Portuguese) that analyses , producing the corresponding graph structure , the multiword <entity id=\"C90-2057.27\">expression </entity> selected as the result of lexical transfer . This process seems to bring considerable advantage in what readability and ease of bilingual dictionary development is concerned , and to furnish maximal flexibility together with minimal storage requirements . Finally, it also provides complete independence between dictionary and grammar formalisms .", "tag": "USAGE"}, {"qas_id": "C92-1037.3_C92-1037.4", "question_text": "treatment [BREAK] semantics", "context": "Genetic NPs And Habitual VPs . We propose a simple , intuitively satisfying treatment of the semantics of bare plural NPs . This treatment avoids the use of nonstandard logics , and avoids the need for systematic ambiguity of verb semantics .", "tag": "TOPIC"}, {"qas_id": "C92-1037.7_C92-1037.9", "question_text": "ambiguity [BREAK] semantics", "context": "Genetic NPs And Habitual VPs . We propose a simple , intuitively satisfying treatment of the semantics of bare plural NPs . This treatment avoids the use of nonstandard logics , and avoids the need for systematic ambiguity of verb semantics .", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-1049.3_C92-1049.4", "question_text": "Model [BREAK] Dialogue", "context": "Using Linguistic, World, And Contextual Knowledge In A Plan Recognition Model Of Dialogue . This paper presents a plan-based <entity id=\"C92-1049.7\">model </entity> of dialogue that combines world, linguistic, and contextual knowledge in order to recognize complex communicative actions such as expressing doubt. Linguistic knowledge suggests certain discourse acts, a speaker's beliefs, and the strength of those beliefs; contextual knowledge suggests the most coherent continuation of the dialogue ; and world knowledge provides evidence that the applicability conditions hold for those discourse acts that capture the relationship of the current utterance to the discourse as a whole.", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-1049.5_C92-1049.6", "question_text": "paper [BREAK] plan-based <entity id=\"C92-1049.7\">model", "context": "Using Linguistic, World, And Contextual Knowledge In A Plan Recognition Model Of Dialogue . This paper presents a plan-based <entity id=\"C92-1049.7\">model </entity> of dialogue that combines world, linguistic, and contextual knowledge in order to recognize complex communicative actions such as expressing doubt. Linguistic knowledge suggests certain discourse acts, a speaker's beliefs, and the strength of those beliefs; contextual knowledge suggests the most coherent continuation of the dialogue ; and world knowledge provides evidence that the applicability conditions hold for those discourse acts that capture the relationship of the current utterance to the discourse as a whole.", "tag": "TOPIC"}, {"qas_id": "C92-1052.1_C92-1052.2", "question_text": "Structure [BREAK] Discourse", "context": "Temporal Structure Of Discourse . In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed . This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora <entity id=\"C92-1052.16\">resolution </entity> .", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-1052.6_C92-1052.7", "question_text": "method [BREAK] discourse", "context": "Temporal Structure Of Discourse . In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed . This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora <entity id=\"C92-1052.16\">resolution </entity> .", "tag": "USAGE"}, {"qas_id": "C92-1052.12_C92-1052.15", "question_text": "method [BREAK] anaphora <entity id=\"C92-1052.16\">resolution", "context": "Temporal Structure Of Discourse . In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed . This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora <entity id=\"C92-1052.16\">resolution </entity> .", "tag": "USAGE"}, {"qas_id": "C92-1053.25_C92-1053.26", "question_text": "patterns [BREAK] discourse", "context": "Organizing Dialogue From An Incoherent Stream Of Goals . their reasoning for structure their dialogues . Instead, computer-generated conversation must rely on some other mechanism for its organisation. In this paper , we discuss such mechanism . We describe provides a guide for conversation . The template is built from schemata representing discourse convention . As goals arrive from the problem solver they are added to the template . Because accepted discourse structures are used to connect a new goal to the existing template , goals are organised into sub-groups that follow conventional, coherent patterns of discourse . We present JUDIS , an interface to distributed problem solver that uses this approach to organise dialogues from incoherent of goals .", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-2108.3_C92-2108.5", "question_text": "temporal <entity id=\"C92-2108.4\">relations [BREAK] events", "context": "Preventing False Temporal Implicatures: Interactive Defaults For Text Generation . Given the causal and temporal <entity id=\"C92-2108.4\">relations </entity> between events in a knowledge base , what are the ways they can be described in text ? Elsewhere, we have argued that during interpretation , the reader-hearer // must infer certain temporal information from knowledge about the world, language use and pragmatics. It is generally agreed that processes of Gricean implicature help determine the interpretation of text in context . But without a notion of logical con se quo ii ce to underwrite them, the inferences -often defeasible in nature - will appear arbitrary, and unprincipled. Hence, we have explored the requirements on a formal model of temporal implicature, and outlined one possible nonmonotonic framework for discourse interpretation (Lascarides &amp;; Asher [1991], Lascarides Oberlander [1992a]). Here, we argue that if the writer-speaker 22077.", "tag": "PART_WHOLE"}, {"qas_id": "C92-2108.14_C92-2108.16", "question_text": "context [BREAK] interpretation", "context": "Preventing False Temporal Implicatures: Interactive Defaults For Text Generation . Given the causal and temporal <entity id=\"C92-2108.4\">relations </entity> between events in a knowledge base , what are the ways they can be described in text ? Elsewhere, we have argued that during interpretation , the reader-hearer // must infer certain temporal information from knowledge about the world, language use and pragmatics. It is generally agreed that processes of Gricean implicature help determine the interpretation of text in context . But without a notion of logical con se quo ii ce to underwrite them, the inferences -often defeasible in nature - will appear arbitrary, and unprincipled. Hence, we have explored the requirements on a formal model of temporal implicature, and outlined one possible nonmonotonic framework for discourse interpretation (Lascarides &amp;; Asher [1991], Lascarides Oberlander [1992a]). Here, we argue that if the writer-speaker 22077.", "tag": "USAGE"}, {"qas_id": "C92-2110.2_C92-2110.11", "question_text": "Tool [BREAK] Database", "context": "Design Tool Combining Keyword Analyzer And Case- Based Parser For Developing Natural Language Database Interfaces . We have designed and experimentally implemented a tool for developing a natural <entity id=\"C92-2110.17\">language systems </entity> that can accept extra-grammatical expressions , keyword sequences , and linguistic fragments , as well as ordinary natural language queries . The key to this tool 's efficiency is its effective use of a simple keyword analyzer in combination with a conventional case-based parser . The keyword analyzer performs a majority of those queries which are simple data retrievals . Since it uses only keywords in any query , this analyzer is robust with regard to extra-grammatical expressions . Since little labor is required of the application designer in using the keyword analyzer portion of the tool , and since the case-based parser processes only those queries which the keyword analyzer fails to interpret, total labor required of the designer is less than that for a tool which employs a conventional case-based parser alone.", "tag": "USAGE"}, {"qas_id": "C92-2110.14_C92-2110.16", "question_text": "tool [BREAK] natural <entity id=\"C92-2110.17\">language systems", "context": "Design Tool Combining Keyword Analyzer And Case- Based Parser For Developing Natural Language Database Interfaces . We have designed and experimentally implemented a tool for developing a natural <entity id=\"C92-2110.17\">language systems </entity> that can accept extra-grammatical expressions , keyword sequences , and linguistic fragments , as well as ordinary natural language queries . The key to this tool 's efficiency is its effective use of a simple keyword analyzer in combination with a conventional case-based parser . The keyword analyzer performs a majority of those queries which are simple data retrievals . Since it uses only keywords in any query , this analyzer is robust with regard to extra-grammatical expressions . Since little labor is required of the application designer in using the keyword analyzer portion of the tool , and since the case-based parser processes only those queries which the keyword analyzer fails to interpret, total labor required of the designer is less than that for a tool which employs a conventional case-based parser alone.", "tag": "USAGE"}, {"qas_id": "C92-2110.42_C92-2110.43", "question_text": "robust [BREAK] analyzer", "context": "Design Tool Combining Keyword Analyzer And Case- Based Parser For Developing Natural Language Database Interfaces . We have designed and experimentally implemented a tool for developing a natural <entity id=\"C92-2110.17\">language systems </entity> that can accept extra-grammatical expressions , keyword sequences , and linguistic fragments , as well as ordinary natural language queries . The key to this tool 's efficiency is its effective use of a simple keyword analyzer in combination with a conventional case-based parser . The keyword analyzer performs a majority of those queries which are simple data retrievals . Since it uses only keywords in any query , this analyzer is robust with regard to extra-grammatical expressions . Since little labor is required of the application designer in using the keyword analyzer portion of the tool , and since the case-based parser processes only those queries which the keyword analyzer fails to interpret, total labor required of the designer is less than that for a tool which employs a conventional case-based parser alone.", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-2119.3_C92-2119.4", "question_text": "limits [BREAK] speech <entity id=\"C92-2119.5\">recognition", "context": "A Robust Approach For Handling Oral Dialogues . Present limits of speech <entity id=\"C92-2119.5\">recognition </entity> and understanding in the context of free spoken language (altlwugh with a limited vocabulary ) have perverse effects on the flow of the dialogue with a system . Typically a non robust dialogue manager will fail to face with these limits and conversations will often be a failure. This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues . Several types of communication failures are presented and explained. They must be dealt with by the dialogue manager if we strike to have a robust system . The exposed strategies for handling these failures are based on a structural approach of the conversation and are implemented in the SUNDIAL system . We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-2119.18_C92-2119.19", "question_text": "paper [BREAK] possibilities", "context": "A Robust Approach For Handling Oral Dialogues . Present limits of speech <entity id=\"C92-2119.5\">recognition </entity> and understanding in the context of free spoken language (altlwugh with a limited vocabulary ) have perverse effects on the flow of the dialogue with a system . Typically a non robust dialogue manager will fail to face with these limits and conversations will often be a failure. This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues . Several types of communication failures are presented and explained. They must be dealt with by the dialogue manager if we strike to have a robust system . The exposed strategies for handling these failures are based on a structural approach of the conversation and are implemented in the SUNDIAL system . We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .", "tag": "TOPIC"}, {"qas_id": "C92-2119.23_C92-2119.24", "question_text": "task-oriented [BREAK] dialogues", "context": "A Robust Approach For Handling Oral Dialogues . Present limits of speech <entity id=\"C92-2119.5\">recognition </entity> and understanding in the context of free spoken language (altlwugh with a limited vocabulary ) have perverse effects on the flow of the dialogue with a system . Typically a non robust dialogue manager will fail to face with these limits and conversations will often be a failure. This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues . Several types of communication failures are presented and explained. They must be dealt with by the dialogue manager if we strike to have a robust system . The exposed strategies for handling these failures are based on a structural approach of the conversation and are implemented in the SUNDIAL system . We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-2119.43_C92-2119.46", "question_text": "system [BREAK] repairing", "context": "A Robust Approach For Handling Oral Dialogues . Present limits of speech <entity id=\"C92-2119.5\">recognition </entity> and understanding in the context of free spoken language (altlwugh with a limited vocabulary ) have perverse effects on the flow of the dialogue with a system . Typically a non robust dialogue manager will fail to face with these limits and conversations will often be a failure. This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues . Several types of communication failures are presented and explained. They must be dealt with by the dialogue manager if we strike to have a robust system . The exposed strategies for handling these failures are based on a structural approach of the conversation and are implemented in the SUNDIAL system . We first recall some aspects of the model and then describe the strategies for preventing and repairing communication failure in oral conversations with a system .", "tag": "USAGE"}, {"qas_id": "C92-3131.6_C92-3131.7", "question_text": "representation [BREAK] relations", "context": "Causal Ambiguity In Natural Language : Conceptual Representation Of'parce Que/because' And 'puisque/since' . This research deals with the representation of causal relations found in texts written in natural language , in order for KALIPSOS [1], an NL-understanding and question-answering system , to encode causal information in conceptual graphs so as to handle causal information and reasoning . Natural <entity id=\"C92-3131.17\">languages </entity> such as French or English have many ways to express a causal relation . It can be syntactic (parce que/because) {provoquer/to produce), (Je me suis cass e la jambe el je n'ai pas pu venir/1 broke my leg and I couldn't come), parce que/because puisque/since parce que/because puisque/since", "tag": "MODEL-FEATURE"}, {"qas_id": "C92-3147.5_C92-3147.6", "question_text": "paper [BREAK] system", "context": "B-SURE: A Believed Situation And Uncertain- Action Representation Environment . Tliis paper presents a system that is capable of representing situations , states, and nondeterniinistic nonmonotonic-outcome actions occurring in multiple possible worlds. The system supports explicit representations of actions and situations used in intentional action theory and situation theory , l oth types and instances are supported . Situations and states before and after nonmonotonic actions can be represented simultaneously. Agents have free will as to whether to choose to perform an action or not. Situations mid actions can have expected values, allowing the system to support decision-making and decision-based plan inferencing . The system can perform global reasoning simultaneously across multiple possible worlds, without being forced to extend each world explicitly. The resulting system is useful for Biich natural language tasks as plan recognition , intentions modeling , and parallel task scheduling.", "tag": "TOPIC"}, {"qas_id": "C92-3147.26_C92-3147.28", "question_text": "system [BREAK] decision-making", "context": "B-SURE: A Believed Situation And Uncertain- Action Representation Environment . Tliis paper presents a system that is capable of representing situations , states, and nondeterniinistic nonmonotonic-outcome actions occurring in multiple possible worlds. The system supports explicit representations of actions and situations used in intentional action theory and situation theory , l oth types and instances are supported . Situations and states before and after nonmonotonic actions can be represented simultaneously. Agents have free will as to whether to choose to perform an action or not. Situations mid actions can have expected values, allowing the system to support decision-making and decision-based plan inferencing . The system can perform global reasoning simultaneously across multiple possible worlds, without being forced to extend each world explicitly. The resulting system is useful for Biich natural language tasks as plan recognition , intentions modeling , and parallel task scheduling.", "tag": "USAGE"}, {"qas_id": "C94-1038.3_C94-1038.5", "question_text": "Case <entity id=\"C94-1038.4\">Study [BREAK] Syntactic <entity id=\"C94-1038.6\">Information", "context": "An Architecture For A Universal Lexicon A Case <entity id=\"C94-1038.4\">Study </entity> On Shared Syntactic <entity id=\"C94-1038.6\">Information </entity> In Japanese , Hindi, Bengali , Greek , And English . Pustejovsky , James ,The Generative Lexicon , Computation al Linguistics ,1991", "tag": "TOPIC"}, {"qas_id": "C94-1047.20_C94-1047.22", "question_text": "structure [BREAK] mechanisms", "context": "Logic Compression Of Dictionaries For Multilingual Spelling Checkers . \"To provide practical spelling checkers on micro-computers , good compression algorithms are essential. Current techniques used to compress lexicons for indo-Europcan languages provide efficient spelling checker. Applying the same methods to languages which have a different morphological system (Arabic, Turkish,...) gives insufficient results . To get better results , we apply other \"\"logical\"\" compression mechanisms based on the structure of the language itself. Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods . KEY WORDS : \"", "tag": "USAGE"}, {"qas_id": "C94-1047.24_C94-1047.30", "question_text": "Experiments [BREAK] results", "context": "Logic Compression Of Dictionaries For Multilingual Spelling Checkers . \"To provide practical spelling checkers on micro-computers , good compression algorithms are essential. Current techniques used to compress lexicons for indo-Europcan languages provide efficient spelling checker. Applying the same methods to languages which have a different morphological system (Arabic, Turkish,...) gives insufficient results . To get better results , we apply other \"\"logical\"\" compression mechanisms based on the structure of the language itself. Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods . KEY WORDS : \"", "tag": "RESULT"}, {"qas_id": "C94-1103.1_C94-1103.2", "question_text": "Tagging [BREAK] Corpus", "context": "CLAWS4: The Tagging Of The British National Corpus . The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100- million-word British National Corpus , of which c.70 million words have been tagged at the time of writing (April 1994 ).tagsets input formats . output formats :", "tag": "USAGE"}, {"qas_id": "C94-1103.7_C94-1103.8", "question_text": "tagging [BREAK] million-word", "context": "CLAWS4: The Tagging Of The British National Corpus . The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger, used for the tagging of the 100- million-word British National Corpus , of which c.70 million words have been tagged at the time of writing (April 1994 ).tagsets input formats . output formats :", "tag": "USAGE"}, {"qas_id": "C94-2169.10_C94-2169.12", "question_text": "cost [BREAK] retrieval", "context": "Thesaurus- Based Efficient Example Retrieval By Generating Retrieval Queries From Similarities . In example-based NLP, the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database . This paper proposes a novel example retrieval method for avoiding full retrieval of examples . The proposed method has the following three features , 1) it generates", "tag": "MODEL-FEATURE"}, {"qas_id": "C94-2169.19_C94-2169.23", "question_text": "paper [BREAK] method", "context": "Thesaurus- Based Efficient Example Retrieval By Generating Retrieval Queries From Similarities . In example-based NLP, the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database . This paper proposes a novel example retrieval method for avoiding full retrieval of examples . The proposed method has the following three features , 1) it generates", "tag": "TOPIC"}, {"qas_id": "C96-1001.3_C96-1001.4", "question_text": "segments [BREAK] discourses", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "PART_WHOLE"}, {"qas_id": "C96-1001.5_C96-1001.6", "question_text": "recognition [BREAK] segment", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "USAGE"}, {"qas_id": "C96-1001.10_C96-1001.14", "question_text": "boundaries [BREAK] language", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "PART_WHOLE"}, {"qas_id": "C96-1001.63_C96-1001.64", "question_text": "generation [BREAK] speech", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "USAGE"}, {"qas_id": "C96-1001.69_C96-1001.70", "question_text": "techniques [BREAK] discourse <entity id=\"C96-1001.71\">structure", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "USAGE"}, {"qas_id": "C96-1001.85_C96-1001.86", "question_text": "spontaneous <entity id=\"C96-1001.87\">speech [BREAK] corpora", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "PART_WHOLE"}, {"qas_id": "C96-1001.100_C96-1001.101", "question_text": "spontaneous <entity id=\"C96-1001.102\">speech [BREAK] samples", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "PART_WHOLE"}, {"qas_id": "C96-1001.106_C96-1001.107", "question_text": "semantics [BREAK] discourse structure", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "MODEL-FEATURE"}, {"qas_id": "C96-1001.109_C96-1001.110", "question_text": "tagging [BREAK] corpora", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "USAGE"}, {"qas_id": "C96-1001.141_C96-1001.142", "question_text": "segment [BREAK] discourses", "context": "Discovering The Sounds Of Discourse Structure Extended Abstract . It is widely accepted that discourses are composed of segments and that the recognition of segment boundaries is essential to a determination of discourse meaning ( Grosz and Sidner, 1986 ). Written language has orthographic cues such as section headings, paragraph boundaries , and punctuation which can assist in identifying discourse structure . In spoken language , into-national variation provides essential information about discourse structure . For instance , it may be used to mark structural features of discourse at the global level , such as segment boundaries . Intonation also provides more local information about relations among utterances within a segment , for example indicating whether phrases are parenthetical. It can also help distinguish between different interpretations of phrases that can function either as cue phrases that indicate discourse segment boundaries or sentcntially to convey domain information . Finally, variations in intonational prominence may be used to convey information about the discourse status of entities referred to by definite noun phrases and pronouns. An understanding of intonational variation and the ways in which it carries information about discourse characteristics of spoken language is important for computer-based interpretation and generation of speech . From the interpretation perspective , this understanding may provide new techniques for identifying discourse <entity id=\"C96-1001.71\">structure </entity> . From the generation perspective , it would lead to more natural synthetic speech , making it possible to produce computer speech that is easier for people to understand and less susceptible to misinterpretation. Three major challenges have faced researchers attempting to discover the relationship between intonational features and the structure of spoken discourse . First, the collection of corpora of spontaneous <entity id=\"C96-1001.87\">speech </entity> has required the development of * The research described in this presentation was supported by the National Science Foundation, Grant IRI 94-04756. The research has been done collaboratively with Julia Hirschberg and Christine Nakatani . David Ahn provided invaluable technical assistance. new experimental methodologies . Whereas it is straightforward to have the same text read by many speakers, it is much more difficult to obtain similar samples of spontaneous <entity id=\"C96-1001.102\">speech </entity> from multiple speakers. Second, techniques must be developed to obtain reliable segmentations and labelings of the corpora . Because discourse structure is rooted in semantics rather than syntax , this has proved more difficult than tagging corpora for sentence structure . Third, measures of agreement among segmentations must be designed . In this area too, the semantic nature of discourse structure leads to a more complex problem than comparing sentence parse structures . This talk will begin with a summary of pilot studies that demonstrated reliable correlations of discourse structure and intonational features ( Grosz and Hirschberg, 1992 ; Hirschbcrg and Grosz, 1992 ; Hirschbcrg and Grosz, 1994 ). It will then focus on a new corpus of direction-giving monologues, the Boston Directions Corpus ( Nakatani et al., 1995a ; Hirschberg and Nakatani, 1996 ). I will describe the methodology we developed to elicit fluent spontaneous direction-giving monologues ranging over a spectrum of planning complexity . Next I will describe the development of annotation instructions used to train labelers to segment spoken discourses ( Nakatani et ah, 1995b) and will discuss agreement among segmentations on the Boston Directions Corpus obtained using these instructions . Then I will describe results of our analyses of the correlation between discourse structure and intonational features . Finally, I will present a list of challenges for future research in this area .", "tag": "USAGE"}, {"qas_id": "C96-1010.1_C96-1010.2", "question_text": "Parsing [BREAK] Spoken <entity id=\"C96-1010.3\">Language", "context": "Parsing Spoken <entity id=\"C96-1010.3\">Language </entity> Without Syntax . Parsing spontaneous <entity id=\"C96-1010.7\">speech </entity> is a difficult task because of the ungrammatical nature of most spoken utterances . To overpass this problem , we propose in this paper to handle the spoken language without considering syntax . We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming. Experimental results on spontaneous speech show that this parser stands for a robust alternative to standard ones.", "tag": "USAGE"}, {"qas_id": "C96-1010.5_C96-1010.6", "question_text": "Parsing [BREAK] spontaneous <entity id=\"C96-1010.7\">speech", "context": "Parsing Spoken <entity id=\"C96-1010.3\">Language </entity> Without Syntax . Parsing spontaneous <entity id=\"C96-1010.7\">speech </entity> is a difficult task because of the ungrammatical nature of most spoken utterances . To overpass this problem , we propose in this paper to handle the spoken language without considering syntax . We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming. Experimental results on spontaneous speech show that this parser stands for a robust alternative to standard ones.", "tag": "USAGE"}, {"qas_id": "C96-1010.9_C96-1010.10", "question_text": "nature [BREAK] utterances", "context": "Parsing Spoken <entity id=\"C96-1010.3\">Language </entity> Without Syntax . Parsing spontaneous <entity id=\"C96-1010.7\">speech </entity> is a difficult task because of the ungrammatical nature of most spoken utterances . To overpass this problem , we propose in this paper to handle the spoken language without considering syntax . We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming. Experimental results on spontaneous speech show that this parser stands for a robust alternative to standard ones.", "tag": "MODEL-FEATURE"}, {"qas_id": "C86-1014.6_C86-1014.7", "question_text": "languages [BREAK] sample", "context": "Processing Word Order Variation Within A Modified ID/LP Framework . \"From a \"\"well represented sample of world languages Steele (1978) shows that about \"", "tag": "PART_WHOLE"}, {"qas_id": "C86-1074.4_C86-1074.5", "question_text": "paper [BREAK] translation", "context": "A Compositional Approach To The Translation Of Temporal Expressions In The ROSETTA System . This paper discusses the translation of temporal expressions , in the framework of the machine translation system Rosetta. The translation method of Rosetta, the 'isomorphic grammar method ', is based on Montague 's Compositionality Principle . It is shown that a compositional approach leads to a transparent account of the complex aspects of time in natural language and can be used for the translation of temporal expressions .", "tag": "TOPIC"}, {"qas_id": "C86-1074.19_C86-1074.20", "question_text": "translation [BREAK] expressions", "context": "A Compositional Approach To The Translation Of Temporal Expressions In The ROSETTA System . This paper discusses the translation of temporal expressions , in the framework of the machine translation system Rosetta. The translation method of Rosetta, the 'isomorphic grammar method ', is based on Montague 's Compositionality Principle . It is shown that a compositional approach leads to a transparent account of the complex aspects of time in natural language and can be used for the translation of temporal expressions .", "tag": "USAGE"}, {"qas_id": "C86-1078.2_C86-1078.3", "question_text": "system [BREAK] text <entity id=\"C86-1078.4\">analysis", "context": "Pragmatics In Machine Translation . TEXAN is a system of transferi-oriented text <entity id=\"C86-1078.4\">analysis </entity> . Its linguistic concept is based on a communicative approach within the framework of speech act theory . In this view texts are considered to be the result of linguistic actions . It is assumed that they control the selection of translation equivalents. The transition of this concept of linguistic actions ( text acts) to the model of computer analysis is performed by a context-free il locution grammar processing categories of actions and a propositional structure of states of affairs. The grammar which is related to a text lexicon provides the connection of these categories and the linguistic surface units of a single language .", "tag": "USAGE"}, {"qas_id": "C86-1133.6_C86-1133.8", "question_text": "paper [BREAK] system", "context": "From Structure To Process Computer- Assisted Teaching Of Various Strategies For Generating Pronoun Constructions In French . This paper describes an implemented tutoring system (2), designed to help students to generate clitic-constructions in French. While showing various ways of converting a given meaning structure into its corresponding surface expression , the system helps not only to discover what", "tag": "TOPIC"}, {"qas_id": "C86-1139.4_C86-1139.6", "question_text": "parsing [BREAK] utterances", "context": "Divided And Valency- Oriented Parsing In Speech Undstanding . A parsing scheme for spoken utterances is proposed that deviates from traditional 'one go' left to right sentence parsing in that it d vides the parsing process first into two aeperate parallel processes . Verbal constituents and nominal phrases ( including prepositonal phrases ) are treated seperately and only brought together in an utterance parser . This allows especially the utterance parser to draw on valency information right from beginning when amalgamating the nominal constituents to the verbal core by means of binary sentence rules . The paper also discusses problems of representing the valency information in case-frames arising in a spoken language environment .", "tag": "USAGE"}, {"qas_id": "C86-1144.3_C86-1144.4", "question_text": "Research [BREAK] text-to-speech", "context": "Computational Phonology : Merged, Not Mixed . Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics. Particularly for English , several text-to-speech systems have been established (cf. for example Hertz (1982), Klatt (1976)). For Dutch, text-to-speech systems are being developed at the University of Nijmegen (cf. Wester (1984)) and at the Universities of Utrecht and Leyden and the Institute of Perception Research (IPO) Eindhoven as well. In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven. One of our primary interests is that the grapheme-to-phoneme system not only has to generate the input for speech <entity id=\"C86-1144.33\">synthesis </entity> , either in allophone or diphone form , but that it had to be used for other purposes as well. Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ). In our view, there are two aspects to a grapheme-to-phoneme conversion system : a linguistic and a computational one. The linguist , in fact, provides the grammar necessary for the conversion and the engineer implements this grammar into a computer system . Thus, knowledge about spelling and linguistics are separated", "tag": "TOPIC"}, {"qas_id": "C86-1144.21_C86-1144.25", "question_text": "grapheme-to-phoneme [BREAK] text-to-speech", "context": "Computational Phonology : Merged, Not Mixed . Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics. Particularly for English , several text-to-speech systems have been established (cf. for example Hertz (1982), Klatt (1976)). For Dutch, text-to-speech systems are being developed at the University of Nijmegen (cf. Wester (1984)) and at the Universities of Utrecht and Leyden and the Institute of Perception Research (IPO) Eindhoven as well. In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven. One of our primary interests is that the grapheme-to-phoneme system not only has to generate the input for speech <entity id=\"C86-1144.33\">synthesis </entity> , either in allophone or diphone form , but that it had to be used for other purposes as well. Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ). In our view, there are two aspects to a grapheme-to-phoneme conversion system : a linguistic and a computational one. The linguist , in fact, provides the grammar necessary for the conversion and the engineer implements this grammar into a computer system . Thus, knowledge about spelling and linguistics are separated", "tag": "PART_WHOLE"}, {"qas_id": "C86-1144.28_C86-1144.32", "question_text": "grapheme-to-phoneme [BREAK] speech <entity id=\"C86-1144.33\">synthesis", "context": "Computational Phonology : Merged, Not Mixed . Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics. Particularly for English , several text-to-speech systems have been established (cf. for example Hertz (1982), Klatt (1976)). For Dutch, text-to-speech systems are being developed at the University of Nijmegen (cf. Wester (1984)) and at the Universities of Utrecht and Leyden and the Institute of Perception Research (IPO) Eindhoven as well. In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven. One of our primary interests is that the grapheme-to-phoneme system not only has to generate the input for speech <entity id=\"C86-1144.33\">synthesis </entity> , either in allophone or diphone form , but that it had to be used for other purposes as well. Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ). In our view, there are two aspects to a grapheme-to-phoneme conversion system : a linguistic and a computational one. The linguist , in fact, provides the grammar necessary for the conversion and the engineer implements this grammar into a computer system . Thus, knowledge about spelling and linguistics are separated", "tag": "USAGE"}, {"qas_id": "C86-1144.50_C86-1144.51", "question_text": "user-friendly [BREAK] system", "context": "Computational Phonology : Merged, Not Mixed . Research into text-to-speech systems has become a rather important topic in the areas of linguistics and phonetics. Particularly for English , several text-to-speech systems have been established (cf. for example Hertz (1982), Klatt (1976)). For Dutch, text-to-speech systems are being developed at the University of Nijmegen (cf. Wester (1984)) and at the Universities of Utrecht and Leyden and the Institute of Perception Research (IPO) Eindhoven as well. In this paper we will be concerned with the grapheme-to-phoneme conversion component as part of the Dutch text-to-speech system which is being developed in Utrecht, Leyden and Eindhoven. One of our primary interests is that the grapheme-to-phoneme system not only has to generate the input for speech <entity id=\"C86-1144.33\">synthesis </entity> , either in allophone or diphone form , but that it had to be used for other purposes as well. Thus, the system has to satisfy the following demands: - its output must form a proper and flexible input for diphone as well as allophone synthesis ; - it must be possible to easily generate phonematized lists on the basis of orthographic input ; - it must be possible to automatically obtain information regarding the relation between graphemes and phonemes in texts ; - the system has to be user-friendly , so that it can be addressed by linguists without computer training (for example to test their phonological rules ). In our view, there are two aspects to a grapheme-to-phoneme conversion system : a linguistic and a computational one. The linguist , in fact, provides the grammar necessary for the conversion and the engineer implements this grammar into a computer system . Thus, knowledge about spelling and linguistics are separated", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1037.4_C88-1037.6", "question_text": "paper [BREAK] method", "context": "Expressing Quantifier Scope In French Generation . In this paper we propose a new method to express quantification and especially quantifier scope in French generation . Our approach is based on two points: the identification of the sentence components between which quantifier scope can indeed be expressed and a mechanism to reinforce the expression of quantifier scope . This approach is being integrated in a written French generator , called Herm s, which will become the generator of a portable natural <entity id=\"C88-1037.27\">language interface </entity> .", "tag": "TOPIC"}, {"qas_id": "C88-1037.25_C88-1037.26", "question_text": "generator [BREAK] natural <entity id=\"C88-1037.27\">language", "context": "Expressing Quantifier Scope In French Generation . In this paper we propose a new method to express quantification and especially quantifier scope in French generation . Our approach is based on two points: the identification of the sentence components between which quantifier scope can indeed be expressed and a mechanism to reinforce the expression of quantifier scope . This approach is being integrated in a written French generator , called Herm s, which will become the generator of a portable natural <entity id=\"C88-1037.27\">language interface </entity> .", "tag": "PART_WHOLE"}, {"qas_id": "C88-1061.5_C88-1061.9", "question_text": "paper [BREAK] coordination", "context": "Constituent Coordination In Lexical- Functional Grammar . \" Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar. On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions . Function-application is extended to sets by treating a sot formally as the generalization of its functional elements. This causes properties attributed externally to a coordinate structure to be uniformly distributed across its elements, without requiring additional grammatical specifications . Introduction A proper treatment of coordination has long been an elusive goal of both theoretical and computational approaches to language . The original transformational formulation in terms of the Coordinate Reduction rule (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have linguistic theories (e g, GPSG / Gazdar et al. 1985 /, Categorial grammar (e.g. / Steedman 1985 /) made substantial progress on characterizing the complex restrictions on coordinate constructions and also on their semantic <entity id=\"C88-1061.40\">interpretations </entity> . Coordination has also presented descriptive problems for computational approaches . Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar. The best known examples of this kind of approach are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and McCord 1983 /. Coordination phenomena are usually divided into two classes , the so-called constituent coordinations where the coordinated elements look like otherwise well-motivated phrasal constituents II), and nonconstituent coordination where the coordinated elements look like fragments of phrasal constituents (2). (1) (a) A girl saw Mary and ran to Iiill. (Coordinated verb phrases ) (b) A girl saw and hoard Mary . (Coordinated verbs ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated constituent depends on the details of the particular grammatical theory . Constituents in transformationally-oriented theories , for example , are units that simplify the feeding relations of transformational rules , whereas \"\" constituents \"\" in categorial grammars merely reflect the order of binary combinations and have no other special motivation . In lexical-functional grammar, surface constituents are taken to be the units of phonological interpretation . These may differ markedly from the units of functional or semantic interpretation , as shown in the analysis of Dutch cross serial dependencies given by/ Bresnan et al. 1982 /. N'onconstituent coordination , of course, presents a wide variety of complex and difficult descriptive problems , but constituent coordination also raises important linguistic issues . It is the latter that we focus on in this brief paper . To a first approximation , constituent coordinations can be analyzed as the result of taking two independent clauses and factoring out their common subparts. The verb coordination in (lb) is thus related to the fuller sentence coordination in (3). This intuition , which was the basis of the Coordinate Reduction Transformation , accounts for more complex patterns of acceptability such as (4) illustrates. The coordination in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first approximation is frought with difficulties . It ensures that constituents of like categories can be conjoined only if they share some finer details of specification , but there are more subtle conditions that it does not cover. For example , even though (5a) and (5b) are both independently grammatical, the coordination in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) \"\"The girl promised and persuaded John to go, (Hint: Who is going\"", "tag": "TOPIC"}, {"qas_id": "C88-1061.12_C88-1061.14", "question_text": "arc [BREAK] constructions", "context": "Constituent Coordination In Lexical- Functional Grammar . \" Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar. On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions . Function-application is extended to sets by treating a sot formally as the generalization of its functional elements. This causes properties attributed externally to a coordinate structure to be uniformly distributed across its elements, without requiring additional grammatical specifications . Introduction A proper treatment of coordination has long been an elusive goal of both theoretical and computational approaches to language . The original transformational formulation in terms of the Coordinate Reduction rule (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have linguistic theories (e g, GPSG / Gazdar et al. 1985 /, Categorial grammar (e.g. / Steedman 1985 /) made substantial progress on characterizing the complex restrictions on coordinate constructions and also on their semantic <entity id=\"C88-1061.40\">interpretations </entity> . Coordination has also presented descriptive problems for computational approaches . Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar. The best known examples of this kind of approach are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and McCord 1983 /. Coordination phenomena are usually divided into two classes , the so-called constituent coordinations where the coordinated elements look like otherwise well-motivated phrasal constituents II), and nonconstituent coordination where the coordinated elements look like fragments of phrasal constituents (2). (1) (a) A girl saw Mary and ran to Iiill. (Coordinated verb phrases ) (b) A girl saw and hoard Mary . (Coordinated verbs ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated constituent depends on the details of the particular grammatical theory . Constituents in transformationally-oriented theories , for example , are units that simplify the feeding relations of transformational rules , whereas \"\" constituents \"\" in categorial grammars merely reflect the order of binary combinations and have no other special motivation . In lexical-functional grammar, surface constituents are taken to be the units of phonological interpretation . These may differ markedly from the units of functional or semantic interpretation , as shown in the analysis of Dutch cross serial dependencies given by/ Bresnan et al. 1982 /. N'onconstituent coordination , of course, presents a wide variety of complex and difficult descriptive problems , but constituent coordination also raises important linguistic issues . It is the latter that we focus on in this brief paper . To a first approximation , constituent coordinations can be analyzed as the result of taking two independent clauses and factoring out their common subparts. The verb coordination in (lb) is thus related to the fuller sentence coordination in (3). This intuition , which was the basis of the Coordinate Reduction Transformation , accounts for more complex patterns of acceptability such as (4) illustrates. The coordination in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first approximation is frought with difficulties . It ensures that constituents of like categories can be conjoined only if they share some finer details of specification , but there are more subtle conditions that it does not cover. For example , even though (5a) and (5b) are both independently grammatical, the coordination in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) \"\"The girl promised and persuaded John to go, (Hint: Who is going\"", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1061.38_C88-1061.39", "question_text": "semantic <entity id=\"C88-1061.40\">interpretations [BREAK] constructions", "context": "Constituent Coordination In Lexical- Functional Grammar . \" Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar. On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions . Function-application is extended to sets by treating a sot formally as the generalization of its functional elements. This causes properties attributed externally to a coordinate structure to be uniformly distributed across its elements, without requiring additional grammatical specifications . Introduction A proper treatment of coordination has long been an elusive goal of both theoretical and computational approaches to language . The original transformational formulation in terms of the Coordinate Reduction rule (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have linguistic theories (e g, GPSG / Gazdar et al. 1985 /, Categorial grammar (e.g. / Steedman 1985 /) made substantial progress on characterizing the complex restrictions on coordinate constructions and also on their semantic <entity id=\"C88-1061.40\">interpretations </entity> . Coordination has also presented descriptive problems for computational approaches . Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar. The best known examples of this kind of approach are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and McCord 1983 /. Coordination phenomena are usually divided into two classes , the so-called constituent coordinations where the coordinated elements look like otherwise well-motivated phrasal constituents II), and nonconstituent coordination where the coordinated elements look like fragments of phrasal constituents (2). (1) (a) A girl saw Mary and ran to Iiill. (Coordinated verb phrases ) (b) A girl saw and hoard Mary . (Coordinated verbs ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated constituent depends on the details of the particular grammatical theory . Constituents in transformationally-oriented theories , for example , are units that simplify the feeding relations of transformational rules , whereas \"\" constituents \"\" in categorial grammars merely reflect the order of binary combinations and have no other special motivation . In lexical-functional grammar, surface constituents are taken to be the units of phonological interpretation . These may differ markedly from the units of functional or semantic interpretation , as shown in the analysis of Dutch cross serial dependencies given by/ Bresnan et al. 1982 /. N'onconstituent coordination , of course, presents a wide variety of complex and difficult descriptive problems , but constituent coordination also raises important linguistic issues . It is the latter that we focus on in this brief paper . To a first approximation , constituent coordinations can be analyzed as the result of taking two independent clauses and factoring out their common subparts. The verb coordination in (lb) is thus related to the fuller sentence coordination in (3). This intuition , which was the basis of the Coordinate Reduction Transformation , accounts for more complex patterns of acceptability such as (4) illustrates. The coordination in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first approximation is frought with difficulties . It ensures that constituents of like categories can be conjoined only if they share some finer details of specification , but there are more subtle conditions that it does not cover. For example , even though (5a) and (5b) are both independently grammatical, the coordination in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) \"\"The girl promised and persuaded John to go, (Hint: Who is going\"", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-1061.46_C88-1061.48", "question_text": "devices [BREAK] algorithms", "context": "Constituent Coordination In Lexical- Functional Grammar . \" Abstract : This paper outlines a theory of constituent coordination for Lexical- Functional Grammar. On this theory LFG's flat, unstructured sets arc used as the functional representation of coordinate constructions . Function-application is extended to sets by treating a sot formally as the generalization of its functional elements. This causes properties attributed externally to a coordinate structure to be uniformly distributed across its elements, without requiring additional grammatical specifications . Introduction A proper treatment of coordination has long been an elusive goal of both theoretical and computational approaches to language . The original transformational formulation in terms of the Coordinate Reduction rule (e.g. / Dougherty 1970 /) was quickly shown to have many theoretical and empirical inadequacies, and only recently have linguistic theories (e g, GPSG / Gazdar et al. 1985 /, Categorial grammar (e.g. / Steedman 1985 /) made substantial progress on characterizing the complex restrictions on coordinate constructions and also on their semantic <entity id=\"C88-1061.40\">interpretations </entity> . Coordination has also presented descriptive problems for computational approaches . Typically these have been solved by special devices that are added to the parsing algorithms to analyze coordinate constructions that cannot easily be characterized in explicit rules of grammar. The best known examples of this kind of approach are SYSCONJ / Woods 1973 /, ESP / Sager 1981 /, and MSG / Dahl and McCord 1983 /. Coordination phenomena are usually divided into two classes , the so-called constituent coordinations where the coordinated elements look like otherwise well-motivated phrasal constituents II), and nonconstituent coordination where the coordinated elements look like fragments of phrasal constituents (2). (1) (a) A girl saw Mary and ran to Iiill. (Coordinated verb phrases ) (b) A girl saw and hoard Mary . (Coordinated verbs ) (2) Iiill went to Chicago on Wednesday and New York on Thursday. Of course, what is or is not a well-motivated constituent depends on the details of the particular grammatical theory . Constituents in transformationally-oriented theories , for example , are units that simplify the feeding relations of transformational rules , whereas \"\" constituents \"\" in categorial grammars merely reflect the order of binary combinations and have no other special motivation . In lexical-functional grammar, surface constituents are taken to be the units of phonological interpretation . These may differ markedly from the units of functional or semantic interpretation , as shown in the analysis of Dutch cross serial dependencies given by/ Bresnan et al. 1982 /. N'onconstituent coordination , of course, presents a wide variety of complex and difficult descriptive problems , but constituent coordination also raises important linguistic issues . It is the latter that we focus on in this brief paper . To a first approximation , constituent coordinations can be analyzed as the result of taking two independent clauses and factoring out their common subparts. The verb coordination in (lb) is thus related to the fuller sentence coordination in (3). This intuition , which was the basis of the Coordinate Reduction Transformation , accounts for more complex patterns of acceptability such as (4) illustrates. The coordination in (4c) is acceptable because both (4a) and (4b) are, while (4e) is bad because of the independent subcategorization violation in (4d). (3) A girl saw Mary and a girl heard Mary . (4) (a) A gir l dedicated a pie to Bill . (b) A girl gave a pie to Bill . (c) A girl dedicated and gave a pie to Bill . (d) *A girl ate a pie to Bill . (e) *A girl dedicated and ate a pie to Bill . This first approximation is frought with difficulties . It ensures that constituents of like categories can be conjoined only if they share some finer details of specification , but there are more subtle conditions that it does not cover. For example , even though (5a) and (5b) are both independently grammatical, the coordination in (5c) is unacceptable: (5) (a) The girl promised John to go. (b) The girl persuaded John to go. (c) \"\"The girl promised and persuaded John to go, (Hint: Who is going\"", "tag": "USAGE"}, {"qas_id": "C88-2089.1_C88-2089.3", "question_text": "Model [BREAK] Time", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-2089.38_C88-2089.39", "question_text": "translation [BREAK] verbs", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "USAGE"}, {"qas_id": "C88-2089.40_C88-2089.41", "question_text": "translation [BREAK] adverbs", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "USAGE"}, {"qas_id": "C88-2089.46_C88-2089.47", "question_text": "methodology [BREAK] translation", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "TOPIC"}, {"qas_id": "C88-2089.72_C88-2089.73", "question_text": "trees [BREAK] words", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "MODEL-FEATURE"}, {"qas_id": "C88-2089.77_C88-2089.78", "question_text": "features [BREAK] nodes", "context": "An Integrated Model For The Treatment Of Time In MT-Systems . One of the ways to achieve a good translation of verbal foras is the morphosyntactic approach , which consists in a function pairing the different morphological tenses that occur in a given language with the tenses of the other language . Complicated rules must be established to calculate the right pair for an expression , because of the amount of discrepancies that different languages show with respect to each other. he way wa have chosen to deal with this problem is, conversely, the projection of the different values coming from verbs ( type , processivity, morftense, morfaspect, sioodrequirement), from adverbs , prepositional phrases and temporal M.?s (deixis, aspect , iteration ), and from subordinate conjunctions ( aspect , moodrequirement). All this information permits to obtain a final value for aspect and tense for the whole sentence , which later on is percolated, not only to the verb node , but also to the the rest of elements conveying information . Our proposal relies on the fact that tense/ aspect calculation is relevant not only for a good translation of verbs , but also for a good translation of adverbs , PPs, temporal Ni-n and conjunctions , as we have intended to demonstrate in this paper . I. Introduction Thin article deals with a methodology to achieve the right translation of temporal expressions by giving account of the temporal reference and temporal relations in/ between sentences . The task to accomplish is to translate syntactic marks into semantic values that decide/ reflect the aspectunl value of the sentences . For our treatment of time and aspect we draw on the work of Kamp [1979] and Partee [1984] who have argued for taking status and events as primitives and relations of precedence und overlapping between them. The ordering relation between events is crucial for deciding about the aspect of the sentences involved. The present proposal presumes an analysis and a generation component that deliver a set of S- trees whose leaves correspond to words . The pre-terminals have morphosyntatitic and relational information . As usual, features am percolated and nodes get features assigned . She tine/ aspectual problem ia dealt with under the perspectiv of MT with the aim of sketching a system that can be implemented independently of the particular formalisms of different MT-systems . To outline a general model for the time / aspect calculation in MT we subsume a system with PSG rules that obtain some sentence structure with no regard to a specific grammar type ; it could be an augmented PSG, as in METAL, or some kind of deep syntactic structure , as it is the case in Eurotra. The problem is the well known fact that translations of temporal expressions in Ni does not involve a simple mapping of tenses and adverbials. We could just compare Spanish, rich in aspect and tenses vs. German or English . That is, a MT dealing with Germanic and Romance languages is concerned with different parameters for each language ; the whole practice in MT systems is to translate morphological tenses, and syntactical values into reference times that include events or slates", "tag": "MODEL-FEATURE"}, {"qas_id": "D08-1053.1_D08-1053.2", "question_text": "Alignment [BREAK] Sentence", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "USAGE"}, {"qas_id": "D08-1053.5_D08-1053.9", "question_text": "data [BREAK] web <entity id=\"D08-1053.6\">pages", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "PART_WHOLE"}, {"qas_id": "D08-1053.11_D08-1053.12", "question_text": "paper [BREAK] approach", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "TOPIC"}, {"qas_id": "D08-1053.14_D08-1053.15", "question_text": "alignment [BREAK] web <entity id=\"D08-1053.16\">pages", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "USAGE"}, {"qas_id": "D08-1053.24_D08-1053.26", "question_text": "tree [BREAK] web <entity id=\"D08-1053.25\">page", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "MODEL-FEATURE"}, {"qas_id": "D08-1053.34_D08-1053.36", "question_text": "method [BREAK] accuracy", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "RESULT"}, {"qas_id": "D08-1053.45_D08-1053.47", "question_text": "systems [BREAK] quality", "context": "Improved Sentence Alignment on Parallel Web Pages Using a Stochastic Tree Alignment Model . \"Parallel web <entity id=\"D08-1053.6\">pages </entity> are important source of training data for statistical machine translation . In this paper , we present a new approach to sentence alignment on parallel web <entity id=\"D08-1053.16\">pages </entity> . Parallel web pages tend to have parallel structures , and the structural correspondence can be indicative information for identifying parallel sentences . In our approach , the web <entity id=\"D08-1053.25\">page </entity> is represented as a tree , and a stochastic tree alignment model is used to exploit the structural correspondence for sentence alignment . Experiments show that this method significantly enhances alignment accuracy and robustness for parallel web pages which are much more diverse and noisy than standard parallel corpora such as \"\"Hansard\"\". With improved sentence alignment performance , web mining systems are able to acquire parallel sentences of higher quality from the web. \"", "tag": "RESULT"}, {"qas_id": "D08-1055.5_D08-1055.7", "question_text": "paper [BREAK] method", "context": "A Japanese Predicate Argument Structure Analysis using Decision Lists . This paper describes a new automatic method for Japanese predicate <entity id=\"D08-1055.10\">argument structure </entity> analysis . The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types , words , semantic categories , parts of speech , functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights . Using our method , we integrated the tasks of semantic role labeling and zero-pronoun identification , and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis .", "tag": "TOPIC"}, {"qas_id": "D08-1055.9_D08-1055.11", "question_text": "analysis [BREAK] predicate <entity id=\"D08-1055.10\">argument structure", "context": "A Japanese Predicate Argument Structure Analysis using Decision Lists . This paper describes a new automatic method for Japanese predicate <entity id=\"D08-1055.10\">argument structure </entity> analysis . The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types , words , semantic categories , parts of speech , functional words and predicate voices. We constructed decision lists in which these features were sorted by their learned weights . Using our method , we integrated the tasks of semantic role labeling and zero-pronoun identification , and achieved a 17% improvement compared with a baseline method in a sentence level performance analysis .", "tag": "TOPIC"}, {"qas_id": "D08-1068.10_D08-1068.11", "question_text": "paper [BREAK] approach", "context": "Joint Unsupervised Coreference Resolution with Markov Logic . Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data . Some unsuper-vised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper , we present the first un-supervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions , in contrast to the pairwise classification typically used in supervised methods , and by using Markov logic as a representation language , which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein 's one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models .", "tag": "TOPIC"}, {"qas_id": "E99-1043.14_E99-1043.16", "question_text": "information [BREAK] papers", "context": "The GENIA Project : Corpus- Based Knowledge Acquisition And Information Extraction From Genome Research Papers . We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts . GENIA will be available over the Internet and is designed to aid in information extraction , retrieval and visualisation and to help reduce information overload on researchers . The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.", "tag": "PART_WHOLE"}, {"qas_id": "E99-1043.25_E99-1043.26", "question_text": "papers [BREAK] databases", "context": "The GENIA Project : Corpus- Based Knowledge Acquisition And Information Extraction From Genome Research Papers . We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts . GENIA will be available over the Internet and is designed to aid in information extraction , retrieval and visualisation and to help reduce information overload on researchers . The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet.", "tag": "PART_WHOLE"}, {"qas_id": "E03-1086.4_E03-1086.7", "question_text": "paper [BREAK] word <entity id=\"E03-1086.8\">alignment", "context": "Interactive Word Alignment For Language Engineering . In this paper we report ongoing work on developing an interactive word <entity id=\"E03-1086.8\">alignment </entity> environment that will assist a user to quickly produce accurate full-coverage word alignment in bitexts for different language engineering tasks , such as MT lexicons and gold <entity id=\"E03-1086.18\">standards </entity> for evaluation . The system uses a graphical interface , static and dynamic resources as well as machine learning techniques . We also sketch how the system is being integrated with an automatic word aligner.", "tag": "TOPIC"}, {"qas_id": "E03-1086.17_E03-1086.19", "question_text": "gold <entity id=\"E03-1086.18\">standards [BREAK] evaluation", "context": "Interactive Word Alignment For Language Engineering . In this paper we report ongoing work on developing an interactive word <entity id=\"E03-1086.8\">alignment </entity> environment that will assist a user to quickly produce accurate full-coverage word alignment in bitexts for different language engineering tasks , such as MT lexicons and gold <entity id=\"E03-1086.18\">standards </entity> for evaluation . The system uses a graphical interface , static and dynamic resources as well as machine learning techniques . We also sketch how the system is being integrated with an automatic word aligner.", "tag": "USAGE"}, {"qas_id": "E06-1032.7_E06-1032.8", "question_text": "evaluation <entity id=\"E06-1032.9\">metric [BREAK] translation", "context": "Re- Evaluation The Role Of Bleu In Machine Translation Research . We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation <entity id=\"E06-1032.9\">metric </entity> . We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation <entity id=\"E06-1032.13\">quality </entity> , and give two significant counterexamples to Bleu 's correlation with human judgments of quality . This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.", "tag": "USAGE"}, {"qas_id": "E06-1043.7_E06-1043.8", "question_text": "expressions [BREAK] class", "context": "Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations . We investigate the lexical and syntactic flexibility of a class of idiomatic expressions . We develop measures that draw on such linguistic properties , and demonstrate that these statistical , corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation .", "tag": "PART_WHOLE"}, {"qas_id": "E06-1043.17_E06-1043.20", "question_text": "representation [BREAK] idiom", "context": "Automatically Constructing A Lexicon Of Verb Phrase Idiomatic Combinations . We investigate the lexical and syntactic flexibility of a class of idiomatic expressions . We develop measures that draw on such linguistic properties , and demonstrate that these statistical , corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones. We also propose a means for automatically determining which syntactic forms a particular idiom can appear in, and hence should be included in its lexical representation .", "tag": "MODEL-FEATURE"}, {"qas_id": "C96-2197.2_C96-2197.4", "question_text": "Tool [BREAK] Semantics", "context": "An Education And Research Tool For Computational Semantics . This paper describes an interactive graphical environment for computational semantics . The system provides a teaching tool , a stand alone extendible grapher, and a library of algorithms together with test suites . The teaching tool allows users to work step by step through derivations of semantic representations , and to compare the properties of various semantic formalisms such as Intensional Logic , DRT, and Situation Semantics . The system is freely available on the Internet.", "tag": "USAGE"}, {"qas_id": "C96-2197.12_C96-2197.13", "question_text": "library [BREAK] algorithms", "context": "An Education And Research Tool For Computational Semantics . This paper describes an interactive graphical environment for computational semantics . The system provides a teaching tool , a stand alone extendible grapher, and a library of algorithms together with test suites . The teaching tool allows users to work step by step through derivations of semantic representations , and to compare the properties of various semantic formalisms such as Intensional Logic , DRT, and Situation Semantics . The system is freely available on the Internet.", "tag": "PART_WHOLE"}, {"qas_id": "C00-1006.49_C00-1006.51", "question_text": "order-sensitive [BREAK] bag-of-words", "context": "The Effects Of Word Order And Segmentation On Translation Retrieval Performance . This research looks at the effects of word order and segmentation on translation retrieval performance for an experimental Japanese- English translation memory system . We implement a number of both bag-of-words and word order-sensitive similarity metrics , and test each over character-based and word-based indexing . The translation retrieval performance of each system configuration is evaluated empirically through the notion of word edit distance between translation candidate outputs and the model translation . Our results indicate that character-based indexing is consistently superior to word-based indexing , suggesting that segmentation is an unnecessary luxury in the given domain . Word order-sensitive approaches are demonstrated to generally outperform bag-of-words methods , with source language segment-level edit distance proving the most effective similarity metric .", "tag": "COMPARE"}, {"qas_id": "C00-1014.8_C00-1014.9", "question_text": "semantic <entity id=\"C00-1014.10\">classes [BREAK] classifiers", "context": "Reusing An Ontology To Generate Numeral Classifiers . In this paper , we present a solution to the problem of generating Japanese numeral classifiers using semantic <entity id=\"C00-1014.10\">classes </entity> from an ontology . Most nouns must take a numeral classifier when they are quantified in languages such as Chinese , Japanese , Korean, Malay and Thai. In order to select an appropriate classifier , we propose an algorithm which associates classifiers with semantic classes and uses inheritance to list only those classifiers which have to be listed . It generates sortal classifiers with an accuracy of 81%. We reuse the ontology provided by Goi-Taikei a Japanese lexicon , and show that it is a reasonable choice for this task , requiring information to be entered for less than 6% of individual nouns .", "tag": "USAGE"}, {"qas_id": "C00-1035.28_C00-1035.30", "question_text": "substructures [BREAK] parse", "context": "Aspects Of Pattern- Matching In Data- Oriented Parsing . Data- Oriented Parsing (dop) ranks among the best parsing schemes , pairing state-of-the art parsing accuracy to the psycholinguistic insight that larger chunks of syntactic structures are relevant grammatical and probabilistic units . Parsing with the dop-model , however, seems to involve a lot of CPU cycles and a considerable amount of double work, brought on by the concept of multiple derivations , which is necessary for probabilistic processing , but which is not convincingly related to a proper linguistic backbone . It is however possible to reinterpret the dop-model as a pattern-matching model , which tries to maximize the size of the substructures that construct the parse , rather than the probability of the parse . By emphasizing this memory-based aspect of the dop-model , it is possible to do away with multiple derivations , opening up possibilities for efficient Viterbi-style optimizations , while still retaining acceptable parsing accuracy through enhanced context-sensitivity .", "tag": "PART_WHOLE"}, {"qas_id": "C00-1060.5_C00-1060.7", "question_text": "paper [BREAK] method", "context": "A Hybrid Japanese Parser With Hand- Crafted Grammar And Statistics . This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique . The key feature of our system is that in order to estimate likelihood for a parse tree , the system uses information taken from alternative partial parse <entity id=\"C00-1060.21\">trees </entity> generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model .We", "tag": "TOPIC"}, {"qas_id": "C00-1060.17_C00-1060.20", "question_text": "information [BREAK] parse <entity id=\"C00-1060.21\">trees", "context": "A Hybrid Japanese Parser With Hand- Crafted Grammar And Statistics . This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted grammar and a statistical technique . The key feature of our system is that in order to estimate likelihood for a parse tree , the system uses information taken from alternative partial parse <entity id=\"C00-1060.21\">trees </entity> generated by the grammar. This utilization of alternative trees enables us to construct a new statistical model called Triplet/Quadruplet Model .We", "tag": "PART_WHOLE"}, {"qas_id": "C00-2101.6_C00-2101.7", "question_text": "paper [BREAK] approach", "context": "Learning Semantic- Level Information Extraction Rules By Type- Oriented ILP . This paper describes an approach to using semantic representations information extraction (IE) inductive logic programming (ILP)", "tag": "TOPIC"}, {"qas_id": "C00-2105.9_C00-2105.11", "question_text": "rules [BREAK] parsing", "context": "Robust German Noun Chunking With A Probabilistic Context- Free Grammar . We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text . The model <entity id=\"C00-2105.14\">parameters </entity> were learned from unlabelled training data by a probabilistic context-free parser . For extracting noun chunks , the parser generates all possible noun chunk analyses , scores them with a novel algorithm which maximizes the best chunk sequence criterion , and chooses the most probable chunk sequence . An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision .", "tag": "USAGE"}, {"qas_id": "C00-2105.13_C00-2105.16", "question_text": "model <entity id=\"C00-2105.14\">parameters [BREAK] data", "context": "Robust German Noun Chunking With A Probabilistic Context- Free Grammar . We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text . The model <entity id=\"C00-2105.14\">parameters </entity> were learned from unlabelled training data by a probabilistic context-free parser . For extracting noun chunks , the parser generates all possible noun chunk analyses , scores them with a novel algorithm which maximizes the best chunk sequence criterion , and chooses the most probable chunk sequence . An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision .", "tag": "USAGE"}, {"qas_id": "C00-2161.14_C00-2161.17", "question_text": "framework [BREAK] syntactic <entity id=\"C00-2161.15\">analysis", "context": "Querying Temporal Databases Using Controlled Natural Language . Recent years have shown a surge in interest in temporal database systems , which allow users to store time-dependent information . We present a novel controlled natural language interface to temporal databases , based on translating natural language questions into SQL/Temporal, a temporal database query language . The syntactic <entity id=\"C00-2161.15\">analysis </entity> is done using the Type- Logical Grammar framework , highlighting its utility not only as a theoretical framework but also as a practical tool . The semantic <entity id=\"C00-2161.22\">analysis </entity> is done using a novel theory of the semantics of temporal questions , focusing on the role of temporal preposition phrases rather than the more traditional focus on tense and aspect . Our translation method is considerably simpler than previous attempts in this direction . We present a prototype software implementation .", "tag": "USAGE"}, {"qas_id": "C00-2161.21_C00-2161.23", "question_text": "theory [BREAK] semantic <entity id=\"C00-2161.22\">analysis", "context": "Querying Temporal Databases Using Controlled Natural Language . Recent years have shown a surge in interest in temporal database systems , which allow users to store time-dependent information . We present a novel controlled natural language interface to temporal databases , based on translating natural language questions into SQL/Temporal, a temporal database query language . The syntactic <entity id=\"C00-2161.15\">analysis </entity> is done using the Type- Logical Grammar framework , highlighting its utility not only as a theoretical framework but also as a practical tool . The semantic <entity id=\"C00-2161.22\">analysis </entity> is done using a novel theory of the semantics of temporal questions , focusing on the role of temporal preposition phrases rather than the more traditional focus on tense and aspect . Our translation method is considerably simpler than previous attempts in this direction . We present a prototype software implementation .", "tag": "USAGE"}, {"qas_id": "C02-1003.6_C02-1003.8", "question_text": "paper [BREAK] method", "context": "Learning Chinese Bracketing Knowledge Based On A Bilingual Language Model . This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora . Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model . Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets . The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied . Although this paper discusses experiments with Chinese and English , the method is also applicable to other language <entity id=\"C02-1003.49\">pairs </entity> .", "tag": "TOPIC"}, {"qas_id": "C02-1003.13_C02-1003.17", "question_text": "knowledge [BREAK] corpora", "context": "Learning Chinese Bracketing Knowledge Based On A Bilingual Language Model . This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora . Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model . Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets . The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied . Although this paper discusses experiments with Chinese and English , the method is also applicable to other language <entity id=\"C02-1003.49\">pairs </entity> .", "tag": "PART_WHOLE"}, {"qas_id": "C02-1003.47_C02-1003.48", "question_text": "method [BREAK] language <entity id=\"C02-1003.49\">pairs", "context": "Learning Chinese Bracketing Knowledge Based On A Bilingual Language Model . This paper proposes a new method for automatic acquisition of Chinese bracketing knowledge from English- Chinese sentence-aligned bilingual corpora . Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model . Chinese bracketing knowledge is then extracted automatically. The preliminary experiments show automatically learned knowledge accords well with manually annotated brackets . The proposed method is particularly useful to acquire bracketing knowledge for a less studied language that lacks tools and resources found in a second language more studied . Although this paper discusses experiments with Chinese and English , the method is also applicable to other language <entity id=\"C02-1003.49\">pairs </entity> .", "tag": "USAGE"}, {"qas_id": "C02-1007.11_C02-1007.16", "question_text": "statistical <entity id=\"C02-1007.12\">models [BREAK] corpora", "context": "The Computation Of Word Associations: Comparing Syntagmatic And Paradigmatic Approaches . It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical <entity id=\"C02-1007.12\">models </entity> that analyze the distribution of words in large text corpora . According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts . The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics . The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data . It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects .", "tag": "USAGE"}, {"qas_id": "C02-1007.45_C02-1007.48", "question_text": "performance [BREAK] performance", "context": "The Computation Of Word Associations: Comparing Syntagmatic And Paradigmatic Approaches . It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical <entity id=\"C02-1007.12\">models </entity> that analyze the distribution of words in large text corpora . According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts . The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics . The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data . It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects .", "tag": "COMPARE"}, {"qas_id": "C02-1023.1_C02-1023.3", "question_text": "Parsing <entity id=\"C02-1023.2\">Algorithm [BREAK] Semantic <entity id=\"C02-1023.4\">Analysis", "context": "A Chart- Parsing <entity id=\"C02-1023.2\">Algorithm </entity> For Efficient Semantic <entity id=\"C02-1023.4\">Analysis </entity> . \"In some contexts , well-formed natural language cannot be expected as input to information or communication systems . In these contexts , the use of grammar-independent input ( sequences of uninflected semantic units like e.g. language-independent icons ) can be an answer to the users ' needs. However, this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time . Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units . It uses an algorithm inspired by the idea of \"\"chart parsing \"\" known in Natural Language Processing , which stores intermediate parsing results in order to bring the calculation time down. \"", "tag": "USAGE"}, {"qas_id": "C02-1023.27_C02-1023.28", "question_text": "analysis [BREAK] sequences", "context": "A Chart- Parsing <entity id=\"C02-1023.2\">Algorithm </entity> For Efficient Semantic <entity id=\"C02-1023.4\">Analysis </entity> . \"In some contexts , well-formed natural language cannot be expected as input to information or communication systems . In these contexts , the use of grammar-independent input ( sequences of uninflected semantic units like e.g. language-independent icons ) can be an answer to the users ' needs. However, this requires that an intelligent system should be able to interpret this input with reasonable accuracy and in reasonable time . Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units . It uses an algorithm inspired by the idea of \"\"chart parsing \"\" known in Natural Language Processing , which stores intermediate parsing results in order to bring the calculation time down. \"", "tag": "TOPIC"}, {"qas_id": "C02-1029.15_C02-1029.18", "question_text": "dynamic <entity id=\"C02-1029.16\">programming [BREAK] disambiguation", "context": "A Generative Probability Model For Unification- Based Grammars . A generative probability model for unification-based grammars is presented in which rule probabilities depend on the feature structure of the expanded constituent . The presented model is the first model which requires no normalization and allows the application of dynamic <entity id=\"C02-1029.16\">programming algorithms </entity> for disambiguation ( Viterbi ) and training (Inside-Outside). Another advantage is the small number of parameters .", "tag": "USAGE"}, {"qas_id": "C02-1038.5_C02-1038.6", "question_text": "paper [BREAK] method", "context": "Augmenting Noun Taxonomies By Combining Lexical Similarity Metrics . This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics . The combined approach is evaluated by comparing their results against the annotated SEMCOR corpus . An implementation is described in which WordNet is augmented with thesaural information from the CIDE+ machine readable dictionary .", "tag": "TOPIC"}, {"qas_id": "C02-1045.4_C02-1045.5", "question_text": "Indexing [BREAK] Data", "context": "A Method Of Cluster- Based Indexing Of Textual Data . This paper presents a framework for clustering in text-based information retrieval systems . The prominent feature of the proposed method is that documents , terms , and other related elements of textual information are clustered simultaneously into small overlapping clusters . In the paper , the mathematical formulation and implementation of the clustering method are briefly introduced, together with some experimental results .", "tag": "USAGE"}, {"qas_id": "C02-1045.6_C02-1045.7", "question_text": "paper [BREAK] framework", "context": "A Method Of Cluster- Based Indexing Of Textual Data . This paper presents a framework for clustering in text-based information retrieval systems . The prominent feature of the proposed method is that documents , terms , and other related elements of textual information are clustered simultaneously into small overlapping clusters . In the paper , the mathematical formulation and implementation of the clustering method are briefly introduced, together with some experimental results .", "tag": "TOPIC"}, {"qas_id": "C02-1059.23_C02-1059.25", "question_text": "paper [BREAK] method", "context": "Processing Japanese Self- Correction In Speech Dialog Systems . Speech dialog systems need to deal with various kinds of ill-formed speech inputs that appear in natural human-human dialog . Self-correction (or speech-repair ) is a particularly problematic phenomenon . Although many ways of dealing with self-correction have been proposed , these have limitations in both detecting and correcting for this phenomenon . In this paper , we propose a method to overcome these problems in Japanese speech dialog . We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done.", "tag": "TOPIC"}, {"qas_id": "C02-1122.8_C02-1122.10", "question_text": "paper [BREAK] method", "context": "Fertilization Of Case Frame Dictionary For Robust Japanese Case Analysis . This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions : double nominative sentences , non-gapping relation of relative clauses , and case change. Our method is divided into two stages. In the first stage, we parse a large corpus and construct a Japanese case frame dictionary automatically from the parse results . In the second stage, we apply case analysis to the large corpus utilizing the constructed case frame dictionary , and upgrade the case frame dictionary by incorporating newly acquired information .", "tag": "TOPIC"}, {"qas_id": "C02-1169.11_C02-1169.13", "question_text": "system [BREAK] components", "context": "Open- Domain Voice-Activated Question Answering . Voice-Activated Question Answering (VAQA) systems represent the next generation capability for universal access by integrating state-of-the-art in question answering Q&amp;A and automatic speech recognition (ASR) in such a way that the performance of the combined system is better than the individual components . This paper presents an implemented VAQA system and describes the techniques that enable the terative refinement of both Q&amp;A and ASR. The results of our experiments show that spoken questions can be processed with surprising accuracy when using our VAQA implementation .", "tag": "COMPARE"}, {"qas_id": "C02-1169.14_C02-1169.16", "question_text": "paper [BREAK] system", "context": "Open- Domain Voice-Activated Question Answering . Voice-Activated Question Answering (VAQA) systems represent the next generation capability for universal access by integrating state-of-the-art in question answering Q&amp;A and automatic speech recognition (ASR) in such a way that the performance of the combined system is better than the individual components . This paper presents an implemented VAQA system and describes the techniques that enable the terative refinement of both Q&amp;A and ASR. The results of our experiments show that spoken questions can be processed with surprising accuracy when using our VAQA implementation .", "tag": "TOPIC"}, {"qas_id": "C04-1048.5_C04-1048.6", "question_text": "paper [BREAK] system", "context": "Generating Discourse Structures For Written Text . This paper presents a system for automatically generating discourse structures from written text . The system is divided into two levels : sentence-level and text-level . The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences . At the text-level , constraints about textual adjacency and textual organization are integrated in a beam search in order to generate best discourse structures . The experiments were done with documents from the RST Discourse Treebank. It shows promising results in a reasonable search space compared to the discourse trees generated by human analysts .", "tag": "TOPIC"}, {"qas_id": "C04-1050.4_C04-1050.12", "question_text": "paper [BREAK] system", "context": "Improving Japanese Zero Pronoun Resolution By Global Word Sense Disambiguation . This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system . The word <entity id=\"C04-1050.14\">sense disambiguation </entity> is applied to verbs and nouns . We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis . In addition , according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words . We integrated this global word sense disambiguation into our zero pronoun resolution system , and conducted experiments of zero pronoun resolution on two different domain corpora . Both of the experimental results indicated the effectiveness of our approach .", "tag": "TOPIC"}, {"qas_id": "C04-1050.13_C04-1050.16", "question_text": "word <entity id=\"C04-1050.14\">sense disambiguation [BREAK] verbs", "context": "Improving Japanese Zero Pronoun Resolution By Global Word Sense Disambiguation . This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system . The word <entity id=\"C04-1050.14\">sense disambiguation </entity> is applied to verbs and nouns . We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis . In addition , according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words . We integrated this global word sense disambiguation into our zero pronoun resolution system , and conducted experiments of zero pronoun resolution on two different domain corpora . Both of the experimental results indicated the effectiveness of our approach .", "tag": "USAGE"}, {"qas_id": "C04-1050.44_C04-1050.46", "question_text": "approach [BREAK] results", "context": "Improving Japanese Zero Pronoun Resolution By Global Word Sense Disambiguation . This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system . The word <entity id=\"C04-1050.14\">sense disambiguation </entity> is applied to verbs and nouns . We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis . In addition , according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words . We integrated this global word sense disambiguation into our zero pronoun resolution system , and conducted experiments of zero pronoun resolution on two different domain corpora . Both of the experimental results indicated the effectiveness of our approach .", "tag": "RESULT"}, {"qas_id": "E85-1027.2_E85-1027.3", "question_text": "Theory [BREAK] Natural <entity id=\"E85-1027.4\">Language Generation", "context": "A Computational Theory Of Prose Style For Natural <entity id=\"E85-1027.4\">Language Generation </entity> . \"1. Where in the generation process style is taken into account. 2. How a particular prose style is represented; what \"\"stylistic rules \"\" look like; 3. What modifications to a generation algorithm are needed; what the decision is that evaluates stylistic alternatives ; 4. What elaborations to the normal description of surface structure are necessary to make it usable as a plan for the text and a reference for these decisions ; 5. What kinds of information decisions about style have access to. Our theory emerged out of design experiments we have made over the past year with our natural language generation system , the Zetalisp program MUMBLE. In the process we have extended MUMBLE through the addition of an additional process that now mediates between content planning and linguistic realization . This new process , which we call \"\" attachment \"\", provides the further significant benefit that text structure is no longer dictated by the structure of the message : the sequential order and dominance relationships of concepts in the message no longer force one form onto the words and phrases in the text . Instead, rhetorical and intentional directives can be interpreted flexibly in the context of the ongoing discourse and stylistic preferences . The text is built up through composition under the direction of linguistic organizing principles , rather than having to follow conceptual principles in lockstep. We will begin by describing what we mean by prose style and then introducing the generation task that lead us to this theory , the reproduction of short encyclopedia articles on African tribes. We will then use that task to outline the parts of our theory and the operations of the attachment process . Finally we will compare our techniques to the related work of Davey , McKeown and Derr , and Gabriel , and consider some of the possible psycholinguistic hypotheses that it may lead to. \"", "tag": "USAGE"}, {"qas_id": "E91-1031.22_E91-1031.24", "question_text": "paper [BREAK] technique", "context": "Prediction In Chart Parsing Algorithms For Categorial Unification Grammar . Natural language systems based on Categorial Unification Grammar (CUG) have mainly employed bottom-up parsing algorithms for processing . Conventional prediction techniques to improve the efficiency of the parsing process , appear to fall short when parsing CUG. Nevertheless, prediction seems necessary when parsing grammars with highly ambiguous lexicons or with non-canonical categorial rules . In this paper we present a lexicalist prediction technique for CUG and show that this may lead to considerable gains in efficiency for both bottom-up and top-down parsing .", "tag": "TOPIC"}]}